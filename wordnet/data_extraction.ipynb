{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original information is in XML format and DB files are needed for the Wordnet module. This sections contains all functions created for extracting relevant information from the XML file and converting it into a suitable format for database creation. Data extraction from XML format uses BeautifulSoup and database files are created using SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os, os.path\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the paths for the original XML file and DB files directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = os.path.join('source_data', 'estwn-et-2.6.0.xml')\n",
    "assert os.path.exists(source_path), f'(!) Missing wn source file {source_path}'\n",
    "\n",
    "db_dir = os.path.join('data', 'estwn-et-2.6.0')\n",
    "\n",
    "if not os.path.exists(db_dir):\n",
    "    os.makedirs(db_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting relevant information from all Wordnet entries from the XML-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_entries(lexical_entries):\n",
    "    wn_entries = []\n",
    "\n",
    "    for entry in lexical_entries:\n",
    "        lemma_info = entry.find('lemma')\n",
    "        pos = lemma_info.get('partofspeech')\n",
    "        lemma = lemma_info.get('writtenform')\n",
    "        for sense in entry.find_all('sense'):\n",
    "            sense_id = sense.get('id')\n",
    "            estwn_id = sense.get('synset')\n",
    "            sense = re.sub('[^0-9]', '', sense_id.split('-')[-1])\n",
    "            wn_entries.append((lemma, pos, sense, sense_id, estwn_id))\n",
    "            \n",
    "    return wn_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the name (main phrase of the synset) for synsets, most of them can be found from the definition of the synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synset_names(xml_synsets):\n",
    "    sourcesenses = {}\n",
    "\n",
    "    for s in xml_synsets:\n",
    "        d = s.find('definition')\n",
    "        if d is not None and d.get('sourcesense') not in sourcesenses:\n",
    "            if d.get('sourcesense') is not None:\n",
    "                sourcesenses[s.get('id')] = d.get('sourcesense')\n",
    "                \n",
    "    return sourcesenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list of tuples where each tuple contains relevant information about one lexical entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_entries(wn_entries, names):\n",
    "    sourcesenses = names\n",
    "    db_entries = []\n",
    "\n",
    "    for entry in wn_entries:\n",
    "        #Most of the names were found previously, but for some synsets, they couldn't be obtained.\n",
    "        #In those cases, the name of the first lexical entry of said synset is used as the name\n",
    "        if entry[-1] not in sourcesenses:\n",
    "            sourcesenses[entry[-1]] = entry[-2]\n",
    "        sourcesense = sourcesenses[entry[-1]]\n",
    "        if sourcesense == entry[3]:\n",
    "            #id is obtained from the estwn_id from the XML-file by removing all non-numeric characters from the string\n",
    "            db_entries.append((re.sub(\"[^0-9]\", \"\", entry[-1]), entry[0], entry[1], entry[2], sourcesense, entry[4], 1))\n",
    "        else:\n",
    "            db_entries.append((re.sub(\"[^0-9]\", \"\", entry[-1]), entry[0], entry[1], entry[2], sourcesense, entry[4], 0))\n",
    "            \n",
    "    return db_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list of tuples (of synset relations) where each tuple contains the start vertex, end vertex and specified relation.\n",
    "Start and end vertex of each relation are numeric id's of said synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_relations(xml_synsets):\n",
    "    db_relations = []\n",
    "\n",
    "    for synset in xml_synsets:\n",
    "        synset_id = re.sub(\"[^0-9]\", \"\", synset.get(\"id\"))\n",
    "        relations = synset.find_all(\"synsetrelation\")\n",
    "        for relation in relations:\n",
    "            db_relations.append((re.sub(\"[^0-9]\", \"\", relation.get(\"target\")), synset_id, relation.get(\"reltype\")))\n",
    "            \n",
    "    return db_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list of tuples where each tuple consists of synset name and its definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_definitions(xml_synsets):\n",
    "    db_definitions = []\n",
    "    synset_not_found = []\n",
    "\n",
    "    for synset in xml_synsets:\n",
    "        definitions = synset.find_all('definition')\n",
    "        if definitions is not None:\n",
    "            for definition in definitions:\n",
    "                sourcesense = definition.get('sourcesense')\n",
    "                if sourcesense is None:\n",
    "                    synset_id = synset.get('id')\n",
    "                    if synset_id not in sourcesenses:\n",
    "                        synset_not_found.append(synset_id)\n",
    "                    else:\n",
    "                        db_definitions.append((sourcesenses[synset_id], definition.find(text=True)))\n",
    "                else:\n",
    "                    db_definitions.append((sourcesense, definition.find(text=True)))\n",
    "                    \n",
    "    return db_definitions, synset_not_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list of tuples where each tuple consists of synset sense (literal) and an example for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_examples(xml_senses):\n",
    "    db_examples = []\n",
    "\n",
    "    for sense in xml_senses:\n",
    "        examples = sense.find_all(\"example\")\n",
    "        if examples is not None:\n",
    "            for example in examples:\n",
    "                db_examples.append((sense.get(\"id\"), example.find(text=True)))\n",
    "                \n",
    "    return db_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(file_path, db_name, create_table, insert, values):\n",
    "    conn = sqlite3.connect(file_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(create_table)\n",
    "    with conn:\n",
    "        cur.execute(\"DELETE FROM {};\".format(db_name)) #if database exists, deletes all values so there wouldn't be duplicates\n",
    "        cur.executemany(insert, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Programmid\\Miniconda3\\envs\\py39_devel\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(source_path, encoding=\"UTF-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "soup = BeautifulSoup(data, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = soup.find('lexicalentry')\n",
    "synset = soup.find('synset')\n",
    "sense = soup.find('sense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_entry = wordnet_entries([entry])\n",
    "sourcesenses = synset_names([synset])\n",
    "db_entry = database_entries(wn_entry, sourcesenses)\n",
    "db_relation = database_relations([synset])\n",
    "db_definition, not_found = database_definitions([synset])\n",
    "db_example = database_examples([sense])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_entry_len():\n",
    "    '''\n",
    "    >>> wn_entry_len()\n",
    "    5\n",
    "    '''\n",
    "    return len(wordnet_entries([entry])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sourcesense_test():\n",
    "    '''\n",
    "    >>> sourcesense_test()\n",
    "    True\n",
    "    '''\n",
    "    ss = synset_names([synset])\n",
    "    keys = list(sourcesenses.keys())\n",
    "    values = list(sourcesenses.values())\n",
    "    correct_key = False\n",
    "    correct_value = False\n",
    "    if re.match(r's-.+-', values[0]):\n",
    "        correct_value = True\n",
    "    if re.match(r'estwn-et-', keys[0]):\n",
    "        correct_key = True\n",
    "    return correct_key and correct_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_entry_len():\n",
    "    '''\n",
    "    >>> db_entry_len()\n",
    "    7\n",
    "    '''\n",
    "    wn_entry = wordnet_entries([entry])\n",
    "    sourcesense = synset_names([synset])\n",
    "    return len(database_entries(wn_entry, sourcesense)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_entry_id():\n",
    "    '''\n",
    "    >>> db_entry_id()\n",
    "    True\n",
    "    '''\n",
    "    try:\n",
    "        wn_entry = wordnet_entries([entry])\n",
    "        sourcesense = synset_names([synset])\n",
    "        db_entry = database_entries(wn_entry, sourcesense)[0]\n",
    "        entry_id = int(db_entry[0])\n",
    "        return int == type(entry_id)\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_relation_ids():\n",
    "    '''\n",
    "    >>> db_entry_id()\n",
    "    True\n",
    "    '''\n",
    "    try:\n",
    "        db_relation = database_relations([synset])\n",
    "        synset_1_id = int(db_relation[0])\n",
    "        synset_2_id = int(db_relation[1])\n",
    "        return type(synset_1_id) == int and type(synset_2_id) == int\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    db_entry_id()\n",
      "Expecting:\n",
      "    True\n",
      "ok\n",
      "Trying:\n",
      "    db_entry_len()\n",
      "Expecting:\n",
      "    7\n",
      "ok\n",
      "Trying:\n",
      "    db_entry_id()\n",
      "Expecting:\n",
      "    True\n",
      "ok\n",
      "Trying:\n",
      "    sourcesense_test()\n",
      "Expecting:\n",
      "    True\n",
      "ok\n",
      "Trying:\n",
      "    wn_entry_len()\n",
      "Expecting:\n",
      "    5\n",
      "ok\n",
      "8 items had no tests:\n",
      "    __main__\n",
      "    __main__.create_database\n",
      "    __main__.database_definitions\n",
      "    __main__.database_entries\n",
      "    __main__.database_examples\n",
      "    __main__.database_relations\n",
      "    __main__.synset_names\n",
      "    __main__.wordnet_entries\n",
      "5 items passed all tests:\n",
      "   1 tests in __main__.db_entry_id\n",
      "   1 tests in __main__.db_entry_len\n",
      "   1 tests in __main__.db_relation_ids\n",
      "   1 tests in __main__.sourcesense_test\n",
      "   1 tests in __main__.wn_entry_len\n",
      "5 tests in 13 items.\n",
      "5 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction and import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using previously created functions to extract relevant data from the XML file and create necessary database files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(source_path, encoding=\"UTF-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding all lexical entries, synset objects and senses by their tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = soup.find_all('lexicalentry')\n",
    "synsets = soup.find_all('synset')\n",
    "senses = soup.find_all('sense')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting necessary information to create database files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_entries = wordnet_entries(entries)\n",
    "sourcesenses = synset_names(synsets)\n",
    "db_entries = database_entries(wn_entries, sourcesenses)\n",
    "db_relations = database_relations(synsets)\n",
    "db_definitions, not_found = database_definitions(synsets)\n",
    "db_examples = database_examples(senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some discrepancies when it comes to hyponyms and hypernyms in the original XML file. Here these are found and printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_hypernyms = {}\n",
    "dict_hyponyms = {}\n",
    "\n",
    "for relation in db_relations:\n",
    "    if relation[2] == 'hypernym':\n",
    "        start = relation[0]\n",
    "        end = relation[1]\n",
    "        if start in dict_hypernyms:\n",
    "            dict_hypernyms[start].append(end)\n",
    "        else:\n",
    "            dict_hypernyms[start] = [end]\n",
    "    if relation[2] == 'hyponym':\n",
    "        start = relation[0]\n",
    "        end = relation[1]\n",
    "        if start in dict_hyponyms:\n",
    "            dict_hyponyms[start].append(end)\n",
    "        else:\n",
    "            dict_hyponyms[start] = [end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in dict_hypernyms.items():\n",
    "    for hyp in values:\n",
    "        if hyp not in dict_hyponyms or key not in dict_hyponyms[hyp]:\n",
    "            hyp_tuple = None\n",
    "            key_tuple = None\n",
    "            for entry in db_entries:\n",
    "                if hyp == entry[0]:\n",
    "                    hyp_tuple = entry[4]\n",
    "                if key == entry[0]:\n",
    "                    key_tuple = entry[4]\n",
    "            print(hyp_tuple, \"is hyponym for\", key_tuple, \"but\", key_tuple, \"is not hypernym for\", hyp_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s-tööriided-n1 is hypernym for s-tööülikond-n1 but s-tööülikond-n1 is not hyponym for s-tööriided-n1\n",
      "s-raadiokanal-n1 is hypernym for s-uudistekanal-n1 but s-uudistekanal-n1 is not hyponym for s-raadiokanal-n1\n",
      "s-katusekattematerjal-n1 is hypernym for s-katuseplekk-n1 but s-katuseplekk-n1 is not hyponym for s-katusekattematerjal-n1\n",
      "s-keeleteadus-n1 is hypernym for s-komparativism-n1 but s-komparativism-n1 is not hyponym for s-keeleteadus-n1\n",
      "s-arvutiteadus-n1 is hypernym for s-keeletehnoloogia-n1 but s-keeletehnoloogia-n1 is not hyponym for s-arvutiteadus-n1\n",
      "s-keeleteadus-n1 is hypernym for s-keeletehnoloogia-n1 but s-keeletehnoloogia-n1 is not hyponym for s-keeleteadus-n1\n",
      "s-piiriteadus-n1 is hypernym for s-keeletehnoloogia-n1 but s-keeletehnoloogia-n1 is not hyponym for s-piiriteadus-n1\n",
      "s-arvutiteadus-n1 is hypernym for s-arvutilingvistika-n1 but s-arvutilingvistika-n1 is not hyponym for s-arvutiteadus-n1\n",
      "s-piiriteadus-n1 is hypernym for s-arvutilingvistika-n1 but s-arvutilingvistika-n1 is not hyponym for s-piiriteadus-n1\n",
      "s-matt-n2 is hypernym for s-treeningmatt-n1 but s-treeningmatt-n1 is not hyponym for s-matt-n2\n",
      "s-spordivahend-n1 is hypernym for s-võimlemismatt-n1 but s-võimlemismatt-n1 is not hyponym for s-spordivahend-n1\n",
      "s-piiriteadus-n1 is hypernym for s-raadiometeoroloogia-n1 but s-raadiometeoroloogia-n1 is not hyponym for s-piiriteadus-n1\n",
      "s-infotehnoloogia-n1 is hypernym for s-digihumanitaaria-n1 but s-digihumanitaaria-n1 is not hyponym for s-infotehnoloogia-n1\n",
      "s-atroofia-n1 is hypernym for s-lihasatroofia-n1 but s-lihasatroofia-n1 is not hyponym for s-atroofia-n1\n",
      "s-põllumajandusmasin-n1 is hypernym for s-dibelmasin-n1 but s-dibelmasin-n1 is not hyponym for s-põllumajandusmasin-n1\n",
      "s-masin-n1 is hypernym for s-dibelmasin-n1 but s-dibelmasin-n1 is not hyponym for s-masin-n1\n",
      "s-kaduvik-n1 is hypernym for s-disinvesteerimine-n1 but s-disinvesteerimine-n1 is not hyponym for s-kaduvik-n1\n",
      "s-katusekattematerjal-n1 is hypernym for s-katusekivi-n1 but s-katusekivi-n1 is not hyponym for s-katusekattematerjal-n1\n",
      "s-siinus-n2 is hypernym for s-alumine_kaljuurke_vagu-n1 but s-alumine_kaljuurke_vagu-n1 is not hyponym for s-siinus-n2\n",
      "s-hoolekandeasutus-n1 is hypernym for s-lastekodu-n1 but s-lastekodu-n1 is not hyponym for s-hoolekandeasutus-n1\n",
      "s-jäämine-n3 is hypernym for s-denudatsioon-n1 but s-denudatsioon-n1 is not hyponym for s-jäämine-n3\n",
      "s-ema-n1 is hypernym for s-kesanoorik-n1 but s-kesanoorik-n1 is not hyponym for s-ema-n1\n",
      "s-piiriteadus-n1 is hypernym for s-morfofonoloogia-n1 but s-morfofonoloogia-n1 is not hyponym for s-piiriteadus-n1\n",
      "s-mõjustatavus-n1 is hypernym for s-dispersioon-n2 but s-dispersioon-n2 is not hyponym for s-mõjustatavus-n1\n",
      "s-joogipoolis-n1 is hypernym for s-alkohol-n1 but s-alkohol-n1 is not hyponym for s-joogipoolis-n1\n",
      "s-antiloop-n1 is hypernym for s-redunka-n1 but s-redunka-n1 is not hyponym for s-antiloop-n1\n",
      "s-turgid-n1 is hypernym for s-aser-n1 but s-aser-n1 is not hyponym for s-turgid-n1\n",
      "s-narkootikum-n1 is hypernym for s-morfiin-n1 but s-morfiin-n1 is not hyponym for s-narkootikum-n1\n",
      "s-narkootikum-n1 is hypernym for s-heroiin-n1 but s-heroiin-n1 is not hyponym for s-narkootikum-n1\n",
      "s-narkootikum-n1 is hypernym for s-kokaiin-n1 but s-kokaiin-n1 is not hyponym for s-narkootikum-n1\n",
      "s-narkootikum-n1 is hypernym for s-oopium-n1 but s-oopium-n1 is not hyponym for s-narkootikum-n1\n",
      "s-tuvastamine-n2 is hypernym for s-automaatne_kõnetuvastus-n1 but s-automaatne_kõnetuvastus-n1 is not hyponym for s-tuvastamine-n2\n",
      "s-kõnetöötlus-n1 is hypernym for s-automaatne_kõnetuvastus-n1 but s-automaatne_kõnetuvastus-n1 is not hyponym for s-kõnetöötlus-n1\n",
      "s-tuvastamine-n2 is hypernym for s-kõnelejatuvastus-n1 but s-kõnelejatuvastus-n1 is not hyponym for s-tuvastamine-n2\n",
      "s-kliiniline_uuring-n1 is hypernym for s-skriining-n1 but s-skriining-n1 is not hyponym for s-kliiniline_uuring-n1\n",
      "s-piiriteadus-n1 is hypernym for s-loomapaleontoloogia-n1 but s-loomapaleontoloogia-n1 is not hyponym for s-piiriteadus-n1\n",
      "s-piiriteadus-n1 is hypernym for s-ökofüsioloogia-n1 but s-ökofüsioloogia-n1 is not hyponym for s-piiriteadus-n1\n",
      "s-sõnaliik-n1 is hypernym for s-adjektiiv-n1 but s-adjektiiv-n1 is not hyponym for s-sõnaliik-n1\n",
      "s-sõnaliik-n1 is hypernym for s-nimisõna-n1 but s-nimisõna-n1 is not hyponym for s-sõnaliik-n1\n",
      "s-pöördsõna-n1 is hypernym for s-tegusõna-n1 but s-tegusõna-n1 is not hyponym for s-pöördsõna-n1\n",
      "s-rühm-n5 is hypernym for s-kohtumine-n1 but s-kohtumine-n1 is not hyponym for s-rühm-n5\n",
      "s-suulu-n1 is hypernym for s-suulu_keel-n1 but s-suulu_keel-n1 is not hyponym for s-suulu-n1\n",
      "s-piirkond-n3 is hypernym for s-hobulaius-n1 but s-hobulaius-n1 is not hyponym for s-piirkond-n3\n",
      "s-joon-n3 is hypernym for s-liigierisus-n1 but s-liigierisus-n1 is not hyponym for s-joon-n3\n",
      "s-loogika-n3 is hypernym for s-süllogistika-n1 but s-süllogistika-n1 is not hyponym for s-loogika-n3\n",
      "s-glükokortikoid-n1 is hypernym for s-kortisoon-n1 but s-kortisoon-n1 is not hyponym for s-glükokortikoid-n1\n",
      "s-aeroobika-n1 is hypernym for s-veeaeroobika-n1 but s-veeaeroobika-n1 is not hyponym for s-aeroobika-n1\n",
      "s-turgid-n1 is hypernym for s-kumõkk-n1 but s-kumõkk-n1 is not hyponym for s-turgid-n1\n",
      "s-mängija-n5 is hypernym for s-vahetusmängija-n1 but s-vahetusmängija-n1 is not hyponym for s-mängija-n5\n",
      "s-sooritama-v2 is hypernym for s-disagregeerima-v1 but s-disagregeerima-v1 is not hyponym for s-sooritama-v2\n",
      "s-saama-v7 is hypernym for s-kogenema-v1 but s-kogenema-v1 is not hyponym for s-saama-v7\n",
      "s-kõnetehnoloogia-n1 is hypernym for s-kõnesüntees-n1 but s-kõnesüntees-n1 is not hyponym for s-kõnetehnoloogia-n1\n",
      "s-antant-n1 is hypernym for s-liitlasriik-n1 but s-liitlasriik-n1 is not hyponym for s-antant-n1\n",
      "s-lääneslaavid-n1 is hypernym for s-tšehh-n1 but s-tšehh-n1 is not hyponym for s-lääneslaavid-n1\n",
      "s-lääneslaavid-n1 is hypernym for s-poolakas-n1 but s-poolakas-n1 is not hyponym for s-lääneslaavid-n1\n",
      "s-lõunaslaavid-n1 is hypernym for s-makedoonlane-n1 but s-makedoonlane-n1 is not hyponym for s-lõunaslaavid-n1\n",
      "s-lõunaslaavid-n1 is hypernym for s-bulgaarlane-n1 but s-bulgaarlane-n1 is not hyponym for s-lõunaslaavid-n1\n",
      "s-lõunaslaavid-n1 is hypernym for s-serblane-n1 but s-serblane-n1 is not hyponym for s-lõunaslaavid-n1\n",
      "s-lõunaslaavid-n1 is hypernym for s-horvaat-n1 but s-horvaat-n1 is not hyponym for s-lõunaslaavid-n1\n",
      "s-idaslaavid-n1 is hypernym for s-valgevenelane-n1 but s-valgevenelane-n1 is not hyponym for s-idaslaavid-n1\n",
      "s-lääneslaavid-n1 is hypernym for s-slovakk-n1 but s-slovakk-n1 is not hyponym for s-lääneslaavid-n1\n",
      "s-idaslaavid-n1 is hypernym for s-hoholl-n1 but s-hoholl-n1 is not hyponym for s-idaslaavid-n1\n",
      "s-lõunaslaavid-n1 is hypernym for s-sloveen-n1 but s-sloveen-n1 is not hyponym for s-lõunaslaavid-n1\n",
      "s-puhastama-v1 is hypernym for s-dekapeerima-v1 but s-dekapeerima-v1 is not hyponym for s-puhastama-v1\n",
      "s-aine-n1 is hypernym for s-desinfektsiooniaine-n1 but s-desinfektsiooniaine-n1 is not hyponym for s-aine-n1\n",
      "s-turgid-n1 is hypernym for s-usbekk-n1 but s-usbekk-n1 is not hyponym for s-turgid-n1\n",
      "s-turgid-n1 is hypernym for s-tatarlane-n1 but s-tatarlane-n1 is not hyponym for s-turgid-n1\n",
      "s-turgid-n1 is hypernym for s-turkmeen-n1 but s-turkmeen-n1 is not hyponym for s-turgid-n1\n",
      "s-turgid-n1 is hypernym for s-osman-n1 but s-osman-n1 is not hyponym for s-turgid-n1\n",
      "s-turgid-n1 is hypernym for s-nogai-n1 but s-nogai-n1 is not hyponym for s-turgid-n1\n",
      "s-turgid-n1 is hypernym for s-karakalpakk-n1 but s-karakalpakk-n1 is not hyponym for s-turgid-n1\n",
      "s-turgid-n1 is hypernym for s-dolgaan-n1 but s-dolgaan-n1 is not hyponym for s-turgid-n1\n",
      "s-turgid-n1 is hypernym for s-balkaar-n1 but s-balkaar-n1 is not hyponym for s-turgid-n1\n",
      "s-turgid-n1 is hypernym for s-altailane-n1 but s-altailane-n1 is not hyponym for s-turgid-n1\n",
      "s-turgid-n1 is hypernym for s-tuva-n1 but s-tuva-n1 is not hyponym for s-turgid-n1\n",
      "s-juhtlind-n1 is hypernym for s-juhthani-n1 but s-juhthani-n1 is not hyponym for s-juhtlind-n1\n",
      "s-kooliraamat-n1 is hypernym for s-digitaalne_õpik-n1 but s-digitaalne_õpik-n1 is not hyponym for s-kooliraamat-n1\n",
      "s-rõmm-n1 is hypernym for s-kontsentraat-n2 but s-kontsentraat-n2 is not hyponym for s-rõmm-n1\n"
     ]
    }
   ],
   "source": [
    "for key, values in dict_hyponyms.items():\n",
    "    for hyp in values:\n",
    "        if hyp not in dict_hypernyms or key not in dict_hypernyms[hyp]:\n",
    "            hyp_tuple = None\n",
    "            key_tuple = None\n",
    "            for entry in db_entries:\n",
    "                if hyp == entry[0]:\n",
    "                    hyp_tuple = entry[4]\n",
    "                if key == entry[0]:\n",
    "                    key_tuple = entry[4]\n",
    "            print(hyp_tuple, \"is hypernym for\", key_tuple, \"but\", key_tuple, \"is not hyponym for\", hyp_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the 'Wordnet entries' database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_entry_db = os.path.join(db_dir, \"wordnet_entry.db\")\n",
    "wn_entry_name = \"wordnet_entry\"\n",
    "wn_entry_create = \"CREATE TABLE IF NOT EXISTS wordnet_entry(id INT, literal TEXT, pos TEXT, sense INT, synset_name TEXT, estwn_id TEXT, is_name INT)\"\n",
    "wn_entry_insert = \"insert into wordnet_entry(id, literal, pos, sense, synset_name, estwn_id, is_name) values (?,?,?,?,?,?,?)\"\n",
    "create_database(wn_entry_db, wn_entry_name, wn_entry_create, wn_entry_insert, db_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the 'Wordnet relations' database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_relation_db = os.path.join(db_dir, \"wordnet_relation.db\")\n",
    "wn_relation_name = \"wordnet_relation\"\n",
    "wn_relation_create = \"CREATE TABLE IF NOT EXISTS wordnet_relation(start_vertex INT, end_vertex INT, relation TEXT)\"\n",
    "wn_relation_insert = \"insert into wordnet_relation(start_vertex, end_vertex, relation) values (?,?,?)\"\n",
    "create_database(wn_relation_db, wn_relation_name, wn_relation_create, wn_relation_insert, db_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Creating the 'Wordnet definitions' database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wn_definitions_db = os.path.join(db_dir, \"wordnet_definition.db\")\n",
    "wn_definitions_name = \"wordnet_definition\"\n",
    "wn_definitions_create = \"CREATE TABLE IF NOT EXISTS wordnet_definition(synset_name TEXT, definition TEXT)\"\n",
    "wn_definitions_insert = \"insert into wordnet_definition(synset_name, definition) values (?,?)\"\n",
    "create_database(wn_definitions_db, wn_definitions_name, wn_definitions_create, wn_definitions_insert, db_definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the 'Wordnet examples' database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_examples_db = os.path.join(db_dir, \"wordnet_example.db\")\n",
    "wn_examples_name = \"wordnet_example\"\n",
    "wn_examples_create = \"CREATE TABLE IF NOT EXISTS wordnet_example(synset_name TEXT, example TEXT)\"\n",
    "wn_examples_insert = \"insert into wordnet_example(synset_name, example) values (?,?)\"\n",
    "create_database(wn_examples_db, wn_examples_name, wn_examples_create, wn_examples_insert, db_examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
