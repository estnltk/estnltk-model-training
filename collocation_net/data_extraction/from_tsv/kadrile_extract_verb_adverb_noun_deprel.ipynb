{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} pygraphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ülesande püstitus\n",
    "\n",
    "Proovime lihtsamat ülesannet. Vaata kuidas verb + määrsõna teiste nimisõnadega kombineerub\n",
    "    \n",
    "     kirjutas lepingule alla \n",
    "\n",
    "Ka siin tekib sul palju jama, sest sul võivad olla aja ja koha määrused sees, aga  proovi eraldada andmeid kujul\n",
    "\n",
    "kirjutas alla -->{lepingule:obl, jaanuaris:obl}\n",
    "kirjutas alla -->{dokumedile:obl}\n",
    "\n",
    "st jäta alles kõik lapstipud mis pole alus ja märgi ära ka vastav deprel annotatsioon\n",
    "\n",
    "Äkki õnnestub selle peale vaadates  osa ebaolulistest määrustest välja lõigata ja see subcat seos sealt välja õngitseda  \n",
    "\n",
    "\n",
    "## Täpsustus\n",
    "\n",
    "Võtame sisse ainult compound:prt verbi alluvad, kui seda pole, siis läheb verb \"üksi kirja\"\n",
    "\n",
    "Võtame sisse nimisõnad, asesõnad, pärisnimed ja nende käänded\n",
    "\n",
    "Lisame info, kas verb oli eitusega (feat sisaldab neg väärtust). Kui on eitav, siis muudame verbi kujule \"ei VERB\"\n",
    "\n",
    "\"alus\" ei jää välja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vajalikud teegid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#import etnltk\n",
    "#from estnltk.storage.postgres import PostgresStorage\n",
    "\n",
    "import networkx as nx\n",
    "import sqlite3\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abifunktsioonid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graphFunctions:\n",
    "\n",
    "    # kahe listi ühisosa \n",
    "    def intersection(a, b):\n",
    "        return list(set(a).intersection(b))\n",
    "\n",
    "    # tipu leidmine atribuudi väärtuse järgi\n",
    "    def get_nodes_by_attributes(G,  attrname, attrvalue ):\n",
    "        nodes = defaultdict(list)\n",
    "        {nodes[v].append(k) for k, v in nx.get_node_attributes(G,attrname).items()}\n",
    "        if attrvalue in nodes:\n",
    "            return dict(nodes)[attrvalue]\n",
    "        return []\n",
    "\n",
    "    # graafi joonistamine \n",
    "    # tipp - lemma\n",
    "    # serv - deprel\n",
    "    def drawGraph(G):\n",
    "        # joonise suurus, et enamik puudest ära mahuks\n",
    "        plt.rcParams[\"figure.figsize\"] = (18.5, 10.5)\n",
    "        pos = graphviz_layout(G, prog='dot')\n",
    "        labels = nx.get_node_attributes(G, 'lemma')\n",
    "        nx.draw(G, pos, cmap = plt.get_cmap('jet'),labels=labels, with_labels=True)\n",
    "        edge_labels = nx.get_edge_attributes(G, 'deprel')\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCase(arr):\n",
    "    for attr in arr:\n",
    "        if attr in ('nom' #nimetav\n",
    "                    , 'gen' #omastav\n",
    "                    , 'part' #osastav\n",
    "                    , 'adit' #lyh sisse\n",
    "                    , 'ill' #sisse\n",
    "                    , 'in'#sees\n",
    "                    , 'el' #seest\n",
    "                    , 'ad' #alale\n",
    "                    , 'all' #alal\n",
    "                    , 'abl' #alalt\n",
    "                    , 'tr' #saav\n",
    "                    , 'term' #rajav\n",
    "                    , 'es' #olev\n",
    "                    , 'abes'  #ilma#\n",
    "                    , 'kom' #kaasa#\n",
    "                   ) :\n",
    "            return attr\n",
    "\n",
    "def getNumber(arr):\n",
    "    for attr in arr:\n",
    "        if attr in ('sg' #ainsus\n",
    "                    , 'pl' #mitmus\n",
    "                   ):\n",
    "            return attr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baasi  (sqlite3)  ettevalmistamine täitmiseks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luuakse tabelid:\n",
    "* collections_processed - salvestatakse viimane salvestatud collection Id\n",
    "* {TABLENAME} - tabel kollokatsioonidega\n",
    " * lemma1 text\n",
    " * pos1 text\n",
    " * lemma2 text\n",
    " * pos2 text\n",
    " * deprel text\n",
    " * children json\n",
    " * total integer\n",
    " * example1 text\n",
    " * example2 text\n",
    " * example3 text\n",
    "\n",
    "Luuakse indeksid:\n",
    "* collections_processed_uniq\n",
    "* {TABLENAME}_col1_col2_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepCollDb(TABLENAME, cursor, conn):\n",
    "  \n",
    "\n",
    "    cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS collections_processed\n",
    "                      (tablename text, lastcollection integer);\n",
    "                      \"\"\")\n",
    "    \n",
    "    cursor.execute(f\"\"\"CREATE UNIQUE INDEX IF NOT EXISTS collections_processed_uniq ON collections_processed(tablename);\n",
    "    \"\"\")\n",
    "    \n",
    "    #tsv failist lugemise korral loome tabeli alati nullist\n",
    "    cursor.execute(f\"\"\"\n",
    "        INSERT INTO collections_processed VALUES (?,?)\n",
    "        ON CONFLICT(tablename) DO UPDATE SET lastcollection=?;\"\"\", (TABLENAME, 0, 0,) )\n",
    "    \n",
    "    cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS {TABLENAME}\n",
    "                      (lemma1 text, pos1 text, lemma2 text, pos2 text, deprel text, children json, total integer, example1 text, example2 text, example3 text);\n",
    "                      \"\"\")\n",
    "\n",
    "\n",
    "    cursor.execute(f\"\"\"DELETE FROM {TABLENAME};\"\"\")\n",
    "    \n",
    "    INDEXNAME = f'col1_col2_unique'\n",
    "    \n",
    "    cursor.execute(f\"\"\"CREATE UNIQUE INDEX IF NOT EXISTS {INDEXNAME}\n",
    "        ON {TABLENAME}(lemma1, pos1, lemma2, pos2, deprel, children);\n",
    "        \"\"\")\n",
    "\n",
    "   \n",
    "    conn.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tulemuse salvestamine baasi (sqlite3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCollToDb(TABLENAME, cursor, conn, collocations, examples, lastcollection):\n",
    "   \n",
    "    sqlColls = []\n",
    "   \n",
    "    \n",
    "    for key in collocations.keys():\n",
    "        #print ('key', key)\n",
    "        #save key in parents or update count, get rowid\n",
    "        #lemma1 text, pos1 text, lemma2 text, pos2 text, total integer, deprel\n",
    "     \n",
    "        example1 = None\n",
    "        example2 = None\n",
    "        example3 = None\n",
    "\n",
    "        if len(examples[key])>0:\n",
    "            example1 = examples[key][0]\n",
    "        if len(examples[key])>1:\n",
    "            example2 = examples[key][1]\n",
    "        if len(examples[key])>2:\n",
    "            example3 = examples[key][2]\n",
    "        \n",
    "        lemma1 = key[0]\n",
    "        pos1 = key[2]\n",
    "        lemma2 = key[1]\n",
    "        pos2 = key[3]\n",
    "        deprel = key[4]\n",
    "        children = key[5]\n",
    "        \n",
    "        sqlColls.append( (lemma1, pos1, lemma2,  pos2, deprel, children, collocations[key], example1, example2, example3 , collocations[key], example1, example2,example3,) )\n",
    "\n",
    "    cursor.executemany(f\"\"\"\n",
    "        INSERT INTO {TABLENAME} VALUES (?,?,?,?,?,?,?,?,?,?)\n",
    "            ON CONFLICT(lemma1, pos1, lemma2, pos2, deprel, children)\n",
    "            DO UPDATE SET total=total+? \n",
    "            ,  example1= ?\n",
    "            ,  example2= ?\n",
    "            ,  example3= ?\n",
    "            ;\"\"\", sqlColls)\n",
    "\n",
    "    cursor.execute(f\"\"\"\n",
    "        INSERT INTO collections_processed VALUES (?,?)\n",
    "        ON CONFLICT(tablename) DO UPDATE SET lastcollection=?;\"\"\", (TABLENAME, lastcollection, lastcollection,) )\n",
    "    \n",
    "    conn.commit()\n",
    "    eprint(f'andmebaasi salvestatud kollokatsioonid kollektsioonidest: 0 - {lastcollection}' )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  sõltuvuste leidmine \n",
    "\n",
    "* Kõik kolmikud, kus \n",
    "    * ülemus on tipp1 sõnaliigiga V ja alluv on tipp2 deprel compound:prt\n",
    "    * ülemusel tipp1 on nimisõna/asesõna/pärisnimi (S/P/H) alluv, selle deprel ja kääne \n",
    "\n",
    "#### Näide 1\n",
    "\n",
    "\n",
    "saama\tV\tvälja\tD\tcompound:prt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_verb_adverb_custom_deprel(G, collocations, examples, sentence):\n",
    "   \n",
    "    \n",
    "    #graphFunctions.drawGraph(G)\n",
    "    #print (sentence)\n",
    "    # lyhim tee tippude vahel\n",
    "    path = nx.all_pairs_shortest_path_length(G)\n",
    "    path_reversed = nx.all_pairs_shortest_path_length(G.reverse())\n",
    "\n",
    "    # kauguste maatriksid\n",
    "    dpath = {x[0]:x[1] for x in path}\n",
    "    dpath_reversed = {x[0]:x[1] for x in path}\n",
    "\n",
    "    #eprint('eraldame tipud vajalike parameetritega ')\n",
    "    verbnodes = graphFunctions.get_nodes_by_attributes(G, attrname = 'pos', attrvalue = 'V')\n",
    "    \n",
    "    #compound tipid\n",
    "    compoundnodes = graphFunctions.get_nodes_by_attributes(G, attrname = 'deprel', attrvalue = 'compound:prt')\n",
    "    \n",
    "    nouns = graphFunctions.get_nodes_by_attributes(G, attrname = 'pos', attrvalue = 'S') + graphFunctions.get_nodes_by_attributes(G, attrname = 'pos', attrvalue = 'H')+ graphFunctions.get_nodes_by_attributes(G, attrname = 'pos', attrvalue = 'P')\n",
    "    \n",
    "    cases = graphFunctions.get_nodes_by_attributes(G, attrname = 'deprel', attrvalue = 'case')\n",
    "    \n",
    "    #print ('cases', cases)\n",
    "    #liigume tegusõnade kaupa\n",
    "    \n",
    "    #see näide on vananenud, kogume ainult compound:prt tippe\n",
    "    #tekitame kollokatsioonipaari sellise:\n",
    "    ##Isa hülgas Jimmy kohe pärast tema sündi .\n",
    "    ## hülgama -[ advmod ]-> kohe sellel on id ja count?\n",
    "    ## children: obj Jimmy H nom\n",
    "    ## children: obj sünd S com pärast D\n",
    "    #    #-[ obj ]-> Jimmy nom\n",
    "\n",
    "     #   #-[ obl ]-> sündi com -[ case ]-> pärast\n",
    "    \n",
    "\n",
    "    for verb in verbnodes:\n",
    "        #print (G.nodes[verb]['lemma'])\n",
    "        childcompounds = []\n",
    "        childnouns = []\n",
    "        for compound in compoundnodes:\n",
    "            #kui on vahetu alluv\n",
    "            if compound in dpath[verb] and dpath[verb][compound]==1: childcompounds.append(compound)\n",
    "        \n",
    "        \n",
    "        # kui ei ole compounds alluvaid, siis l'heb verb üksi kirja\n",
    "        # if not len(childcompounds):\n",
    "        #     #print ('pole compounds')\n",
    "        #    key = ( 'ei ' + G.nodes[verb]['lemma'] if 'neg' in G.nodes[verb]['feat'] else G.nodes[verb]['lemma'], '', G.nodes[verb]['pos'], '', '', json.dumps({}) ,)\n",
    "  \n",
    "            \n",
    "        for noun in nouns:\n",
    "            #kui on vahetu alluv        \n",
    "            if noun in dpath[verb] and dpath[verb][noun]==1: childnouns.append(noun)\n",
    "\n",
    "        #kui ei ole compound adverbe, siis lisame ühe rea, kus kõik nimisõnad sees käänetega\n",
    "        if not len(childcompounds):\n",
    "            childcompounds.append('-')\n",
    "            \n",
    "        for compound in childcompounds:\n",
    "\n",
    "            #print ('collocation', key)\n",
    "            children = []\n",
    "            for noun in childnouns:\n",
    "                child = {\n",
    "                        'lemma': G.nodes[noun]['lemma']\n",
    "                        , 'pos': G.nodes[noun]['pos']\n",
    "                        , 'case':getCase(G.nodes[noun]['feat'])\n",
    "                        , 'number':getNumber(G.nodes[noun]['feat'])\n",
    "                        , 'deprel': G.nodes[noun]['deprel']\n",
    "                        , 'case_children':[]\n",
    "                        }\n",
    "                #print (\"\\t\" '-[',G.nodes[noun]['deprel'],']->', G.nodes[noun]['text'], getCase(G.nodes[noun]['feat']))\n",
    "                #print ()\n",
    "                childcases = []\n",
    "                for case in cases:\n",
    "                    if case in dpath[noun] and dpath[noun][case]==1: \n",
    "                        child['case_children'].append( (G.nodes[case]['lemma'], G.nodes[case]['pos'], ) )\n",
    "                children.append(child)\n",
    "            \n",
    "\n",
    "            key = ( \n",
    "                    'ei ' + G.nodes[verb]['lemma'] if 'neg' in G.nodes[verb]['feat']  else G.nodes[verb]['lemma']\n",
    "                    , '' if compound == '-' else G.nodes[compound]['lemma']\n",
    "                    , G.nodes[verb]['pos']\n",
    "                    , '' if compound == '-' else G.nodes[compound]['pos']\n",
    "                    , '' if compound == '-' else G.nodes[compound]['deprel']\n",
    "                    , json.dumps(children) ,)\n",
    "\n",
    " \n",
    "            #print (key) \n",
    "            if not key in collocations:\n",
    "                collocations[key] = 0\n",
    "            if not key in examples:\n",
    "                examples[key] = []\n",
    "            collocations[key] += 1\n",
    "            examples[key].append(sentence)\n",
    "            if len(examples[key]) > 3:\n",
    "                del(examples[key][random.randint(0, 2)])\n",
    "        \n",
    "                \n",
    "            \n",
    "    \n",
    "    return (collocations, examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muutujad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Korpuse TSV fail**\n",
    "\n",
    "Lähte TSV-faili ja DB tabeli nimi, kuhu tulemus salvestatakse (TSV genereerimise kood on failis **collect_texts_db_tsv.ipynb**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collectionName =  'koondkorpus_base_subset_of_5000_v2' \n",
    "collectionName =  'koondkorpus_base_subset_of_5000_v2_short'\n",
    "#collectionName =  'koondkorpus_base_subset_of_5000_v2' \n",
    "corporafile = f'{collectionName}.tsv'\n",
    "#corporafile = f'kadrile_test.tsv'\n",
    "#corporafile = '/Volumes/Selena/Kollokatsioonid/koondkorpus/koondkorpus_base_v2_20220216.tsv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kollokatsiooni tüüp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = 'kadrile_verb_adverb_noun_deprel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tabeli nimi andmebaasis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLENAME = f'all_in_one'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kollektsioonide arv**, mille kaupa salvestatakse vahepealne tulemus andmebaasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Andmebaasi loomine ja ette valmistamine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(f\"{TYPE}_collocations.db\") #\n",
    "conn.row_factory = sqlite3.Row\n",
    "cursor = conn.cursor()\n",
    "prepCollDb(TABLENAME, cursor, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kollokatsioonide leidmine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andmed loetakse TSV failist. Igast lausest tehakse *networkx* suunatud graaf. Graafi servadeks on süntaksipuu \n",
    "head->child seos. Tippudeks on lause sõnad. \n",
    "Tipu omadused on:\n",
    "* *id* - sõna id\n",
    "* deprel\n",
    "* lemma\n",
    "* pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unsaved = 0\n",
    "collocations = {}\n",
    "examples = {}\n",
    "word_id = 0\n",
    "count = 0\n",
    "current_sentence = []\n",
    "\n",
    "#\n",
    "G = None\n",
    "colId = None\n",
    "prevCol = 0\n",
    "f = TSV.split(\"\\n\")\n",
    "\n",
    "with open(corporafile) as f:\n",
    "\n",
    "    for line in f:\n",
    "        count +=1\n",
    "        line = line.strip(\"\\n\")\n",
    "        row = line.split('\\t')\n",
    "        if not len(row) == 9:\n",
    "            continue\n",
    "        data = {}\n",
    "        prevCol = colId\n",
    "        (colId, data['start'], data['id'], data['text'], data['lemma'], data['upostag'], data['deprel'], data['head'], data['feat']) = row\n",
    "        colId = int(colId)\n",
    "        for k in ('id', 'start', 'head'):\n",
    "            data[k] = int(data[k])\n",
    "\n",
    "        word_id +=1\n",
    "        #sentence start\n",
    "        if data['id'] == 1:\n",
    "            if not G==None:\n",
    "                current_sentence_text = ' '.join([s['text'] for s in current_sentence])\n",
    "                (collocations, examples) = extract_verb_adverb_custom_deprel(G, collocations, examples, current_sentence_text)\n",
    "                unsaved = 1\n",
    "            current_sentence = []\n",
    "            #count+=1\n",
    "            word_id +=1\n",
    "            G = nx.DiGraph()\n",
    "            if not prevCol ==colId and not colId==0 and not colId%BATCHSIZE:\n",
    "                saveCollToDb(TABLENAME, cursor, conn, collocations, examples, colId)\n",
    "                collocations = {}\n",
    "                unsaved = 0\n",
    "\n",
    "\n",
    "        #paneme graafi kokku\n",
    "        G.add_node(word_id, id=data['id'], text=data['text'],  lemma=data['lemma'], pos=data['upostag'], deprel=data['deprel'], feat=data['feat'].split('||'))\n",
    "        G.add_edge(word_id - data['id'] + data['head'], word_id, deprel = data['deprel'])\n",
    "        current_sentence.append(data)\n",
    "        \n",
    "        \n",
    "    \n",
    "    #viimane lause\n",
    "    current_sentence_text = ' '.join([s['text'] for s in current_sentence])\n",
    "    (collocations, examples) = extract_verb_adverb_custom_deprel(G, collocations, examples, current_sentence_text)\n",
    "    saveCollToDb(TABLENAME, cursor, conn, collocations, examples, colId)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Andmebaasi indeksite lisamine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tehakse kõige viimasena, et andmete sisestamine andmebaasi oleks kiirem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indexesQ = [\n",
    "    f'CREATE INDEX IF NOT EXISTS \"deprel\" ON {TABLENAME} (\"deprel\");'\n",
    "    , f'CREATE INDEX IF NOT EXISTS \"lemma1\" ON {TABLENAME} (\"lemma1\");'\n",
    "    , f'CREATE INDEX IF NOT EXISTS \"lemma2\" ON {TABLENAME}(\"lemma2\");'\n",
    "    , f'CREATE INDEX IF NOT EXISTS \"pos1\" ON {TABLENAME} (\"pos1\");'\n",
    "    , f'CREATE INDEX IF NOT EXISTS \"pos2\" ON {TABLENAME} (\"pos2\");'\n",
    "    , f'CREATE INDEX IF NOT EXISTS \"total\" ON {TABLENAME} (\"total\" DESC);']\n",
    "\n",
    "for q in indexesQ: cursor.execute(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"SELECT count(*) FROM {TABLENAME}\")\n",
    "all_collocations = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info koondamine\n",
    "\n",
    "Avame uue andmebaasi.\n",
    "Loome tabeli koondatud.\n",
    "Loeme vanast tabelist batchide kaupa.\n",
    "Eemaldame ebavajaliku info, salvestame koondatud read uude tabelisse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn2 = sqlite3.connect(f\"{TYPE}_generalized.db\") \n",
    "cursor2 = conn2.cursor()\n",
    "TABLE = 'all_generalized'\n",
    "prepCollDb(TABLE, cursor2, conn2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"SELECT count(*) AS total FROM all_in_one\")\n",
    "res = cursor.fetchall()\n",
    "TOTALROWS = res[0]['total']\n",
    "\n",
    "BATCHSIZE = 100000\n",
    "DONE = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCHSIZE = 1000\n",
    "DONE = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "collocations2 = {}\n",
    "examples2={}\n",
    "offset = 1\n",
    "\n",
    "print (f'TOTAL ROWS: {TOTALROWS}')\n",
    "while offset <= TOTALROWS:\n",
    "    query = f\"SELECT *   FROM all_in_one   LIMIT {BATCHSIZE} OFFSET {offset} \"\n",
    "    cursor.execute(query)\n",
    "    print (query)\n",
    "    count = 0\n",
    "    for r in cursor.fetchall():\n",
    "        count+=1\n",
    "        new_children = []\n",
    "        children = r['children']\n",
    "        try:\n",
    "            children= json.loads(children)\n",
    "        except:\n",
    "            eprint (f'JSON parsing error on line {count}')\n",
    "            children = []\n",
    "\n",
    "\n",
    "        for ch in children:\n",
    "            #print (ch)\n",
    "            gen_child = []\n",
    "            for p in ['case','number','deprel']:\n",
    "                gen_child.append(ch[p])\n",
    "            #print(gen_child)\n",
    "            new_children.append(gen_child)\n",
    "\n",
    "\n",
    "        key = ( r['lemma1']\n",
    "                , r['lemma2']\n",
    "                , r['pos1']\n",
    "                , r['pos2']\n",
    "                , r['deprel']\n",
    "                , json.dumps(new_children) ,)\n",
    "        if key in collocations2:\n",
    "            collocations2[key] += r['total']\n",
    "        else:\n",
    "            collocations2[key] = r['total']\n",
    "            examples2[key] = []\n",
    "        if r['example1']:\n",
    "            examples2[key].append(r['example1'])\n",
    "        if r['example2']:\n",
    "            examples2[key].append(r['example2'])\n",
    "        if r['example3']:\n",
    "            examples2[key].append(r['example3'])\n",
    "        examples2[key] = list(set(examples2[key]))\n",
    "      \n",
    "\n",
    "    \n",
    "    saveCollToDb(TABLE, cursor2, conn2, collocations2, examples2, offset + BATCHSIZE)\n",
    "    collocations = {}\n",
    "    unsaved = 0\n",
    "    \n",
    "    offset = offset + BATCHSIZE\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lihtverbid koondatud\n",
    "\n",
    "* Koondatud info baasist heidame välja morf info, jätame ainult deprel \n",
    "* Andmed loeme TSV-failist ja salvestame TSV faili\n",
    "* Kustutame deprel 'parataxis' ja 'conj' - need ei sõltu verbist\n",
    "* Kui deprel on tühjad, siis kirjutame 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBDataFilename = '/Volumes/Selena/Kollokatsioonid/kadrile_verb_adverb_compound_children/kadrile_verb_adverb_noun_deprel_generalized_20221014.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import pathname2url\n",
    "\n",
    "try:\n",
    "    dburi = 'file:{}?mode=rw'.format(pathname2url(DBDataFilename))\n",
    "    conn3 = sqlite3.connect(dburi, uri=True)\n",
    "    conn3.row_factory = sqlite3.Row\n",
    "    cursor3 = conn3.cursor()\n",
    "    print ('Connected')\n",
    "except sqlite3.OperationalError:\n",
    "    # handle missing database case\n",
    "    print (f'DB file not found')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3647171"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor3.execute(f\"SELECT count(*) AS total FROM all_generalized  WHERE lemma2 = ''\")\n",
    "res = cursor3.fetchall()\n",
    "\n",
    "BATCHSIZE = 500000\n",
    "TOTALROWS = res[0]['total']\n",
    "\n",
    "TOTALROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL ROWS: 3647171\n",
      "SELECT *   FROM all_generalized WHERE lemma2 = '' LIMIT 500000 OFFSET 1 \n",
      "SELECT *   FROM all_generalized WHERE lemma2 = '' LIMIT 500000 OFFSET 500001 \n",
      "SELECT *   FROM all_generalized WHERE lemma2 = '' LIMIT 500000 OFFSET 1000001 \n",
      "SELECT *   FROM all_generalized WHERE lemma2 = '' LIMIT 500000 OFFSET 1500001 \n",
      "SELECT *   FROM all_generalized WHERE lemma2 = '' LIMIT 500000 OFFSET 2000001 \n",
      "SELECT *   FROM all_generalized WHERE lemma2 = '' LIMIT 500000 OFFSET 2500001 \n",
      "SELECT *   FROM all_generalized WHERE lemma2 = '' LIMIT 500000 OFFSET 3000001 \n",
      "SELECT *   FROM all_generalized WHERE lemma2 = '' LIMIT 500000 OFFSET 3500001 \n"
     ]
    }
   ],
   "source": [
    "collocations3 = {}\n",
    "examples3={}\n",
    "offset = 1\n",
    "\n",
    "print (f'TOTAL ROWS: {TOTALROWS}')\n",
    "while offset <= TOTALROWS:\n",
    "    query = f\"SELECT *   FROM all_generalized WHERE lemma2 = '' LIMIT {BATCHSIZE} OFFSET {offset} \"\n",
    "    cursor3.execute(query)\n",
    "    print (query)\n",
    "    count = 0\n",
    "    for r in cursor3.fetchall():\n",
    "        count+=1\n",
    "        new_children = []\n",
    "        children = r['children']\n",
    "        try:\n",
    "            children= json.loads(children)\n",
    "        except:\n",
    "            eprint (f'JSON parsing error on line {count}')\n",
    "            children = []\n",
    "\n",
    "        for ch in children:\n",
    "            new_children.append(ch[2])\n",
    "        if 'parataxis' in new_children:\n",
    "            new_children.remove('parataxis')\n",
    "        if 'conj' in new_children:\n",
    "            new_children.remove('conj')\n",
    "        if not len(new_children): new_children.append('NULL')\n",
    "\n",
    "        key = ( r['lemma1']\n",
    "                , r['pos1']\n",
    "                , ','.join(sorted(new_children)) ,)\n",
    "\n",
    "        if key in collocations3:\n",
    "            collocations3[key] += r['total']\n",
    "        else:\n",
    "            collocations3[key] = r['total']\n",
    "            examples3[key] = []\n",
    "        #if r['example1']:\n",
    "        #    examples3[key].append(r['example1'])\n",
    "        #if r['example2']:\n",
    "        #    examples3[key].append(r['example2'])\n",
    "        #if r['example3']:\n",
    "        #    examples3[key].append(r['example3'])\n",
    "        #examples3[key] = list(set(examples3[key]))\n",
    "      \n",
    "    offset = offset + BATCHSIZE\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultfilename = 'kadrile_lihtverb_deprel_sorted.tsv'\n",
    "import csv\n",
    "\n",
    "with open(resultfilename, 'w', newline='') as f:\n",
    "    csvwriter = csv.writer(f, delimiter='\\t')\n",
    "    for k, v in  sorted(collocations3.items(), key = lambda x: x[1], reverse = True) : \n",
    "        row = list(k)\n",
    "        row.append(v)\n",
    "        \n",
    "        \n",
    "        csvwriter.writerow(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
