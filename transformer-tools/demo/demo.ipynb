{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installime Rust'i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1minfo:\u001b[0m downloading installer\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mprofile set to 'default'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdefault host triple is x86_64-unknown-linux-gnu\n",
      "\u001b[0m\u001b[33m\u001b[0m\u001b[1m\u001b[33mwarning: \u001b[0mUpdating existing toolchain, profile choice will be ignored\n",
      "\u001b[0m\u001b[1minfo: \u001b[0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
      "\u001b[0m\u001b[1minfo: \u001b[0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
      "\n",
      "  \u001b[0m\u001b[1mstable-x86_64-unknown-linux-gnu unchanged\u001b[0m - rustc 1.76.0 (07dca489a 2024-02-04)\n",
      "\n",
      "\u001b[0m\u001b[1m\n",
      "Rust is installed now. Great!\n",
      "\u001b[0m\n",
      "To get started you may need to restart your current shell.\n",
      "This would reload your \u001b[0m\u001b[1mPATH\u001b[0m environment variable to include\n",
      "Cargo's bin directory ($HOME/.cargo/bin).\n",
      "\n",
      "To configure your current shell, you need to source\n",
      "the corresponding \u001b[0m\u001b[1menv\u001b[0m file under $HOME/.cargo.\n",
      "\n",
      "This is usually done by running one of the following (note the leading DOT):\n",
      ". \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\n",
      "source \"$HOME/.cargo/env.fish\"  # For fish\n"
     ]
    }
   ],
   "source": [
    "! cd ~ ; curl https://sh.rustup.rs -sSf | sh -s -- -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Täiendame keskkonnamuutujat PATH\n",
    "\n",
    "Selle peab kuskile `~/.bashrc` või `~/.zshrc` vms faili panema ja siis aktiviseerima. \n",
    "\n",
    "Kui Rust ei ole keskkonnamuutuja kaudu leitav, siis hiljem virtuaalkeskkonna tegemine läheb pekki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export PATH=\"$HOME/.cargo/bin:$PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tõmbame sortsud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/home/tarmo/git2/huggingface_github/huggingface_tokenizers_github'...\n",
      "remote: Enumerating objects: 18363, done.\u001b[K\n",
      "remote: Counting objects: 100% (231/231), done.\u001b[K\n",
      "remote: Compressing objects: 100% (154/154), done.\u001b[K\n",
      "remote: Total 18363 (delta 109), reused 144 (delta 75), pack-reused 18132\u001b[K\n",
      "Receiving objects: 100% (18363/18363), 9.82 MiB | 16.46 MiB/s, done.\n",
      "Resolving deltas: 100% (12181/12181), done.\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p ~/git/huggingface_github && git clone https://github.com/huggingface/tokenizers ~/git/huggingface_github/huggingface_tokenizers_github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tekitame virtuaalkeskkonna\n",
    "\n",
    "Tekitame ta selle notebookiga samasse kataloogi, antud juhul on see \n",
    "`~/git/huggingface_github/estnltk_model_training_github/transformer-tools/demo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/tarmo/git/huggingface_github/huggingface_tokenizers_github/bindings/python\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.16.4\n",
      "  Using cached huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "Collecting tqdm>=4.42.1\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting packaging>=20.9\n",
      "  Using cached packaging-24.0-py3-none-any.whl (53 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building editable for tokenizers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tokenizers: filename=tokenizers-0.15.3.dev0-cp310-cp310-linux_x86_64.whl size=3502 sha256=1f3a51485938203357af07aa6dd1eddbae42a3ef5fc54f0f742a846ebe107d2a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-25jx0t3j/wheels/61/cb/fe/bdaca14d9769abe1f07c952f3e22930b11c36788efc4e6acf4\n",
      "Successfully built tokenizers\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, pyyaml, packaging, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers\n",
      "Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2024.3.1 huggingface-hub-0.21.4 idna-3.6 packaging-24.0 pyyaml-6.0.1 requests-2.31.0 tokenizers-0.15.3.dev0 tqdm-4.66.2 typing-extensions-4.10.0 urllib3-2.2.1\n"
     ]
    }
   ],
   "source": [
    "! python3 -m venv ~/git/huggingface_github/estnltk_model_training_github/transformer-tools/demo/env_tkn/\n",
    "! ~/git/huggingface_github/estnltk_model_training_github/transformer-tools/demo/env_tkn/bin/pip3 install -e ~/git/huggingface_github/huggingface_tokenizers_github/bindings/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jooksutame demoprogrammi\n",
    "\n",
    "Tilluke demokorpus kataloogis `tv_tmp_dataset`. Põhimõtteliselt peaks seda nüüd jooksutama eelmises sammus tehtud virt keskkonnas, kuidas iganes...\n",
    "\n",
    "Sisendfailid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mees peeti kinni.\n",
      "Karu tuli koopast, vahtis kuud.\n",
      "Elas metsas mutionu, keset\n",
      "kuuski noori-vanu.\n",
      "Põdral maja metsa sees,\n",
      "väiksest aknast välja vaatas."
     ]
    }
   ],
   "source": [
    "! cat ~/git/huggingface_github/estnltk_model_training_github/transformer-tools/demo/tv_tmp_dataset/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treenime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K[00:00:00] Tokenize words                 ██████████████████ 26       /       26[00:00:00] Tokenize words                 ██████████████████ 0        /        0\n",
      "\u001b[2K[00:00:00] Count pairs                    ██████████████████ 26       /       26\n",
      "\u001b[2K[00:00:00] Compute merges                 ██████████████████ 14       /       14\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p ~/git/huggingface_github/estnltk_model_training_github/transformer-tools/demo/tv_tmp_out/\n",
    "! cd ~/git/huggingface_github/estnltk_model_training_github/transformer-tools/demo/ && \\\n",
    "    ~/git/huggingface_github/estnltk_model_training_github/transformer-tools/demo/env_tkn/bin/python3 \\\n",
    "        ~/git/huggingface_github/huggingface_tokenizers_github/bindings/python/examples/train_bert_wordpiece.py \\\n",
    "            --files='tv_tmp_dataset/*.txt' \\\n",
    "            --out=tv_tmp_out/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaatame tulemust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      "[UNK]\n",
      "[CLS]\n",
      "[SEP]\n",
      "[MASK]\n",
      ",\n",
      "-\n",
      ".\n",
      "a\n",
      "d\n",
      "e\n",
      "h\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "r\n",
      "s\n",
      "t\n",
      "u\n",
      "v\n",
      "##a\n",
      "##t\n",
      "##s\n",
      "##o\n",
      "##d\n",
      "##r\n",
      "##l\n",
      "##e\n",
      "##u\n",
      "##k\n",
      "##i\n",
      "##n\n",
      "##j\n",
      "##h\n",
      "##p\n",
      "va\n",
      "##as\n",
      "##et\n",
      "##es\n",
      "ku\n",
      "met\n",
      "##ti\n",
      "##oo\n",
      "##ees\n",
      "##nu\n",
      "##ja\n",
      "##ast\n",
      "kuu\n",
      "mets\n"
     ]
    }
   ],
   "source": [
    "! cat ~/git/huggingface_github/estnltk_model_training_github/transformer-tools/demo/tv_tmp_out/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Skripti\n",
    "`~/git/huggingface_github/huggingface_tokenizers_github/bindings/python/examples/train_bert_wordpiece.py`\n",
    "käsurea parameetritest tuleks leksikon etteandmine läbi vedada"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
