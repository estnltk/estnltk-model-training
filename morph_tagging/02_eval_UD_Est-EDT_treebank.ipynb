{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "   1. [**Estonian Universal Dependencies' EDT corpus**](#eesti_ud_edt_korpus)\n",
    "      1. [**Converting UD corpus to Vabamorf format**](#eesti_ud_edt_konverteerimine)\n",
    "      2. [**Creating and preparing the dataset from converted UD corpus**](#teisendatud_ud_andmestiku_loomine_ja_tootlemine)\n",
    "      3. [**Model evaluation on UD corpus**](#mudeli_testimine)\n",
    "      4. [**Vabamorf evaluation on UD corpus**](#vabamorf_ud_korpusel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import pkg_resources\n",
    "import types\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import estnltk\n",
    "import torch\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "from est_ud_utils import load_ud_file_texts_with_corrections, load_ud_file_with_corrections\n",
    "from est_ud_morph_conv import convert_ud_layer_to_reduced_morph_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estnltk==1.7.3\n",
      "evaluate==0.4.2\n",
      "pandas==2.2.2\n",
      "simpletransformers==0.70.1\n",
      "torch==2.4.0\n"
     ]
    }
   ],
   "source": [
    "# Get locally imported modules from current notebook - https://stackoverflow.com/questions/40428931/package-for-listing-version-of-packages-used-in-a-jupyter-notebook - Alex P. Miller\n",
    "def get_imports():\n",
    "\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "\n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system/pip names. Unfortunately,\n",
    "        # there is no systematic way to get pip names from\n",
    "        # a package's imported name. You'll have to add\n",
    "        # exceptions to this list manually!\n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "# The only way I found to get the version of the root package\n",
    "# from only the name of the package is to cross-check the names \n",
    "# of installed packages vs. imported packages\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eesti_ud_edt_korpus'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estonian Universal Dependencies' EDT [corpus](https://github.com/UniversalDependencies/UD_Estonian-EDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eesti_ud_edt_konverteerimine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting UD corpus to Vabamorf format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\"Convert Universal Dependencies' corpus to Vabamorf format\" notebook](https://github.com/estnltk/estnltk-model-training/blob/main/ud_morph_tools/amb_morph_reordering/01_convert_ud_corpus_to_vm.ipynb) was used to convert Estonian UD EDT treebank into Vabamorf format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_no_xpostag_rows():\n",
    "    \"\"\"\n",
    "    Finds rows in the Estonian UD EDT treebank that contain rows where\n",
    "    <code>xpostag == '_'</code>\n",
    "\n",
    "    <i>In file <code>est_ud_utils.py</code> class <code>EstUDCorrectionsRewriter</code> has function <code>rewrite</code>, which has comment: \\n\n",
    "    #72: If <code>xpostag == '_'</code>, then add it based on upostag \\n\n",
    "    But not all xpostag conditions exist in the code as convertion throws an <code>AssertionError</code>.</i>\n",
    "    \"\"\"\n",
    "    no_xpostag_regex = r\"^\\d+\\t\\S+\\t\\S+\\t\\S+\\t_\"\n",
    "    conllu_dir = \"UD_Estonian-EDT-r2.14\"\n",
    "    conllu_files = [\"et_edt-ud-dev.conllu\", \"et_edt-ud-test.conllu\", \"et_edt-ud-train.conllu\"]\n",
    "    for c_file in conllu_files:\n",
    "        print(\"\\n\", c_file, \"\\n\")\n",
    "        with open(file=os.path.join(conllu_dir, c_file), mode=\"r\") as f:\n",
    "            text = f.read()\n",
    "            # Find all matches\n",
    "            matches = re.findall(no_xpostag_regex, text, re.MULTILINE)\n",
    "\n",
    "            # Print the matching rows\n",
    "            for match in matches:\n",
    "                print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ud_to_vabamorf(ud_corpus_dir, output_dir):\n",
    "    \"\"\"Converts Universal Dependencies' (UD) corpus to Vabamorf format\n",
    "\n",
    "    Args:\n",
    "        ud_corpus_dir (str): path to directory containing UD corpus .conllu files\n",
    "        output_dir (str): path to directory, where Vabamorf jsons files will be written\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.isdir( output_dir ):\n",
    "        os.makedirs(output_dir)\n",
    "    assert os.path.isdir( output_dir )\n",
    "\n",
    "    # Load UD corpus' files as EstNLTK Text objects\n",
    "    loaded_texts  = []\n",
    "    ud_layer_name = 'ud_syntax'\n",
    "    for fname in os.listdir( ud_corpus_dir ):\n",
    "        #if 'train' in fname:\n",
    "        #    continue\n",
    "        #if 'dev' in fname:\n",
    "        #    continue\n",
    "        #if 'test' in fname:\n",
    "        #    continue\n",
    "        if fname.endswith('.conllu'):\n",
    "            fpath = os.path.join( ud_corpus_dir, fname )\n",
    "            texts = load_ud_file_texts_with_corrections( fpath, ud_layer_name )\n",
    "            for text in texts:\n",
    "                text.meta['file'] = fname\n",
    "                loaded_texts.append( text )\n",
    "\n",
    "    # Convert UD's morphosyntactic annotations to Vabamorf-like annotations\n",
    "    for tid, text in enumerate(loaded_texts):\n",
    "        convert_ud_layer_to_reduced_morph_layer( text, 'ud_syntax', 'ud_morph_reduced', add_layer=True )\n",
    "        fname = text.meta['file'].replace('.conllu', '_'+('{:03d}'.format(tid))+'.json')\n",
    "        fpath = os.path.join(output_dir, fname)\n",
    "        estnltk.converters.text_to_json(text, file=fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_corpus_dir = \"UD_Estonian-EDT-r2.14\" # UD Corpus location\n",
    "output_dir = 'UD_converted' # Output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    convert_ud_to_vabamorf(ud_corpus_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='teisendatud_ud_andmestiku_loomine_ja_tootlemine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and preparing the dataset from converted UD corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_ud_corpus(jsons, in_dir, tokenizer, csv_filename):\n",
    "    \"\"\"\n",
    "    Creates a new dataset from converted the Estonian UD EDT <a href=\"https://github.com/UniversalDependencies/UD_Estonian-EDT\">corpus</a>. \\n\n",
    "    For each <code>.json</code> file, the following info is gathered:\n",
    "    <ul>\n",
    "        <li><code>sentence_id</code> -- given for each sentence</li>\n",
    "        <li><code>words</code> -- words gathered from text</li>\n",
    "        <li><code>form</code> -- word form notation</li>\n",
    "        <li><code>pos</code> -- part of speech</li>\n",
    "        <li><code>file_prefix</code> -- metadata</li>\n",
    "        <li><code>source</code> -- file name where the text is taken from</li>\n",
    "    </ul>\n",
    "    <a href=\"https://github.com/Filosoft/vabamorf/blob/e6d42371006710175f7ec328c98f90b122930555/doc/tagset.md\">Tables of morphological categories</a> for more information about <code>form</code> and <code>pos</code>.\n",
    "\n",
    "    Args:\n",
    "        jsons (list[str]): List of json files from which to read in the text\n",
    "        in_dir (str): Directory containing list of files (<code>jsons</code>)\n",
    "        tokenizer (str): Use goldstandard (<code>ud_morph_reduced</code>) or Vabamorf tokenization ((<code>morph_analysis</code>))\n",
    "        csv_filename (str): CSV filename where to save the gathered text\n",
    "    \"\"\"\n",
    "    if tokenizer not in {'ud_morph_reduced', 'morph_analysis'}:\n",
    "        raise ValueError(\"create_df_ud_corpus: tokenizer must be one of %r.\" % {'ud_morph_reduced', 'morph_analysis'})\n",
    "\n",
    "    tokens = list()\n",
    "    sentence_id = 0\n",
    "    fieldnames = ['sentence_id', 'words', 'form', 'pos', 'file_prefix', 'source']\n",
    "\n",
    "    print(\"Beginning tokenization file by file. This can take a while.\")\n",
    "    for file_name in jsons:\n",
    "        # print(f\"Beginning to tokenize {file_name}\")\n",
    "        sentence_id = 0\n",
    "\n",
    "        # Tokenization\n",
    "        text = estnltk.converters.json_to_text(file=os.path.join(in_dir, file_name))\n",
    "        if tokenizer == 'morph_analysis':\n",
    "            text.tag_layer('morph_analysis')\n",
    "        file_prefix = text.meta.get('file_prefix')\n",
    "        for sentence in text.sentences:\n",
    "            if tokenizer == 'ud_morph_reduced':\n",
    "                sentence_analysis = sentence.ud_morph_reduced\n",
    "                for text, form, pos in zip(sentence_analysis.text, sentence_analysis.form, sentence_analysis.pos):\n",
    "                    if text:\n",
    "                        tokens.append((sentence_id, text, form[0], pos[0], file_prefix, file_name)) # In case of multiplicity, select the first or index 0\n",
    "            else:\n",
    "                sentence_analysis = sentence.morph_analysis\n",
    "                for text, form, pos in zip(sentence_analysis.text, sentence_analysis.form, sentence_analysis.partofspeech):\n",
    "                    if text:\n",
    "                        tokens.append((sentence_id, text, form[0], pos[0], file_prefix, file_name)) # In case of multiplicity, select the first or index 0\n",
    "            sentence_id += 1\n",
    "        # print(f\"{file_name} tokenized\")\n",
    "\n",
    "    print(\"Tokenization completed successfully\")\n",
    "    print(\"Creating Pandas dataframe\")\n",
    "    df = pd.DataFrame(data=tokens, columns=fieldnames)\n",
    "    df.to_csv(path_or_buf=csv_filename, index=False)\n",
    "    print(f\"Tokenized texts saved to {csv_filename}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df, df_file_name=None):\n",
    "    \"\"\"Finishes dataframe by:\n",
    "    <ul>\n",
    "        <li>filling NaN values in columns <code>form</code> and <code>pos</code> with empty strings;</li>\n",
    "        <li>removing NaN words.</li>\n",
    "    </ul>\n",
    "\n",
    "    Args:\n",
    "        df (pandas.core.frame.DataFrame): Pandas dataframe to clean\n",
    "        df_file_name (str): CSV file name from which dataframe was created\n",
    "    \"\"\"\n",
    "    print(\"Assigning NaN values in columns form and pos with an empty string\")\n",
    "    # NaN values are assigned with an empty string\n",
    "    df['form'] = df['form'].fillna('')\n",
    "    df['pos'] = df['pos'].fillna('')\n",
    "    print(\"Removing NaN words\")\n",
    "    # Removing NaN words\n",
    "    df.dropna(subset=['words'], inplace=True)\n",
    "    if df_file_name:\n",
    "        df.to_csv(path_or_buf=df_file_name, index=False)\n",
    "        print(f\"Modified dataframe saved to {df_file_name}\")\n",
    "    else:\n",
    "        print(\"Dataframe cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New 'labels' column\n",
    "def create_labels_column(df, df_file_name=None):\n",
    "    \"\"\"\n",
    "    Creates a new column <code>labels</code> concatenating the values of columns <code>pos</code> (part of speech) and <code>form</code> (word form notation)\n",
    "\n",
    "    Args:\n",
    "        df (pandas.core.frame.DataFrame): Pandas dataframe to create a new column\n",
    "        df_file_name (str): CSV file name from which dataframe was created\n",
    "    \"\"\"\n",
    "    print(\"Creating column 'labels'\")\n",
    "    df['labels'] = df.apply(lambda row: str(row['form']) + '_' + str(row['pos']) if row['form'] and row['pos'] else str(row['form']) or str(row['pos']), axis=1)\n",
    "    print(\"Column 'labels' created\")\n",
    "    if df_file_name:\n",
    "        df.to_csv(path_or_buf=df_file_name, index=False)\n",
    "        print(f\"Modified dataframe saved to {df_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_dir = \"UD_converted\"\n",
    "jsons = os.listdir(ud_dir)\n",
    "# print(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('.\\\\ud_andmestik.csv'):\n",
    "    create_df_ud_corpus(jsons, ud_dir, 'ud_morph_reduced', 'ud_andmestik.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_ud_file = \"ud_andmestik.csv\"\n",
    "df_ud = pd.read_csv(csv_ud_file, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning NaN values in columns form and pos with an empty string\n",
      "Removing NaN words\n",
      "Modified dataframe saved to ud_andmestik.csv\n"
     ]
    }
   ],
   "source": [
    "clean_df(df_ud, csv_ud_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating column 'labels'\n",
      "Column 'labels' created\n",
      "Modified dataframe saved to ud_andmestik.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>form</th>\n",
       "      <th>pos</th>\n",
       "      <th>file_prefix</th>\n",
       "      <th>source</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Aga</td>\n",
       "      <td></td>\n",
       "      <td>J</td>\n",
       "      <td>aja_ee199920</td>\n",
       "      <td>et_edt-ud-dev_000.json</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>mulle</td>\n",
       "      <td>sg all</td>\n",
       "      <td>P</td>\n",
       "      <td>aja_ee199920</td>\n",
       "      <td>et_edt-ud-dev_000.json</td>\n",
       "      <td>sg all_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tundub</td>\n",
       "      <td>b</td>\n",
       "      <td>V</td>\n",
       "      <td>aja_ee199920</td>\n",
       "      <td>et_edt-ud-dev_000.json</td>\n",
       "      <td>b_V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>aja_ee199920</td>\n",
       "      <td>et_edt-ud-dev_000.json</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>et</td>\n",
       "      <td></td>\n",
       "      <td>J</td>\n",
       "      <td>aja_ee199920</td>\n",
       "      <td>et_edt-ud-dev_000.json</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id   words    form pos   file_prefix                  source  \\\n",
       "0            0     Aga           J  aja_ee199920  et_edt-ud-dev_000.json   \n",
       "1            0   mulle  sg all   P  aja_ee199920  et_edt-ud-dev_000.json   \n",
       "2            0  tundub       b   V  aja_ee199920  et_edt-ud-dev_000.json   \n",
       "3            0       ,           Z  aja_ee199920  et_edt-ud-dev_000.json   \n",
       "4            0      et           J  aja_ee199920  et_edt-ud-dev_000.json   \n",
       "\n",
       "     labels  \n",
       "0         J  \n",
       "1  sg all_P  \n",
       "2       b_V  \n",
       "3         Z  \n",
       "4         J  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(437826, 7)\n"
     ]
    }
   ],
   "source": [
    "create_labels_column(df_ud, csv_ud_file)\n",
    "display(df_ud.head(5))\n",
    "print(df_ud.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mudeli_testimine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation on UD corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model<!-- Mudeli ülesehitamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, unique_labels, no_progress_bars=False):\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger('simpletransformers.ner.ner_model')\n",
    "    logger.setLevel(logging.ERROR)\n",
    "\n",
    "    # Suppress specific warnings\n",
    "    # warnings.filterwarnings(\"ignore\", category=FutureWarning) # For warning message \"FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated.\"\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning) # For warnings like \"UserWarning: <tag> seems not to be NE tag.\"\n",
    "\n",
    "    # Configurations\n",
    "    model_args = NERArgs()\n",
    "    model_args.train_batch_size = 8\n",
    "    model_args.evaluate_during_training = False\n",
    "    model_args.learning_rate = 5e-5\n",
    "    model_args.num_train_epochs = 10\n",
    "    model_args.use_early_stopping = True\n",
    "    model_args.use_cuda = torch.cuda.is_available()  # Use GPU if available\n",
    "    model_args.save_eval_checkpoints = False\n",
    "    model_args.save_model_every_epoch = False # Takes a lot of storage space\n",
    "    model_args.save_steps = -1\n",
    "    model_args.overwrite_output_dir = True\n",
    "    model_args.cache_dir = model_name + '/cache'\n",
    "    model_args.best_model_dir = model_name + '/best_model'\n",
    "    model_args.output_dir = model_name\n",
    "    model_args.use_multiprocessing = False\n",
    "    model_args.silent = no_progress_bars\n",
    "\n",
    "    # Initialization\n",
    "    model = NERModel(\"camembert\", model_name, args=model_args, labels=unique_labels)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_ud_file = \"ud_andmestik.csv\"\n",
    "df_ud = pd.read_csv(csv_ud_file, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48489, 7)\n"
     ]
    }
   ],
   "source": [
    "test_df_ud = df_ud[df_ud['source'].str.contains('ud-test')].copy()\n",
    "print(test_df_ud.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in unique labels that the model is trained with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in unique labels\n",
    "with open(\"unique_labels.json\", 'r') as f:\n",
    "    unique_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels unknown to Vabamorf and replacing them with appropriate known unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_labels(unique_labels_list, data):\n",
    "    \"\"\"Finds labels that are not present in the unique labels list.\n",
    "\n",
    "    Args:\n",
    "        unique_labels_list (list): list of unique labels (obtained from reading <code>unique_labels.json</code> file)\n",
    "        data (pandas.core.frame.DataFrame): data to check for labels\n",
    "    \"\"\"\n",
    "    df_unique_labels = pd.DataFrame(unique_labels_list, columns=['labels'])\n",
    "    unique_labels_series = df_unique_labels['labels']\n",
    "    df_labels = data['labels']#.drop_duplicates()\n",
    "    labels_not_in_unique = df_labels[~df_labels.isin(unique_labels_series)]\n",
    "\n",
    "    print(\"Labels in data that are not in unique labels list:\")\n",
    "    print(labels_not_in_unique)\n",
    "    print(\"Unique:\")\n",
    "    print(labels_not_in_unique.unique())\n",
    "    return labels_not_in_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in data that are not in unique labels list:\n",
      "46373    T\n",
      "46374    T\n",
      "46375    T\n",
      "48036    T\n",
      "48037    T\n",
      "        ..\n",
      "85108    T\n",
      "85682    T\n",
      "85683    T\n",
      "85941    T\n",
      "85942    T\n",
      "Name: labels, Length: 85, dtype: object\n",
      "Unique:\n",
      "['T' 'sg g_place']\n"
     ]
    }
   ],
   "source": [
    "labels_not_in_unique = unknown_labels(unique_labels, test_df_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace unknown pos 'T' with '?'\n",
    "test_df_ud['labels'] = test_df_ud['labels'].apply(lambda x: '?' if x == 'T' else x)\n",
    "# Replace unknown pos 'place' with 'S'\n",
    "test_df_ud['labels'] = test_df_ud['labels'].apply(lambda x: 'sg g_S' if x == 'sg g_place' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in data that are not in unique labels list:\n",
      "Series([], Name: labels, dtype: object)\n",
      "Unique:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "labels_not_in_unique = unknown_labels(unique_labels, test_df_ud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unnecessary columns for the model<!-- Mudelile ebavajalike veergude eemaldamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_ud = test_df_ud.drop(labels=['file_prefix', 'source'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model<!-- Mudeli ülesehitamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:433: UserWarning: use_multiprocessing automatically disabled as CamemBERT fails when using multiprocessing for feature conversion.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model('NER_mudel', unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing custom metrics to be used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "poseval = evaluate.load(\"evaluate-metric/poseval\", module_type=\"metric\")\n",
    "\n",
    "def custom_metrics(preds, labels):\n",
    "\n",
    "    # Evaluate using poseval\n",
    "    result = poseval.compute(predictions=preds, references=labels)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model<!-- Mudeli hindamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, preds_list = model.eval_model(test_df_ud, extra_metrics=custom_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss:0.9056\n",
      "Precision: \t0.9315\n",
      "Recall: \t0.9162\n",
      "F1 Score: \t0.9187\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluation Loss:{result['eval_loss']:.4f}\")\n",
    "print(f\"Precision: \\t{result['extra_metrics']['weighted avg']['precision']:.4f}\")\n",
    "print(f\"Recall: \\t{result['extra_metrics']['weighted avg']['recall']:.4f}\")\n",
    "print(f\"F1 Score: \\t{result['extra_metrics']['weighted avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vabamorf_ud_korpusel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vabamorf evaluation on UD corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('.\\\\ud_vabamorf.csv'):\n",
    "    create_df_ud_corpus(jsons, ud_dir, 'morph_analysis', 'ud_vabamorf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ud_vabamorf = pd.read_csv('ud_vabamorf.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning NaN values in columns form and pos with an empty string\n",
      "Removing NaN words\n",
      "Modified dataframe saved to ud_vabamorf.csv\n"
     ]
    }
   ],
   "source": [
    "clean_df(df_ud_vabamorf, 'ud_vabamorf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating column 'labels'\n",
      "Column 'labels' created\n",
      "Modified dataframe saved to ud_vabamorf.csv\n"
     ]
    }
   ],
   "source": [
    "create_labels_column(df_ud_vabamorf, 'ud_vabamorf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_ud_vabamorf = df_ud_vabamorf[df_ud_vabamorf['source'].str.contains('ud-test')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_ud = df_ud[df_ud['source'].str.contains('ud-test')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def group_labels_by_sentence(df):\n",
    "    # Preparing data for seqeval metrics (needs nested lists)\n",
    "    grouped = df.groupby(['source', 'sentence_id'])['labels'].apply(list)\n",
    "    return grouped.reset_index(drop=True).tolist()\n",
    "\n",
    "labels_true = group_labels_by_sentence(test_df_ud)\n",
    "labels_pred = group_labels_by_sentence(test_df_ud_vabamorf)\n",
    "\n",
    "# precision = sk.metrics.precision_score(test_df_ud['labels'], test_df_ud_vabamorf['labels'], average='weighted')\n",
    "# recall = sk.metrics.recall_score(test_df_ud['labels'], test_df_ud_vabamorf['labels'], average='weighted')\n",
    "# f1 = sk.metrics.f1_score(test_df_ud['labels'], test_df_ud_vabamorf['labels'], average='weighted')\n",
    "# print(f\"Precision: \\t{precision:.4f}\")\n",
    "# print(f\"Recall: \\t{recall:.4f}\")\n",
    "# print(f\"F1 Score: \\t{f1:.4f}\")\n",
    "\n",
    "results = poseval.compute(predictions=labels_true, references=labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: \t0.9194\n",
      "Recall: \t0.9067\n",
      "F1 Score: \t0.9082\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: \\t{results[\"weighted avg\"][\"precision\"]:.4f}\")\n",
    "print(f\"Recall: \\t{results[\"weighted avg\"][\"recall\"]:.4f}\")\n",
    "print(f\"F1 Score: \\t{results[\"weighted avg\"][\"f1-score\"]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model    | Precision | Recall | F1 score |\n",
    "|----------|-----------|--------|----------|\n",
    "| Bert     | 0.9315    | 0.9162 | 0.9187   |\n",
    "| Vabamorf | 0.9194    | 0.9067 | 0.9082   |\n",
    "\n",
    "\\* Metrics are from weighted average"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
