{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert-based morphological tagger's comparison with Vabamorf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [**Gathering unused data and comparing Vabamorf / NER model**](#kasutamata_andmete_kogumine)\n",
    "   1. [**Defined functions**](#def_func)\n",
    "   2. [**Gathering unused data**](#gather_data)\n",
    "   3. [**Creating comparison data**](#data_creation)\n",
    "   4. [*Using own custom prediction (deprecated)*](#deprecated_pred)\n",
    "   5. [**Using BertMorphTagger**](#estnltk_tagger)\n",
    "   \n",
    "[end](#end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:config.py:58: PyTorch version 2.4.0 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import typing\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import pkg_resources\n",
    "import types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import estnltk, estnltk.converters, estnltk.taggers\n",
    "import torch\n",
    "import math\n",
    "from morph_eval_utils import MorphDiffSummarizer, MorphDiffFinder, write_formatted_diff_str_to_file\n",
    "from tqdm import tqdm\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "from bert_morph_tagger import BertMorphTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estnltk==1.7.3\n",
      "numpy==1.26.4\n",
      "pandas==2.2.2\n",
      "simpletransformers==0.70.1\n",
      "torch==2.4.0\n",
      "tqdm==4.66.5\n"
     ]
    }
   ],
   "source": [
    "# Get locally imported modules from current notebook - https://stackoverflow.com/questions/40428931/package-for-listing-version-of-packages-used-in-a-jupyter-notebook - Alex P. Miller\n",
    "def get_imports():\n",
    "    \n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "            \n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system/pip names. Unfortunately,\n",
    "        # there is no systematic way to get pip names from\n",
    "        # a package's imported name. You'll have to add\n",
    "        # exceptions to this list manually!\n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "            \n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "# The only way I found to get the version of the root package\n",
    "# from only the name of the package is to cross-check the names \n",
    "# of installed packages vs. imported packages\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering unused data and comparing Vabamorf / NER model <!-- Kasutamata andmete kogumine ja Vabamorfi ning NER mudeli võrdlemine -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kasutamata_andmete_kogumine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='def_func'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading text from files, tokenizing texts and save tokenized texts file by file into JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_file_by_file_enc2017(\n",
    "    jsons: typing.List[str], \n",
    "    in_dir: str, \n",
    "    save_dir: str, \n",
    "    do_morph_layer: bool = True, \n",
    "    bert_morph_tagger: typing.Optional[BertMorphTagger] = None, \n",
    "    necessary_layers: typing.List[str] = ['words', 'sentences', 'morph_analysis', 'bert_morph_tagging']\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates a JSON file for each text file.\n",
    "    <ul>\n",
    "        <li>Skips JSON files that have already been created.</li>\n",
    "        <li>Converts JSON file into EstNLTK Text object.</li>\n",
    "        <li>Adds text type metadata and morph analysis.</li>\n",
    "        <li>Adds <code>BertMorphTagger</code> layer</li>\n",
    "        <li>Removes unnecessary layers.</li>\n",
    "        <li>Converts EstNLTK Text object into JSON using <code>estnltk.converters.text_to_json.</code></li>\n",
    "    </ul>\n",
    "    Args:\n",
    "        jsons (list): List of json files from which to read in the text\n",
    "        in_dir (str): Directory where to read the json files from\n",
    "        save_dir (str): Directory where to save the new json files\n",
    "        bert_morph_tagger (optional, BertMorphTagger): Configured <code>BertMorphTagger</code> class instance, if None, will not use this tagger\n",
    "        necessary_layers (optional, list[str]): Text object layers that will not be deleted\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Beginning tokenization file by file\")\n",
    "    for file_name in tqdm(jsons):\n",
    "\n",
    "        # Skipping previous JSON files\n",
    "        if os.path.exists(os.path.join(save_dir, file_name)):\n",
    "            continue\n",
    "\n",
    "        # Convert json to EstNLTK Text object\n",
    "        text = estnltk.converters.json_to_text(file=os.path.join(in_dir, file_name))\n",
    "\n",
    "        # Add text type metadata\n",
    "        text_type = text.meta.get('texttype') # Text type\n",
    "        if not text_type:\n",
    "            if file_name.startswith('wiki17'):\n",
    "                text.meta.update({'texttype': 'wikipedia'})\n",
    "            elif file_name.startswith('web13'):\n",
    "                text.meta.update({'texttype': 'blogs_and_forums'})\n",
    "            else:\n",
    "                raise RuntimeError(\"Could not assign text type\")\n",
    "\n",
    "        # Add morph layer\n",
    "        if do_morph_layer:\n",
    "            text.tag_layer('morph_analysis')\n",
    "        # Add BERT morph layer\n",
    "        if isinstance(bert_morph_tagger, BertMorphTagger):\n",
    "            if not do_morph_layer:\n",
    "                text.tag_layer('sentences')\n",
    "            text.add_layer(bert_morph_tagger.make_layer(text))\n",
    "\n",
    "        # Remove unnecessary layers\n",
    "        for layer in text.layers:\n",
    "            if layer not in necessary_layers:\n",
    "                text.pop_layer(layer, cascading=False)\n",
    "\n",
    "        if 'morph_analysis' in text.layers and 'bert_morph_tagging' in text.layers: # Assertion that the length of both layers are the same\n",
    "            assert len(text.morph_analysis) == len(text.bert_morph_tagging), \\\n",
    "            f\"\"\"Failed to assert file '{file_name}'\n",
    "            Length of layers aren't the same:\n",
    "            morph_analysis = {len(text.morph_analysis)}\n",
    "            bert_morph_tagging = {len(text.bert_morph_tagging)}\"\"\"\n",
    "        # Save to JSON\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        estnltk.converters.text_to_json(text=text, file=os.path.join(save_dir, file_name))\n",
    "\n",
    "    print(\"Tokenization completed successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that collects texts for each text type *more or less* in proportion to the number of words given as `n` <!-- Funktsioon, millega kogutakse iga tekstiliigi kohta tekste enam-vähem proportsionaalselt sõnade arvu suhtes -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collect texts file by file\n",
    "def gather_rows_for_text_type(\n",
    "    df: pd.DataFrame, \n",
    "    n: int, \n",
    "    random_state: typing.Optional[int] = None\n",
    "    ):\n",
    "    \"\"\"Gathers about `n` (>= n) rows for each text type\\n\n",
    "    Ensures that all text types have about the same number of words.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the text data.\n",
    "        n (int): Number of words to gather.\n",
    "        random_state (optional, int): Seed for the shuffle function (acts like <code>random_state</code>).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Gathered rows for each text type.\n",
    "    \"\"\"\n",
    "\n",
    "    def gather_rows_for_type(\n",
    "        group: pd.Group, \n",
    "        n: int, \n",
    "        random_state: typing.Optional[int] = None\n",
    "        ):\n",
    "        \"\"\"Gathers about `n` (>= n) rows for the text type\\n\n",
    "\n",
    "        Args:\n",
    "            group (): Pandas dataframe group\n",
    "            n (int): Number of words to gather\n",
    "            random_state (int): Seed for the shuffle function (acts like <code>random_state</code>)\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Gathered rows for text type\n",
    "        \"\"\"\n",
    "        gathered_rows = pd.DataFrame()\n",
    "        sources = group['source'].unique()\n",
    "\n",
    "        if random_state:\n",
    "            np.random.seed(random_state)\n",
    "            np.random.shuffle(sources)\n",
    "\n",
    "        for source in sources:\n",
    "            source_rows = group[group['source'] == source]\n",
    "            gathered_rows = pd.concat([gathered_rows, source_rows])\n",
    "            if len(gathered_rows) >= n:\n",
    "                break\n",
    "\n",
    "        return gathered_rows\n",
    "\n",
    "    grouped = df.groupby('type')\n",
    "    data = pd.concat([gather_rows_for_type(group, n, random_state) for _, group in grouped])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model<!-- Mudeli ülesehitamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, unique_labels, no_progress_bars=False):\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger('simpletransformers.ner.ner_model')\n",
    "    logger.setLevel(logging.ERROR)\n",
    "\n",
    "    # Suppress specific warnings\n",
    "    # warnings.filterwarnings(\"ignore\", category=FutureWarning) # For warning message \"FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated.\"\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning) # For warnings like \"UserWarning: <tag> seems not to be NE tag.\"\n",
    "\n",
    "    # Configurations\n",
    "    model_args = NERArgs()\n",
    "    model_args.train_batch_size = 8\n",
    "    model_args.evaluate_during_training = False\n",
    "    model_args.learning_rate = 5e-5\n",
    "    model_args.num_train_epochs = 10\n",
    "    model_args.use_early_stopping = True\n",
    "    model_args.use_cuda = torch.cuda.is_available()  # Use GPU if available\n",
    "    model_args.save_eval_checkpoints = False\n",
    "    model_args.save_model_every_epoch = False # Takes a lot of storage space\n",
    "    model_args.save_steps = -1\n",
    "    model_args.overwrite_output_dir = True\n",
    "    model_args.cache_dir = model_name + '/cache'\n",
    "    model_args.best_model_dir = model_name + '/best_model'\n",
    "    model_args.output_dir = model_name\n",
    "    model_args.use_multiprocessing = False\n",
    "    model_args.silent = no_progress_bars\n",
    "\n",
    "    # Initialization\n",
    "    model = NERModel(\"camembert\", model_name, args=model_args, labels=unique_labels)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to predict labels to the group of sentences. Includes sentence splitting into clauses and clauses into equal length clauses' parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_token_count(model, sentence):\n",
    "    \"\"\"Checks token count in the sentence\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        sentence (list): list of words in a sentence\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the token count exceeds model's maximum sequence length\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = model.tokenizer('  '.join(sentence), return_tensors=\"pt\")\n",
    "    return bool(len(inputs[\"input_ids\"][0]) >= model.args.max_seq_length)\n",
    "\n",
    "def get_clause_parts(model, clause):\n",
    "    \"\"\"Splits clause into equal length clause segments. <i>Clauses can get long when there is a list in a sentence</i>\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        clause (list): list of words in a clause\n",
    "\n",
    "    Returns:\n",
    "        list: List of clause segments. Each segment is a list of words.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = model.tokenizer('  '.join(clause), return_tensors=\"pt\")\n",
    "    clause_parts = np.array_split(clause, math.ceil(len(inputs[\"input_ids\"][0]) / model.args.max_seq_length))\n",
    "    return clause_parts\n",
    "\n",
    "def get_clauses(model, sentence):\n",
    "    \"\"\"Splits sentence into clauses using EstNLTK clauses layer.\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        sentence (list): list of words in a sentence\n",
    "\n",
    "    Returns:\n",
    "        list: List of clauses. Each clause is a list of words.\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_text = '  '.join(sentence)\n",
    "    text = estnltk.Text(sentence_text)\n",
    "    text.tag_layer('clauses')\n",
    "    clauses = list()\n",
    "    for clause in text.clauses:\n",
    "        if check_token_count(model, clause.text):\n",
    "            clause_parts = get_clause_parts(model, clause.text)\n",
    "            clauses.extend(clause_parts)\n",
    "        else:\n",
    "            clauses.append(clause.text)\n",
    "    return clauses\n",
    "\n",
    "def get_clauses_labels(model, sentence):\n",
    "    \"\"\"Predicts labels to clauses\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        sentence (list): list of words in a sentence\n",
    "\n",
    "    Returns:\n",
    "        list: List of tags predicted to words in clauses\n",
    "    \"\"\"\n",
    "\n",
    "    clauses = get_clauses(model, sentence)\n",
    "\n",
    "    ner_labels_parts = list()\n",
    "\n",
    "    # Predict tags\n",
    "    predictions, raw_outputs = model.predict(clauses, split_on_space=False)\n",
    "    for prediction_part in predictions:\n",
    "        # ner_words_part = [list(p.keys())[0] for p in prediction_part]\n",
    "        ner_labels_part = [list(p.values())[0] for p in prediction_part]\n",
    "\n",
    "        # ner_words_parts.append(ner_words_part)\n",
    "        ner_labels_parts.append(ner_labels_part)\n",
    "\n",
    "    # ner_words = list(itertools.chain.from_iterable(ner_words_parts))\n",
    "    ner_labels = list(itertools.chain.from_iterable(ner_labels_parts))\n",
    "\n",
    "    return ner_labels\n",
    "\n",
    "def get_sentence_labels(model, sentence):\n",
    "    \"\"\"Predicts labels to a sentence\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        sentence (list): list of words in a sentence\n",
    "\n",
    "    Returns:\n",
    "        list: List of tags predicted to words in a sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict tags\n",
    "    predictions, raw_outputs = model.predict([sentence], split_on_space=False)\n",
    "\n",
    "    # ner_words = [list(p.keys())[0] for p in predictions[0]]\n",
    "    ner_labels = [list(p.values())[0] for p in predictions[0]]\n",
    "    return ner_labels\n",
    "\n",
    "def process_groups(model, groups):\n",
    "    \"\"\"Predicts labels to a list of groups. This group contains sentences for each source text file.\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        groups (): Group containing sentences for each source text file\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: When the length of predicted labels mismatch the length of the sentence length. \n",
    "        <i>This might mean that generated clauses are wrong (meaning some words are missing because of <code>'  '.join(sentence)</code>)</i>\n",
    "\n",
    "    Returns:\n",
    "        list: List of predicted labels for each sentence in each source text file\n",
    "    \"\"\"\n",
    "    chunk_results = []\n",
    "\n",
    "    for _, group in groups:\n",
    "        sentence = group.words.tolist()\n",
    "\n",
    "        if check_token_count(model, sentence):  # Sentence splitting if token count is above model's max sequence length\n",
    "            ner_labels = get_clauses_labels(model, sentence)\n",
    "        else:\n",
    "            ner_labels = get_sentence_labels(model, sentence)\n",
    "\n",
    "        if len(ner_labels) != len(sentence):\n",
    "            display(group)\n",
    "            print(len(sentence), len(ner_labels))\n",
    "            raise AssertionError(\"Predicted labels length mismatch sentence length.\")\n",
    "\n",
    "        group['ner_labels'] = ner_labels\n",
    "        chunk_results.append(group)\n",
    "\n",
    "    return chunk_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to find morphological differences in multiple JSON files and save a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_summarize_differences(jsons:typing.List[str], \n",
    "                                   in_dir:str, \n",
    "                                   output_dir:str, \n",
    "                                   morph_diff_finder:MorphDiffFinder, \n",
    "                                   morph_diff_summarizer:MorphDiffSummarizer):\n",
    "    \"\"\"\n",
    "    Finds morphological differences in multiple JSON files and saves a summary.\n",
    "\n",
    "    This function processes a list of JSON files, identifying morphological \n",
    "    differences in the text of each file. The differences are then summarized \n",
    "    and stored in an output file. \n",
    "\n",
    "    For each JSON file:\n",
    "    <ul>\n",
    "        <li>Text objects are created from the JSON data.</li>\n",
    "        <li>Morphological differences between the text layers are identified.</li>\n",
    "        <li>These differences are recorded using the provided summarizer.</li>\n",
    "        <li>If any differences are found, they are saved to a separate file.</li>\n",
    "        <li>Finally, a summary of all differences is written to a statistics file.</li>\n",
    "    </ul>\n",
    "\n",
    "    Args:\n",
    "        jsons (List[str]): A list of JSON filenames to be processed.\n",
    "        in_dir (str): Directory path where the input JSON files are located.\n",
    "        output_dir (str): Directory path where output summary files will be stored.\n",
    "        morph_diff_finder (MorphDiffFinder): An instance of MorphDiffFinder used \n",
    "            to identify morphological differences.\n",
    "        morph_diff_summarizer (MorphDiffSummarizer): An instance of MorphDiffSummarizer \n",
    "            used to record and summarize the differences.\n",
    "\n",
    "    Outputs:\n",
    "    <ul>\n",
    "        <li>For each JSON file with differences, a text file will be created in the \n",
    "          output directory containing the formatted differences.</li>\n",
    "        <li>A final summary file is created in the output directory with statistics \n",
    "          on the total differences across all processed files.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    # Finds and saves differences for each json file\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for json in jsons:\n",
    "        text_obj = estnltk.converters.json_to_text(file=os.path.join(in_dir, json))\n",
    "        morph_diff_layer, formatted_diffs_str, total_diff_gaps = morph_diff_finder.find_difference(text_obj, fname=json, text_cat=text_obj.meta['texttype'])\n",
    "        morph_diff_summarizer.record_from_diff_layer( 'morph_analysis', morph_diff_layer, text_obj.meta['texttype'], start_new_doc=True )\n",
    "        if formatted_diffs_str is not None and len(formatted_diffs_str) > 0:\n",
    "            fpath = os.path.join(output_dir, f'_{json}__ann_diffs.txt')\n",
    "            write_formatted_diff_str_to_file( fpath, formatted_diffs_str )\n",
    "        text_obj = None\n",
    "        morph_diff_layer = None\n",
    "        formatted_diffs_str = None\n",
    "    # Summarizes the collected differences into results\n",
    "    summarizer_result_str = morph_diff_summarizer.get_diffs_summary_output( show_doc_count=True )\n",
    "    fpath = os.path.join(output_dir, f'_{'enc_2017'}__stats.txt')\n",
    "    with open(fpath, 'w', encoding='utf-8') as out_f:\n",
    "        out_f.write( 'TOTAL DIFF STATISTICS:'+os.linesep+summarizer_result_str )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering unused data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather_data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `finetune_bert_morph_tagger.ipynb` notebook files `andmestik.csv`, containing the whole enc2017 corpus, and `model_data.csv`, containing the subset of `andmestik.csv` used in model training, were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file\n",
    "df = pd.read_csv(\"andmestik.csv\", keep_default_na=False)\n",
    "model_df = pd.read_csv(\"model_data.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the other subset (unused data) of `andmestik.csv` not used in model training, we perform a left anti-join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both DataFrames have the same columns for comparison\n",
    "common_columns = df.columns.intersection(model_df.columns)\n",
    "# Perform a left anti-join to get the unused data\n",
    "unused_data = df.merge(model_df, on=common_columns.tolist(), how='left', indicator=True)\n",
    "unused_data = unused_data[unused_data['_merge'] == 'left_only'].drop(columns=['_merge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the unused data, which is later saved into `unused_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>form</th>\n",
       "      <th>pos</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131164</th>\n",
       "      <td>0</td>\n",
       "      <td>BAGDAD</td>\n",
       "      <td>sg n</td>\n",
       "      <td>H</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642042.json</td>\n",
       "      <td>sg n_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131165</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642042.json</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131166</th>\n",
       "      <td>0</td>\n",
       "      <td>29.</td>\n",
       "      <td>?</td>\n",
       "      <td>O</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642042.json</td>\n",
       "      <td>?_O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131167</th>\n",
       "      <td>0</td>\n",
       "      <td>november</td>\n",
       "      <td>sg n</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642042.json</td>\n",
       "      <td>sg n_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131168</th>\n",
       "      <td>0</td>\n",
       "      <td>(</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642042.json</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555188</th>\n",
       "      <td>2</td>\n",
       "      <td>tulemusena</td>\n",
       "      <td>sg es</td>\n",
       "      <td>S</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99964_x.json</td>\n",
       "      <td>sg es_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555189</th>\n",
       "      <td>2</td>\n",
       "      <td>[</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99964_x.json</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555190</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>?</td>\n",
       "      <td>N</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99964_x.json</td>\n",
       "      <td>?_N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555191</th>\n",
       "      <td>2</td>\n",
       "      <td>]</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99964_x.json</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555192</th>\n",
       "      <td>2</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99964_x.json</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9813158 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sentence_id       words   form pos         type  \\\n",
       "131164              0      BAGDAD   sg n   H  periodicals   \n",
       "131165              0           ,          Z  periodicals   \n",
       "131166              0         29.      ?   O  periodicals   \n",
       "131167              0    november   sg n   S  periodicals   \n",
       "131168              0           (          Z  periodicals   \n",
       "...               ...         ...    ...  ..          ...   \n",
       "10555188            2  tulemusena  sg es   S    wikipedia   \n",
       "10555189            2           [          Z    wikipedia   \n",
       "10555190            2           2      ?   N    wikipedia   \n",
       "10555191            2           ]          Z    wikipedia   \n",
       "10555192            2           .          Z    wikipedia   \n",
       "\n",
       "                        source   labels  \n",
       "131164    nc_10532_642042.json   sg n_H  \n",
       "131165    nc_10532_642042.json        Z  \n",
       "131166    nc_10532_642042.json      ?_O  \n",
       "131167    nc_10532_642042.json   sg n_S  \n",
       "131168    nc_10532_642042.json        Z  \n",
       "...                        ...      ...  \n",
       "10555188   wiki17_99964_x.json  sg es_S  \n",
       "10555189   wiki17_99964_x.json        Z  \n",
       "10555190   wiki17_99964_x.json      ?_N  \n",
       "10555191   wiki17_99964_x.json        Z  \n",
       "10555192   wiki17_99964_x.json        Z  \n",
       "\n",
       "[9813158 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print or save the result\n",
    "display(unused_data)\n",
    "if not os.path.exists(\"./unused_data.csv\"):\n",
    "    unused_data.to_csv(\"unused_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating comparison data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_creation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison data is generated from unused data (`unused_data.csv`). Roughly 600,000 words will be collected for each text type, summing a total of ~3 million words. After collection, the resulting data is saved as `comparison_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_data = pd.read_csv(\"unused_data.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = gather_rows_for_text_type(unused_data, 600000, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./comparison_data.csv\"):\n",
    "    comparison_data.to_csv(\"comparison_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = pd.read_csv(\"comparison_data.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>form</th>\n",
       "      <th>pos</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Koju</td>\n",
       "      <td>adt</td>\n",
       "      <td>S</td>\n",
       "      <td>blogs_and_forums</td>\n",
       "      <td>web13_274106_x.json</td>\n",
       "      <td>adt_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>jõudes</td>\n",
       "      <td>des</td>\n",
       "      <td>V</td>\n",
       "      <td>blogs_and_forums</td>\n",
       "      <td>web13_274106_x.json</td>\n",
       "      <td>des_V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ootas</td>\n",
       "      <td>s</td>\n",
       "      <td>V</td>\n",
       "      <td>blogs_and_forums</td>\n",
       "      <td>web13_274106_x.json</td>\n",
       "      <td>s_V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ees</td>\n",
       "      <td></td>\n",
       "      <td>D</td>\n",
       "      <td>blogs_and_forums</td>\n",
       "      <td>web13_274106_x.json</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>üllatus</td>\n",
       "      <td>sg n</td>\n",
       "      <td>S</td>\n",
       "      <td>blogs_and_forums</td>\n",
       "      <td>web13_274106_x.json</td>\n",
       "      <td>sg n_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056351</th>\n",
       "      <td>17</td>\n",
       "      <td>aastaks</td>\n",
       "      <td>sg tr</td>\n",
       "      <td>S</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_85786_x.json</td>\n",
       "      <td>sg tr_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056352</th>\n",
       "      <td>17</td>\n",
       "      <td>oli</td>\n",
       "      <td>s</td>\n",
       "      <td>V</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_85786_x.json</td>\n",
       "      <td>s_V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056353</th>\n",
       "      <td>17</td>\n",
       "      <td>ta</td>\n",
       "      <td>sg n</td>\n",
       "      <td>P</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_85786_x.json</td>\n",
       "      <td>sg n_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056354</th>\n",
       "      <td>17</td>\n",
       "      <td>surnud</td>\n",
       "      <td></td>\n",
       "      <td>A</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_85786_x.json</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056355</th>\n",
       "      <td>17</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_85786_x.json</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3056356 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id    words   form pos              type  \\\n",
       "0                  0     Koju    adt   S  blogs_and_forums   \n",
       "1                  0   jõudes    des   V  blogs_and_forums   \n",
       "2                  0    ootas      s   V  blogs_and_forums   \n",
       "3                  0      ees          D  blogs_and_forums   \n",
       "4                  0  üllatus   sg n   S  blogs_and_forums   \n",
       "...              ...      ...    ...  ..               ...   \n",
       "3056351           17  aastaks  sg tr   S         wikipedia   \n",
       "3056352           17      oli      s   V         wikipedia   \n",
       "3056353           17       ta   sg n   P         wikipedia   \n",
       "3056354           17   surnud          A         wikipedia   \n",
       "3056355           17        .          Z         wikipedia   \n",
       "\n",
       "                      source   labels  \n",
       "0        web13_274106_x.json    adt_S  \n",
       "1        web13_274106_x.json    des_V  \n",
       "2        web13_274106_x.json      s_V  \n",
       "3        web13_274106_x.json        D  \n",
       "4        web13_274106_x.json   sg n_S  \n",
       "...                      ...      ...  \n",
       "3056351  wiki17_85786_x.json  sg tr_S  \n",
       "3056352  wiki17_85786_x.json      s_V  \n",
       "3056353  wiki17_85786_x.json   sg n_P  \n",
       "3056354  wiki17_85786_x.json        A  \n",
       "3056355  wiki17_85786_x.json        Z  \n",
       "\n",
       "[3056356 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comparison_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using own custom prediction (deprecated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deprecated_pred'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next steps were an attempt to use the model's predict function to get the labels and then do the comparison. However, the model predicts to each token and does not consider Vabamorf's word format. These results are before BertMorphTagger was made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_groups = comparison_data.groupby(['source', 'sentence_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unique_labels.json\", 'r') as f:\n",
    "    unique_labels = json.load(f)\n",
    "\n",
    "model = initialize_model(\"NER_mudel\", unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_chunks = process_groups(model, comparison_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_comparison_data = pd.concat(predicted_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_comparison_data.to_csv(\"updated_comparison_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_comparison_data = pd.read_csv('./updated_comparison_data.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_differences = updated_comparison_data[updated_comparison_data['labels'] != updated_comparison_data['ner_labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_differences.to_csv(\"label_differences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>form</th>\n",
       "      <th>pos</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>labels</th>\n",
       "      <th>ner_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "      <td>sg g</td>\n",
       "      <td>H</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642051.json</td>\n",
       "      <td>sg g_H</td>\n",
       "      <td>sg n_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Reuters-EPLO</td>\n",
       "      <td>?</td>\n",
       "      <td>Y</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642051.json</td>\n",
       "      <td>?_Y</td>\n",
       "      <td>sg n_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>sg g</td>\n",
       "      <td>H</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642051.json</td>\n",
       "      <td>sg g_H</td>\n",
       "      <td>sg n_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>sajalases</td>\n",
       "      <td>sg in</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642051.json</td>\n",
       "      <td>sg in_S</td>\n",
       "      <td>sg in_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2</td>\n",
       "      <td>Timothy</td>\n",
       "      <td>sg g</td>\n",
       "      <td>H</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642051.json</td>\n",
       "      <td>sg g_H</td>\n",
       "      <td>sg n_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056133</th>\n",
       "      <td>20</td>\n",
       "      <td>maini</td>\n",
       "      <td>pl ter</td>\n",
       "      <td>S</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99902_x.json</td>\n",
       "      <td>pl ter_S</td>\n",
       "      <td>sg ter_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056235</th>\n",
       "      <td>6</td>\n",
       "      <td>Kloogaranna</td>\n",
       "      <td>sg n</td>\n",
       "      <td>H</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99953_x.json</td>\n",
       "      <td>sg n_H</td>\n",
       "      <td>sg g_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056247</th>\n",
       "      <td>7</td>\n",
       "      <td>ehitatud</td>\n",
       "      <td>tud</td>\n",
       "      <td>V</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99953_x.json</td>\n",
       "      <td>tud_V</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056303</th>\n",
       "      <td>9</td>\n",
       "      <td>teise</td>\n",
       "      <td>adt</td>\n",
       "      <td>P</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99953_x.json</td>\n",
       "      <td>adt_P</td>\n",
       "      <td>sg g_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056315</th>\n",
       "      <td>0</td>\n",
       "      <td>bioloogia</td>\n",
       "      <td>sg n</td>\n",
       "      <td>S</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99964_x.json</td>\n",
       "      <td>sg n_S</td>\n",
       "      <td>sg g_S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139917 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id         words    form pos         type  \\\n",
       "0                  0      OKLAHOMA    sg g   H  periodicals   \n",
       "5                  0  Reuters-EPLO       ?   Y  periodicals   \n",
       "8                  0      Oklahoma    sg g   H  periodicals   \n",
       "47                 1     sajalases   sg in   S  periodicals   \n",
       "61                 2       Timothy    sg g   H  periodicals   \n",
       "...              ...           ...     ...  ..          ...   \n",
       "3056133           20         maini  pl ter   S    wikipedia   \n",
       "3056235            6   Kloogaranna    sg n   H    wikipedia   \n",
       "3056247            7      ehitatud     tud   V    wikipedia   \n",
       "3056303            9         teise     adt   P    wikipedia   \n",
       "3056315            0     bioloogia    sg n   S    wikipedia   \n",
       "\n",
       "                       source    labels ner_labels  \n",
       "0        nc_10532_642051.json    sg g_H     sg n_H  \n",
       "5        nc_10532_642051.json       ?_Y     sg n_H  \n",
       "8        nc_10532_642051.json    sg g_H     sg n_H  \n",
       "47       nc_10532_642051.json   sg in_S    sg in_A  \n",
       "61       nc_10532_642051.json    sg g_H     sg n_H  \n",
       "...                       ...       ...        ...  \n",
       "3056133   wiki17_99902_x.json  pl ter_S   sg ter_S  \n",
       "3056235   wiki17_99953_x.json    sg n_H     sg g_H  \n",
       "3056247   wiki17_99953_x.json     tud_V          A  \n",
       "3056303   wiki17_99953_x.json     adt_P     sg g_P  \n",
       "3056315   wiki17_99964_x.json    sg n_S     sg g_S  \n",
       "\n",
       "[139917 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(label_differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences count: 139917 / 3056356 (4.578%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Differences count: {len(label_differences)} / {len(updated_comparison_data)} ({round(len(label_differences) / len(updated_comparison_data) * 100, 3)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BertMorphTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='estnltk_tagger'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a new BertMorphTagger is used to predict the labels according to Vabamorf's format `form` and `partofspeech`. Note that the model predicts a label that is a concatenation of `form` and `partofspeech` joined with `_` (underscore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = pd.read_csv(\"comparison_data.csv\", keep_default_na=False)\n",
    "in_dir = '_plain_texts_json'\n",
    "jsons = comparison_data['source'].unique().tolist()\n",
    "morph_tagger = BertMorphTagger('./NER_mudel/', get_top_n_predictions=1, token_level=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning tokenization file by file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5778/5778 [00:00<00:00, 29029.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_json_file_by_file_enc2017(jsons, in_dir, '_diff_morph_texts_json', True, bert_morph_tagger=morph_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding and summarizing differences using `MorphDiffFinder` and `MorphDiffSummarizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_diff_finder = MorphDiffFinder('morph_analysis', \n",
    "                                    'bert_morph_tagging', \n",
    "                                    diff_attribs  = ['partofspeech', 'form'], \n",
    "                                    focus_attribs = ['partofspeech', 'form'] )\n",
    "morph_diff_summarizer = MorphDiffSummarizer('morph_analysis', 'bert_morph_tagging' )\n",
    "in_dir = './_diff_morph_texts_json/'\n",
    "output_dir = './differences/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_and_summarize_differences(jsons, in_dir, output_dir, morph_diff_finder, morph_diff_summarizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging differences into one single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_files = os.listdir(output_dir)\n",
    "lines = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, diff_file in enumerate(diff_files):\n",
    "    with open(file=os.path.join(output_dir, diff_file), mode='r', encoding='UTF-8') as f:\n",
    "        file_lines = f.readlines()\n",
    "        lines.append(file_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file='differences__ann_diffs_.txt', mode='w', encoding='UTF-8') as f:\n",
    "    for file_lines in lines:\n",
    "        f.writelines(file_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='end'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpulocal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
