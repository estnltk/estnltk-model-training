{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert-based morphological tagger <!-- Morfoloogiline ühestaja -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "[Code](#kood)\n",
    "\n",
    "   1. [**enc2017 corpus**](#enc_korpus)\n",
    "      1. [**Defined functions**](#defineeritud_funktsioonid)\n",
    "      2. [**Collecting and tokenizing data**](#andmete_lugemine_kogumine_ja_margendamine)\n",
    "         * [Verification](#kontrollimine_1)\n",
    "      3. [**Statistics**](#statistika)\n",
    "      4. [**Data splitting**](#andmete_jagamine_treening_test_ja_valideerimishulkadesse)\n",
    "         * [Grouping data by text type](#andmete_grupeerimine_tekstiliigiti)\n",
    "         * [Splitting data to sets](#andmete_jagamine_hulkadesse)\n",
    "      5. [**Token classification model (for assigning morpological categories)**](#ner_mudel)\n",
    "      6. [*Token classification model (for assigning morpological categories) by text type*](#ner_mudel_tekstiliigiti)\n",
    "      7. [**Gathering unused data and comparing Vabamorf / NER model**](#kasutamata_andmete_kogumine)\n",
    "\n",
    "[End](#end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## TODO\n",
    "1) - [ ] Neurovõrkudel põhineva morfoloogilise ühestaja loomine ja integreerimine EstNLTK teegiga:\n",
    "    1) - [ ] mudeli treenimiseks vajaliku töövoo loomine;\n",
    "    2) - [ ] töövoo kasutamine mudeli treenimiseks;\n",
    "    3) - [ ] mudeli integreerimine EstNLTK teeki.\n",
    "\n",
    "2) - [ ] Morfoloogilise ühestaja treenimiseks vajalike andmestike moodustamine olemasolevatest andmestikest:\n",
    "    1) - [ ] morfoloogilisemärgenduse loomine kasutades olemaolevaid EstNLTK automaatmärgendamise töövoogusid;\n",
    "    2) - [ ] morfoloogilisemärgenduse loomine kasutades UD puudepanga andmeid;\n",
    "\n",
    "3) - [ ] Saadud ühestaja tulemuste võrdlemine teiste kasutatavate rakendustega:\n",
    "    1) - [ ] täpsuse ja saagise absoluutne hindamine testvalimil;\n",
    "    2) - [ ] täpsuse ja saagise relatiivne hindamine märgendamata testandmetel;\n",
    "    3) - [ ] uute informatiivsete treeningandmete välja valimine.\n",
    "\n",
    "4) - [ ] Juhendmaterjalide  loomine uue ühestaja kasutamiseks\n",
    "5) - [ ] Zip the model somehow, https://github.com/estnltk/estnltk/blob/main/estnltk_neural/estnltk_neural/taggers/ner/estbertner_tagger.py -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code <!-- Kood -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kood'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:config.py:58: PyTorch version 2.4.0 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import gc\n",
    "import typing\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import pkg_resources\n",
    "import types\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import estnltk, estnltk.converters, estnltk.taggers\n",
    "import torch\n",
    "import math\n",
    "import simpletransformers\n",
    "from morph_eval_utils import MorphDiffSummarizer, MorphDiffFinder, write_formatted_diff_str_to_file\n",
    "from tqdm import tqdm\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "from bert_morph_tagger import BertMorphTagger\n",
    "from bert_tokens_to_words_rewriter import BertTokens2WordsRewriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estnltk==1.7.3\n",
      "numpy==1.26.4\n",
      "pandas==2.2.2\n",
      "scikit-learn==1.5.1\n",
      "simpletransformers==0.70.1\n",
      "torch==2.4.0\n",
      "tqdm==4.66.5\n"
     ]
    }
   ],
   "source": [
    "# Get locally imported modules from current notebook - https://stackoverflow.com/questions/40428931/package-for-listing-version-of-packages-used-in-a-jupyter-notebook - Alex P. Miller\n",
    "def get_imports():\n",
    "    \n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "            \n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system/pip names. Unfortunately,\n",
    "        # there is no systematic way to get pip names from\n",
    "        # a package's imported name. You'll have to add\n",
    "        # exceptions to this list manually!\n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "            \n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "# The only way I found to get the version of the root package\n",
    "# from only the name of the package is to cross-check the names \n",
    "# of installed packages vs. imported packages\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enc2017 corpus\n",
    "\n",
    "Data has been gathered from the [corpus](https://github.com/estnltk/eval_experiments_lrec_2020/blob/master/scripts_and_data/enc2017_selection_plain_texts_json.zip) that was used in evaluation experiments reported in LREC 2020 paper \"EstNLTK 1.6: Remastered Estonian NLP Pipeline\"[^1].\n",
    "\n",
    "[^1]: Sven Laur, Siim Orasmaa, Dage Särg, Paul Tammo. \"EstNLTK 1.6: Remastered Estonian NLP Pipeline\". *Proceedings of The 12th Language Resources and Evaluation Conference*. European Language Resources Association: Marseille, France, May 2020, p. 7154-7162"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='enc_korpus'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defined functions <!-- Defineeritud funktsioonid -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='defineeritud_funktsioonid'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading text from files, tokenizing texts and save tokenized texts file by file into JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_file_by_file_enc2017(\n",
    "    jsons: typing.List[str], \n",
    "    in_dir: str, \n",
    "    save_dir: str, \n",
    "    do_morph_layer: bool = True, \n",
    "    bert_morph_tagger: typing.Optional[BertMorphTagger] = None, \n",
    "    necessary_layers: typing.List[str] = ['words', 'sentences', 'morph_analysis', 'bert_morph_tagging']\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates a JSON file for each text file.\n",
    "    <ul>\n",
    "        <li>Skips JSON files that have already been created.</li>\n",
    "        <li>Converts JSON file into EstNLTK Text object.</li>\n",
    "        <li>Adds text type metadata and morph analysis.</li>\n",
    "        <li>Adds <code>BertMorphTagger</code> layer</li>\n",
    "        <li>Removes unnecessary layers.</li>\n",
    "        <li>Converts EstNLTK Text object into JSON using <code>estnltk.converters.text_to_json.</code></li>\n",
    "    </ul>\n",
    "    Args:\n",
    "        jsons (list): List of json files from which to read in the text\n",
    "        in_dir (str): Directory where to read the json files from\n",
    "        save_dir (str): Directory where to save the new json files\n",
    "        bert_morph_tagger (optional, BertMorphTagger): Configured <code>BertMorphTagger</code> class instance, if None, will not use this tagger\n",
    "        necessary_layers (optional, list[str]): Text object layers that will not be deleted\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Beginning tokenization file by file\")\n",
    "    for file_name in tqdm(jsons):\n",
    "\n",
    "        # Skipping previous JSON files\n",
    "        if os.path.exists(os.path.join(save_dir, file_name)):\n",
    "            continue\n",
    "\n",
    "        # Convert json to EstNLTK Text object\n",
    "        text = estnltk.converters.json_to_text(file=os.path.join(in_dir, file_name))\n",
    "\n",
    "        # Add text type metadata\n",
    "        text_type = text.meta.get('texttype') # Text type\n",
    "        if not text_type:\n",
    "            if file_name.startswith('wiki17'):\n",
    "                text.meta.update({'texttype': 'wikipedia'})\n",
    "            elif file_name.startswith('web13'):\n",
    "                text.meta.update({'texttype': 'blogs_and_forums'})\n",
    "            else:\n",
    "                raise RuntimeError(\"Could not assign text type\")\n",
    "\n",
    "        # Add morph layer\n",
    "        if do_morph_layer:\n",
    "            text.tag_layer('morph_analysis')\n",
    "        # Add BERT morph layer\n",
    "        if isinstance(bert_morph_tagger, BertMorphTagger):\n",
    "            if not do_morph_layer:\n",
    "                text.tag_layer('sentences')\n",
    "            text.add_layer(bert_morph_tagger.make_layer(text))\n",
    "\n",
    "        # Remove unnecessary layers\n",
    "        for layer in text.layers:\n",
    "            if layer not in necessary_layers:\n",
    "                text.pop_layer(layer, cascading=False)\n",
    "\n",
    "        if 'morph_analysis' in text.layers and 'bert_morph_tagging' in text.layers: # Assertion that the length of both layers are the same\n",
    "            assert len(text.morph_analysis) == len(text.bert_morph_tagging), \\\n",
    "            f\"\"\"Failed to assert file '{file_name}'\n",
    "            Length of layers aren't the same:\n",
    "            morph_analysis = {len(text.morph_analysis)}\n",
    "            bert_morph_tagging = {len(text.bert_morph_tagging)}\"\"\"\n",
    "        # Save to JSON\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        estnltk.converters.text_to_json(text=text, file=os.path.join(save_dir, file_name))\n",
    "\n",
    "    print(\"Tokenization completed successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading text from files, tokenizing texts and save tokenized texts file by file into CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_file_by_file_enc2017(\n",
    "    jsons: typing.List[str], \n",
    "    in_dir: str, \n",
    "    csv_dir: str\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates a CSV file for each text file. \\n\n",
    "    Skips CSV files that have already been created. \\n\n",
    "    For each <code>.json</code> file, the following info is gathered:\n",
    "    <ul>\n",
    "        <li><code>sentence_id</code> -- given for each sentence</li>\n",
    "        <li><code>words</code> -- words gathered from text</li>\n",
    "        <li><code>form</code> -- word form notation</li>\n",
    "        <li><code>pos</code> -- part of speech</li>\n",
    "        <li><code>type</code> -- text type (i.e. genre)</li>\n",
    "        <li><code>source</code> -- file name where the text is taken from</li>\n",
    "    </ul>\n",
    "    <a href=\"https://github.com/Filosoft/vabamorf/blob/e6d42371006710175f7ec328c98f90b122930555/doc/tagset.md\">Tables of morphological categories</a> for more information about <code>form</code> and <code>pos</code>.\n",
    "\n",
    "    Args:\n",
    "        jsons (list): List of json files from which to read in the text\n",
    "        in_dir (str): Directory where to read the json files from\n",
    "        csv_dir (str): Directory where to save the new csv files\n",
    "    \"\"\"\n",
    "    print(\"Beginning tokenization file by file\")\n",
    "    for file_name in tqdm(jsons):\n",
    "        tokens = list()\n",
    "        sentence_id = 0\n",
    "\n",
    "        # Skipping previous CSV files\n",
    "        csv_file_name = file_name[:-4]+'csv'\n",
    "        if os.path.exists(os.path.join(csv_dir, csv_file_name)):\n",
    "            # print(f\"Skipping {file_name} as {csv_file_name} already exists.\")\n",
    "            continue\n",
    "\n",
    "        # print(f\"Beginning to tokenize {file_name}\")\n",
    "\n",
    "        # Tokenization using estnltk\n",
    "        text = estnltk.converters.json_to_text(file=os.path.join(in_dir, file_name))\n",
    "        text_type = text.meta.get('texttype') # Text type\n",
    "        if file_name.startswith('wiki17'):\n",
    "            text_type = 'wikipedia'\n",
    "        elif file_name.startswith('web13'):\n",
    "            text_type = 'blogs_and_forums'\n",
    "        morph_analysis = text.tag_layer('morph_analysis')\n",
    "        for sentence in morph_analysis.sentences:\n",
    "            sentence_analysis = sentence.morph_analysis\n",
    "            for text, form, pos in zip(sentence_analysis.text, sentence_analysis.form, sentence_analysis.partofspeech):\n",
    "                if text:\n",
    "                    tokens.append((sentence_id, text, form[0], pos[0], text_type, file_name)) # In case of multiplicity, select the first or index 0\n",
    "            sentence_id += 1\n",
    "\n",
    "        # print(f\"{file_name} tokenized, now saving\")\n",
    "\n",
    "        # Salvestamine\n",
    "        with open(os.path.join(csv_dir, csv_file_name), 'w') as f:\n",
    "            fieldnames = ['sentence_id', 'word', 'form', 'pos']\n",
    "            writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(fieldnames)\n",
    "            for row in tokens:\n",
    "                writer.writerow(row)\n",
    "\n",
    "        # print(f\"{file_name} saved to {csv_file_name}\\n\")\n",
    "\n",
    "    print(\"Tokenization completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading text from files, tokenizing texts and save tokenized texts into one whole dataset. <!-- Failidest teksti lugemine, märgendamine ja salvestamine kokku üheks andmestikuks -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_enc2017_csv(\n",
    "    jsons: typing.List[str], \n",
    "    in_dir: str, \n",
    "    csv_filename: str\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates a new dataset from converted the Estonian UD EDT <a href=\"https://github.com/UniversalDependencies/UD_Estonian-EDT\">corpus</a>. \\n\n",
    "    For each <code>.json</code> file, the following info is gathered:\n",
    "    <ul>\n",
    "        <li><code>sentence_id</code> -- given for each sentence</li>\n",
    "        <li><code>words</code> -- words gathered from text</li>\n",
    "        <li><code>form</code> -- word form notation</li>\n",
    "        <li><code>pos</code> -- part of speech</li>\n",
    "        <li><code>file_prefix</code> -- metadata</li>\n",
    "        <li><code>source</code> -- file name where the text is taken from</li>\n",
    "    </ul>\n",
    "    <a href=\"https://github.com/Filosoft/vabamorf/blob/e6d42371006710175f7ec328c98f90b122930555/doc/tagset.md\">Tables of morphological categories</a> for more information about <code>form</code> and <code>pos</code>.\n",
    "\n",
    "    Args:\n",
    "        jsons (list): List of json files from which to read in the text\n",
    "        in_dir (str): Directory containing list of files (<code>jsons</code>)\n",
    "        csv_filename (str): CSV filename where to save the gathered text\n",
    "    \"\"\"\n",
    "    tokens = list()\n",
    "    sentence_id = 0\n",
    "    fieldnames = ['sentence_id', 'words', 'form', 'pos', 'type', 'source']\n",
    "\n",
    "    print(\"Beginning tokenization file by file. This can take a while.\")\n",
    "    for file_name in tqdm(jsons):\n",
    "        # print(f\"Beginning to tokenize {file_name}\")\n",
    "        sentence_id = 0\n",
    "\n",
    "        # Tokenization\n",
    "        text = estnltk.converters.json_to_text(file=os.path.join(in_dir, file_name))\n",
    "        text_type = text.meta.get('texttype') # Text type\n",
    "        if file_name.startswith('wiki17'):\n",
    "            text_type = 'wikipedia'\n",
    "        elif file_name.startswith('web13'):\n",
    "            text_type = 'blogs_and_forums'\n",
    "        morph_analysis = text.tag_layer('morph_analysis')\n",
    "        for sentence in morph_analysis.sentences:\n",
    "            sentence_analysis = sentence.morph_analysis\n",
    "            for text, form, pos in zip(sentence_analysis.text, sentence_analysis.form, sentence_analysis.partofspeech):\n",
    "                if text:\n",
    "                    tokens.append((sentence_id, text, form[0], pos[0], text_type, file_name)) # In case of multiplicity, select the first or index 0\n",
    "            sentence_id += 1\n",
    "        # print(f\"{file_name} tokenized\")\n",
    "\n",
    "    print(\"Tokenization completed successfully\")\n",
    "    print(\"Creating Pandas dataframe\")\n",
    "    df = pd.DataFrame(data=tokens, columns=fieldnames)\n",
    "    df.to_csv(path_or_buf=csv_filename, index=False)\n",
    "    print(f\"Tokenized texts saved to {csv_filename}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning dataset by:\n",
    "    <ul>\n",
    "        <li>filling NaN values in columns <code>form</code> and <code>pos</code> with empty strings;</li>\n",
    "        <li>removing NaN words.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(\n",
    "    df: pd.DataFrame, \n",
    "    df_file_name: typing.Optional[str] = None\n",
    "    ):\n",
    "    \"\"\"Finishes dataframe by:\n",
    "    <ul>\n",
    "        <li>filling NaN values in columns <code>form</code> and <code>pos</code> with empty strings;</li>\n",
    "        <li>removing NaN words.</li>\n",
    "    </ul>\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Pandas dataframe to clean\n",
    "        df_file_name (str): CSV file name from which dataframe was created\n",
    "    \"\"\"\n",
    "    print(\"Assigning NaN values in columns form and pos with an empty string\")\n",
    "    # NaN values are assigned with an empty string\n",
    "    df['form'] = df['form'].fillna('')\n",
    "    df['pos'] = df['pos'].fillna('')\n",
    "    print(\"Removing NaN words\")\n",
    "    # Removing NaN words\n",
    "    df.dropna(subset=['words'], inplace=True)\n",
    "    if df_file_name:\n",
    "        df.to_csv(path_or_buf=df_file_name, index=False)\n",
    "        print(f\"Modified dataframe saved to {df_file_name}\")\n",
    "    else:\n",
    "        print(\"Dataframe cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a new column `labels` concatenating the values of columns `pos` (part of speech) and `form` (word form notation)<!-- Uue veeru `labels` loomine, mis konkateneerib veergude `pos`(sõnaliik) ja `form`(sõnavormi tähistus) väärtused -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New 'labels' column\n",
    "def create_labels_column(\n",
    "    df: pd.DataFrame, \n",
    "    df_file_name: typing.Optional[str] = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates a new column <code>labels</code> concatenating the values of columns <code>pos</code> (part of speech) and <code>form</code> (word form notation)\n",
    "\n",
    "    Args:\n",
    "        df (pandas.core.frame.DataFrame): Pandas dataframe to create a new column\n",
    "        df_file_name (str): CSV file name from which dataframe was created\n",
    "    \"\"\"\n",
    "    print(\"Creating column 'labels'\")\n",
    "    df['labels'] = df.apply(lambda row: str(row['form']) + '_' + str(row['pos']) if row['form'] and row['pos'] else str(row['form']) or str(row['pos']), axis=1)\n",
    "    print(\"Column 'labels' created\")\n",
    "    if df_file_name:\n",
    "        df.to_csv(path_or_buf=df_file_name, index=False)\n",
    "        print(f\"Modified dataframe saved to {df_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining unique tags that the model must predict<!-- Unikaalsete väärtuste saamine, mida mudel peab hakkama ennustama -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_labels():\n",
    "    \"\"\"Creates list of unique labels that the model must predict by creating all possible combinations of POS (Part Of Speech) and form.\n",
    "\n",
    "    <i>Gathering unique labels from the enc2017 database proved to be insufficient for future model evaluation,\n",
    "    because the database does not contain all possible combinations of POS and form.\n",
    "    Evaluating model with UD Est-EDT test corpus proved that this problem existed.</i>\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique labels\n",
    "    \"\"\"\n",
    "    # Separately, if one of two doesn't exist\n",
    "    pos_labels = ['A', 'C', 'D', 'G', 'H', 'I', 'J', 'K', 'N', 'O', 'P', 'S', 'U', 'V', 'X', 'Y', 'Z']\n",
    "    form_labels = ['ab', 'abl', 'ad', 'adt', 'all', 'el', 'es', 'g', 'ill', 'in', 'kom', 'n', 'p', 'pl', 'sg', 'ter', 'tr', 'b', 'd', 'da', 'des', 'ge', 'gem', 'gu', 'ks', 'ksid', 'ksime', 'ksin', 'ksite', 'ma', 'maks', 'mas', 'mast', 'mata', 'me', 'n', 'neg', 'nud', 'nuks', 'nuksid', 'nuksime', 'nuksin', 'nuksite', 'nuvat', 'o', 's', 'sid', 'sime', 'sin', 'site', 'ta', 'tagu', 'taks', 'takse', 'tama', 'tav', 'tavat', 'te', 'ti', 'tud', 'tuks', 'tuvat', 'v', 'vad', 'vat']\n",
    "\n",
    "    pos_labels_mutable = ['A', 'C', 'N', 'H', 'O', 'P', 'S', 'U', 'Y', 'X']\n",
    "    #pos_labels_immutable = ['D', 'G', 'I', 'J', 'K', 'X', 'Y', 'Z']\n",
    "    pos_label_verb = ['V']\n",
    "    form_labels_noun = ['ab', 'abl', 'ad', 'adt', 'all', 'el', 'es', 'g', 'ill', 'in', 'kom', 'n', 'p', 'ter', 'tr']\n",
    "    form_labels_noun_count = ['pl', 'sg']\n",
    "    form_labels_verb = ['b', 'd', 'da', 'des', 'ge', 'gem', 'gu', 'ks', 'ksid', 'ksime', 'ksin', 'ksite', 'ma', 'maks', 'mas', 'mast', 'mata', 'me', 'n', 'neg', 'neg ge', 'neg gem', 'neg gu', 'neg ks', 'neg me', 'neg nud', 'neg nuks', 'neg o', 'neg vat', 'neg tud', 'nud', 'nuks', 'nuksid', 'nuksime', 'nuksin', 'nuksite', 'nuvat', 'o', 's', 'sid', 'sime', 'sin', 'site', 'ta', 'tagu', 'taks', 'takse', 'tama', 'tav', 'tavat', 'te', 'ti', 'tud', 'tuks', 'tuvat', 'v', 'vad', 'vat']\n",
    "\n",
    "    noun_labels_without_pos = list(itertools.product(form_labels_noun_count, form_labels_noun))\n",
    "    noun_labels_nested = list(itertools.product(noun_labels_without_pos, pos_labels_mutable))\n",
    "    form_pos_labels = list(itertools.product(form_labels, pos_labels))\n",
    "    noun_labels = list()\n",
    "    verb_labels = list(itertools.product(form_labels_verb, pos_label_verb))\n",
    "\n",
    "    # Connect count and form in mutables\n",
    "    for form, pos in noun_labels_nested:\n",
    "        noun_labels.append((form[0] + ' ' + form[1], pos))\n",
    "\n",
    "    pos_label_only = [('', pos) for pos in pos_labels]\n",
    "    form_label_only = [(form, '') for form in form_labels]\n",
    "    unknown_form_labels = [('?', pos) for pos in pos_labels] # form '?' comes from enc2017 corpus after tokenization\n",
    "\n",
    "    unique_labels = pos_label_only + form_label_only + noun_labels + verb_labels + unknown_form_labels + form_pos_labels + ['?'] # '?' for labels unknown to Vabamorf\n",
    "\n",
    "    unique_labels_df = pd.DataFrame(unique_labels, columns=['form', 'pos'])\n",
    "    create_labels_column(unique_labels_df)\n",
    "    unique_labels_df.drop(labels=['form', 'pos'], axis=1)\n",
    "    print(\"List of unique labels created\")\n",
    "    return unique_labels_df['labels'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that collects texts for each text type *more or less* in proportion to the number of words given as `n` <!-- Funktsioon, millega kogutakse iga tekstiliigi kohta tekste enam-vähem proportsionaalselt sõnade arvu suhtes -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collect texts file by file\n",
    "def gather_rows_for_text_type(\n",
    "    df: pd.DataFrame, \n",
    "    n: int, \n",
    "    random_state: typing.Optional[int] = None\n",
    "    ):\n",
    "    \"\"\"Gathers about `n` (>= n) rows for each text type\\n\n",
    "    Ensures that all text types have about the same number of words.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the text data.\n",
    "        n (int): Number of words to gather.\n",
    "        random_state (optional, int): Seed for the shuffle function (acts like <code>random_state</code>).\n",
    "\n",
    "    Returns:\n",
    "        pandas.core.frame.DataFrame: Gathered rows for each text type.\n",
    "    \"\"\"\n",
    "\n",
    "    def gather_rows_for_type(\n",
    "        group: pd.Group, \n",
    "        n: int, \n",
    "        random_state: typing.Optional[int] = None\n",
    "        ):\n",
    "        \"\"\"Gathers about `n` (>= n) rows for the text type\\n\n",
    "\n",
    "        Args:\n",
    "            group (): Pandas dataframe group\n",
    "            n (int): Number of words to gather\n",
    "            random_state (int): Seed for the shuffle function (acts like <code>random_state</code>)\n",
    "\n",
    "        Returns:\n",
    "            pandas.core.frame.DataFrame: Gathered rows for text type\n",
    "        \"\"\"\n",
    "        gathered_rows = pd.DataFrame()\n",
    "        sources = group['source'].unique()\n",
    "\n",
    "        if random_state:\n",
    "            np.random.seed(random_state)\n",
    "            np.random.shuffle(sources)\n",
    "\n",
    "        for source in sources:\n",
    "            source_rows = group[group['source'] == source]\n",
    "            gathered_rows = pd.concat([gathered_rows, source_rows])\n",
    "            if len(gathered_rows) >= n:\n",
    "                break\n",
    "\n",
    "        return gathered_rows\n",
    "\n",
    "    grouped = df.groupby('type')\n",
    "    data = pd.concat([gather_rows_for_type(group, n, random_state) for _, group in grouped])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model<!-- Mudeli ülesehitamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, unique_labels, is_progress_bars=False):\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger('simpletransformers.ner.ner_model')\n",
    "    logger.setLevel(logging.ERROR)\n",
    "\n",
    "    # Suppress specific warnings\n",
    "    # warnings.filterwarnings(\"ignore\", category=FutureWarning) # For warning message \"FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated.\"\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning) # For warnings like \"UserWarning: <tag> seems not to be NE tag.\"\n",
    "\n",
    "    # Configurations\n",
    "    model_args = NERArgs()\n",
    "    model_args.train_batch_size = 8\n",
    "    model_args.evaluate_during_training = False\n",
    "    model_args.learning_rate = 5e-5\n",
    "    model_args.num_train_epochs = 10\n",
    "    model_args.use_early_stopping = True\n",
    "    model_args.use_cuda = torch.cuda.is_available()  # Use GPU if available\n",
    "    model_args.save_eval_checkpoints = False\n",
    "    model_args.save_model_every_epoch = False # Takes a lot of storage space\n",
    "    model_args.save_steps = -1\n",
    "    model_args.overwrite_output_dir = True\n",
    "    model_args.cache_dir = model_name + '/cache'\n",
    "    model_args.best_model_dir = model_name + '/best_model'\n",
    "    model_args.output_dir = model_name\n",
    "    model_args.use_multiprocessing = False\n",
    "    model_args.silent = is_progress_bars\n",
    "\n",
    "    # Initialization\n",
    "    model = NERModel(\"camembert\", model_name, args=model_args, labels=unique_labels)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to predict tags to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, sentences):\n",
    "\n",
    "    if isinstance(sentences, str):\n",
    "        text = estnltk.Text(sentences)\n",
    "        text.tag_layer(\"morph_analysis\")\n",
    "        text.morph_analysis;\n",
    "        sentences = [s.text for s in text.sentences]\n",
    "        predictions, raw_outputs = model.predict(sentences, split_on_space=False)\n",
    "\n",
    "    elif isinstance(sentences, list) and isinstance(sentences[0], list):\n",
    "        predictions, raw_outputs = model.predict(sentences, split_on_space=False)\n",
    "\n",
    "    else:\n",
    "        raise TypeError(f\"Input is in wrong format. Possible formats are str or list of lists. Your input is {type(sentences)}\")\n",
    "\n",
    "    return predictions, raw_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for predicting top n tags to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_tags(raw_outputs, tag_list, n=3, with_confidence=True):\n",
    "    \"\"\"Extract the top <code>n</code> tags and their probabilities for each word based on the raw output logits.\n",
    "\n",
    "    Args:\n",
    "        raw_outputs (list): Raw prediction logits from the model.\n",
    "        tag_list (list): List of all possible tags.\n",
    "        n (int): Number of top tags to extract.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists, where each sublist contains tuples of the top `n` tags and their probabilities for a word.\n",
    "    \"\"\"\n",
    "    top_n_tags = []\n",
    "\n",
    "    for sentence_logits in raw_outputs:\n",
    "        for word_logits in sentence_logits:\n",
    "\n",
    "            # Get the logits for the word\n",
    "            logits = list(word_logits.values())[0][0]\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "\n",
    "            # Get the indices of the top `n` probabilities\n",
    "            top_n_indices = np.argsort(probabilities)[-n:][::-1]\n",
    "\n",
    "            # Map these indices to the actual tags and their probabilities\n",
    "            if with_confidence:\n",
    "                top_n_tags.append(\n",
    "                    [(tag_list[i], probabilities[i]) for i in top_n_indices]\n",
    "                )\n",
    "            else:\n",
    "                top_n_tags.append(\n",
    "                    [(tag_list[i]) for i in top_n_indices]\n",
    "                )\n",
    "\n",
    "    return top_n_tags\n",
    "\n",
    "def predict_top_n_tags(model, to_predict, with_confidence=True):\n",
    "    \"\"\"Predicts top <code>n</code> tags and their probabilities for each word.\n",
    "\n",
    "    Args:\n",
    "        to_predict (list): A python list of text (str) to be sent to the model for prediction.\n",
    "        Must be a a list of lists, with the inner list being a list of strings consisting of the split sequences. \n",
    "        The outer list is the list of sequences to predict on.\n",
    "        with_confidence (bool): Whether to output confidence values for each tag.\n",
    "    \"\"\"\n",
    "    _, raw_outputs = model.predict(to_predict, split_on_space=False)\n",
    "\n",
    "    tag_list = model.config.id2label  # Assuming the model has a mapping from ids to labels\n",
    "    top_n_tags_per_word = get_top_n_tags(raw_outputs, tag_list, n=3, with_confidence=with_confidence)\n",
    "\n",
    "    for sentence in to_predict:\n",
    "        for word, tags in zip(sentence, top_n_tags_per_word):\n",
    "            print(f\"Word: {word}\")\n",
    "            if with_confidence:\n",
    "                for tag, confidence in tags:\n",
    "                    print(f\"\\tTag: {tag} \\t Confidence: {confidence:.4f}\")\n",
    "            else:\n",
    "                for tag in tags:\n",
    "                    print(f\"\\tTag: {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to predict labels to the group of sentences. Includes sentence splitting into clauses and clauses into equal length clauses' parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_token_count(model, sentence):\n",
    "    \"\"\"Checks token count in the sentence\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        sentence (list): list of words in a sentence\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the token count exceeds model's maximum sequence length\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = model.tokenizer('  '.join(sentence), return_tensors=\"pt\")\n",
    "    return bool(len(inputs[\"input_ids\"][0]) >= model.args.max_seq_length)\n",
    "\n",
    "def get_clause_parts(model, clause):\n",
    "    \"\"\"Splits clause into equal length clause segments. <i>Clauses can get long when there is a list in a sentence</i>\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        clause (list): list of words in a clause\n",
    "\n",
    "    Returns:\n",
    "        list: List of clause segments. Each segment is a list of words.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = model.tokenizer('  '.join(clause), return_tensors=\"pt\")\n",
    "    clause_parts = np.array_split(clause, math.ceil(len(inputs[\"input_ids\"][0]) / model.args.max_seq_length))\n",
    "    return clause_parts\n",
    "\n",
    "def get_clauses(model, sentence):\n",
    "    \"\"\"Splits sentence into clauses using EstNLTK clauses layer.\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        sentence (list): list of words in a sentence\n",
    "\n",
    "    Returns:\n",
    "        list: List of clauses. Each clause is a list of words.\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_text = '  '.join(sentence)\n",
    "    text = estnltk.Text(sentence_text)\n",
    "    text.tag_layer('clauses')\n",
    "    clauses = list()\n",
    "    for clause in text.clauses:\n",
    "        if check_token_count(model, clause.text):\n",
    "            clause_parts = get_clause_parts(model, clause.text)\n",
    "            clauses.extend(clause_parts)\n",
    "        else:\n",
    "            clauses.append(clause.text)\n",
    "    return clauses\n",
    "\n",
    "def get_clauses_labels(model, sentence):\n",
    "    \"\"\"Predicts labels to clauses\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        sentence (list): list of words in a sentence\n",
    "\n",
    "    Returns:\n",
    "        list: List of tags predicted to words in clauses\n",
    "    \"\"\"\n",
    "\n",
    "    clauses = get_clauses(model, sentence)\n",
    "\n",
    "    ner_labels_parts = list()\n",
    "\n",
    "    # Predict tags\n",
    "    predictions, raw_outputs = model.predict(clauses, split_on_space=False)\n",
    "    for prediction_part in predictions:\n",
    "        # ner_words_part = [list(p.keys())[0] for p in prediction_part]\n",
    "        ner_labels_part = [list(p.values())[0] for p in prediction_part]\n",
    "\n",
    "        # ner_words_parts.append(ner_words_part)\n",
    "        ner_labels_parts.append(ner_labels_part)\n",
    "\n",
    "    # ner_words = list(itertools.chain.from_iterable(ner_words_parts))\n",
    "    ner_labels = list(itertools.chain.from_iterable(ner_labels_parts))\n",
    "\n",
    "    return ner_labels\n",
    "\n",
    "def get_sentence_labels(model, sentence):\n",
    "    \"\"\"Predicts labels to a sentence\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        sentence (list): list of words in a sentence\n",
    "\n",
    "    Returns:\n",
    "        list: List of tags predicted to words in a sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict tags\n",
    "    predictions, raw_outputs = model.predict([sentence], split_on_space=False)\n",
    "\n",
    "    # ner_words = [list(p.keys())[0] for p in predictions[0]]\n",
    "    ner_labels = [list(p.values())[0] for p in predictions[0]]\n",
    "    return ner_labels\n",
    "\n",
    "def process_groups(model, groups):\n",
    "    \"\"\"Predicts labels to a list of groups. This group contains sentences for each source text file.\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        groups (): Group containing sentences for each source text file\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: When the length of predicted labels mismatch the length of the sentence length. \n",
    "        <i>This might mean that generated clauses are wrong (meaning some words are missing because of <code>'  '.join(sentence)</code>)</i>\n",
    "\n",
    "    Returns:\n",
    "        list: List of predicted labels for each sentence in each source text file\n",
    "    \"\"\"\n",
    "    chunk_results = []\n",
    "\n",
    "    for _, group in groups:\n",
    "        sentence = group.words.tolist()\n",
    "\n",
    "        if check_token_count(model, sentence):  # Sentence splitting if token count is above model's max sequence length\n",
    "            ner_labels = get_clauses_labels(model, sentence)\n",
    "        else:\n",
    "            ner_labels = get_sentence_labels(model, sentence)\n",
    "\n",
    "        if len(ner_labels) != len(sentence):\n",
    "            display(group)\n",
    "            print(len(sentence), len(ner_labels))\n",
    "            raise AssertionError(\"Predicted labels length mismatch sentence length.\")\n",
    "\n",
    "        group['ner_labels'] = ner_labels\n",
    "        chunk_results.append(group)\n",
    "\n",
    "    return chunk_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting and tokenizing data <!-- Andmete lugemine kogumine ja margendamine -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='andmete_lugemine_kogumine_ja_margendamine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading text from files, tokenizing texts and save them file by file. <!--Failidest teksti lugemine, märgendamine ja salvestamine failhaaval -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting `.json` files from directory <!--`json` failide kogumine kaustast-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = '_plain_texts_json'\n",
    "\n",
    "jsons = os.listdir(in_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data creation and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./andmestik.csv'):\n",
    "    create_df_enc2017_csv(jsons, in_dir, 'andmestik.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning NaN values in columns form and pos with an empty string\n",
      "Removing NaN words\n",
      "Modified dataframe saved to andmestik.csv\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"andmestik.csv\"\n",
    "df = pd.read_csv(csv_file, keep_default_na=False)\n",
    "clean_df(df, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'labels' created\n"
     ]
    }
   ],
   "source": [
    "create_labels_column(df, csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verification <!-- Kontrollimine -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kontrollimine_1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring that correct info was collected from the json file<!-- Kontrollitakse, et json failist said õiged infokillud kogutud -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For JSON file\n",
    "text = estnltk.converters.json_to_text(file='.\\\\_plain_texts_json\\\\nc_255_27981.json')\n",
    "morph_analysis = text.tag_layer('morph_analysis')\n",
    "ma_json = morph_analysis.sentences[0].morph_analysis\n",
    "test_json = pd.DataFrame(data=list(zip(ma_json.text, ma_json.form, ma_json.partofspeech)), columns=(\"text\", \"form\", \"partofspeech\"))\n",
    "# display(test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CSV file\n",
    "lines = list()\n",
    "\n",
    "with open('.\\\\_plain_texts_csv\\\\nc_255_27981.csv', 'r') as f:\n",
    "    csvFile = csv.reader(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for line in csvFile:\n",
    "        if line:\n",
    "            lines.append(line)\n",
    "\n",
    "test_csv = pd.DataFrame(lines[1:], columns=lines[0])\n",
    "\n",
    "# display(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Pandas dataframe\n",
    "test_csv = df[df['source'] == 'nc_255_27981.json']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the first sentence in file `nc_255_27981`<!-- Näide esimesest lausest failis `nc_255_27981` -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>form</th>\n",
       "      <th>partofspeech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>samas</td>\n",
       "      <td>(sg in)</td>\n",
       "      <td>(P)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kestab</td>\n",
       "      <td>(b)</td>\n",
       "      <td>(V)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>kohtuvaidlus</td>\n",
       "      <td>(sg n)</td>\n",
       "      <td>(S)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>veel</td>\n",
       "      <td>()</td>\n",
       "      <td>(D)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30</td>\n",
       "      <td>(?)</td>\n",
       "      <td>(N)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FIEna</td>\n",
       "      <td>(sg es)</td>\n",
       "      <td>(Y)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ametis</td>\n",
       "      <td>(sg in)</td>\n",
       "      <td>(S)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>olnud</td>\n",
       "      <td>(nud, , sg n, pl n)</td>\n",
       "      <td>(V, A, A, A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>töötaja</td>\n",
       "      <td>(sg g)</td>\n",
       "      <td>(S)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>palga</td>\n",
       "      <td>(sg g)</td>\n",
       "      <td>(S)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kompenseerimiseks</td>\n",
       "      <td>(sg tr)</td>\n",
       "      <td>(S)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>,</td>\n",
       "      <td>()</td>\n",
       "      <td>(Z)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>teatas</td>\n",
       "      <td>(s)</td>\n",
       "      <td>(V)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ETA</td>\n",
       "      <td>(?)</td>\n",
       "      <td>(Y)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>.</td>\n",
       "      <td>()</td>\n",
       "      <td>(Z)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text                 form  partofspeech\n",
       "15              samas              (sg in)           (P)\n",
       "16             kestab                  (b)           (V)\n",
       "17       kohtuvaidlus               (sg n)           (S)\n",
       "18               veel                   ()           (D)\n",
       "19                 30                  (?)           (N)\n",
       "20              FIEna              (sg es)           (Y)\n",
       "21             ametis              (sg in)           (S)\n",
       "22              olnud  (nud, , sg n, pl n)  (V, A, A, A)\n",
       "23            töötaja               (sg g)           (S)\n",
       "24              palga               (sg g)           (S)\n",
       "25  kompenseerimiseks              (sg tr)           (S)\n",
       "26                  ,                   ()           (Z)\n",
       "27             teatas                  (s)           (V)\n",
       "28                ETA                  (?)           (Y)\n",
       "29                  .                   ()           (Z)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>form</th>\n",
       "      <th>pos</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4861762</th>\n",
       "      <td>0</td>\n",
       "      <td>samas</td>\n",
       "      <td>sg in</td>\n",
       "      <td>P</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861763</th>\n",
       "      <td>0</td>\n",
       "      <td>kestab</td>\n",
       "      <td>b</td>\n",
       "      <td>V</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861764</th>\n",
       "      <td>0</td>\n",
       "      <td>kohtuvaidlus</td>\n",
       "      <td>sg n</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861765</th>\n",
       "      <td>0</td>\n",
       "      <td>veel</td>\n",
       "      <td></td>\n",
       "      <td>D</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861766</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>?</td>\n",
       "      <td>N</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861767</th>\n",
       "      <td>0</td>\n",
       "      <td>FIEna</td>\n",
       "      <td>sg es</td>\n",
       "      <td>Y</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861768</th>\n",
       "      <td>0</td>\n",
       "      <td>ametis</td>\n",
       "      <td>sg in</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861769</th>\n",
       "      <td>0</td>\n",
       "      <td>olnud</td>\n",
       "      <td>nud</td>\n",
       "      <td>V</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861770</th>\n",
       "      <td>0</td>\n",
       "      <td>töötaja</td>\n",
       "      <td>sg g</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861771</th>\n",
       "      <td>0</td>\n",
       "      <td>palga</td>\n",
       "      <td>sg g</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861772</th>\n",
       "      <td>0</td>\n",
       "      <td>kompenseerimiseks</td>\n",
       "      <td>sg tr</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861773</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861774</th>\n",
       "      <td>0</td>\n",
       "      <td>teatas</td>\n",
       "      <td>s</td>\n",
       "      <td>V</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861775</th>\n",
       "      <td>0</td>\n",
       "      <td>ETA</td>\n",
       "      <td>?</td>\n",
       "      <td>Y</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861776</th>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id              words   form pos         type  \\\n",
       "4861762            0              samas  sg in   P  periodicals   \n",
       "4861763            0             kestab      b   V  periodicals   \n",
       "4861764            0       kohtuvaidlus   sg n   S  periodicals   \n",
       "4861765            0               veel          D  periodicals   \n",
       "4861766            0                 30      ?   N  periodicals   \n",
       "4861767            0              FIEna  sg es   Y  periodicals   \n",
       "4861768            0             ametis  sg in   S  periodicals   \n",
       "4861769            0              olnud    nud   V  periodicals   \n",
       "4861770            0            töötaja   sg g   S  periodicals   \n",
       "4861771            0              palga   sg g   S  periodicals   \n",
       "4861772            0  kompenseerimiseks  sg tr   S  periodicals   \n",
       "4861773            0                  ,          Z  periodicals   \n",
       "4861774            0             teatas      s   V  periodicals   \n",
       "4861775            0                ETA      ?   Y  periodicals   \n",
       "4861776            0                  .          Z  periodicals   \n",
       "\n",
       "                    source  \n",
       "4861762  nc_255_27981.json  \n",
       "4861763  nc_255_27981.json  \n",
       "4861764  nc_255_27981.json  \n",
       "4861765  nc_255_27981.json  \n",
       "4861766  nc_255_27981.json  \n",
       "4861767  nc_255_27981.json  \n",
       "4861768  nc_255_27981.json  \n",
       "4861769  nc_255_27981.json  \n",
       "4861770  nc_255_27981.json  \n",
       "4861771  nc_255_27981.json  \n",
       "4861772  nc_255_27981.json  \n",
       "4861773  nc_255_27981.json  \n",
       "4861774  nc_255_27981.json  \n",
       "4861775  nc_255_27981.json  \n",
       "4861776  nc_255_27981.json  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_json[-15:])\n",
    "display(test_csv[test_csv.sentence_id == 0][-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics <!-- Statistika -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='statistika'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file\n",
    "csv_file = \"andmestik.csv\"\n",
    "df = pd.read_csv(csv_file, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token count in each text type<!-- Sõnade arv igal tekstiliigil -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "type\n",
       "fiction             2170692\n",
       "science             2121583\n",
       "periodicals         2099374\n",
       "wikipedia           2007179\n",
       "blogs_and_forums    1991307\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 10390135\n"
     ]
    }
   ],
   "source": [
    "print(\"Word count\")\n",
    "display(df['type'].value_counts())\n",
    "print(f'Sum: {df['type'].count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence count in each text type<!-- Lausete arv igal tekstiliigil -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence count\n",
      "periodicals: 144979\n",
      "fiction: 165249\n",
      "science: 129810\n",
      "blogs_and_forums: 159826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia: 136377\n",
      "\n",
      "Sum: 736241\n"
     ]
    }
   ],
   "source": [
    "text_types = df['type'].unique().tolist()\n",
    "print(\"Sentence count\")\n",
    "for text_type in text_types:\n",
    "    sentence_grouped = df[df['type'] == text_type].groupby(['source', 'sentence_id'])\n",
    "    sentence_count = sentence_grouped.ngroups\n",
    "    print(f\"{text_type}: {sentence_count}\")\n",
    "sentence_grouped = df.groupby(['source', 'sentence_id'])\n",
    "sentence_count = sentence_grouped.ngroups\n",
    "print(f\"\\nSum: {sentence_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File count in each text type<!-- Failide arv tekstiliigiti -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File count\n",
      "periodicals: 5917\n",
      "fiction: 53\n",
      "science: 229\n",
      "blogs_and_forums: 3016\n",
      "wikipedia: 9270\n",
      "\n",
      "Sum: 18485\n"
     ]
    }
   ],
   "source": [
    "print(\"File count\")\n",
    "for text_type in text_types:\n",
    "    print(f\"{text_type}: {len(df['source'][df['type'] == text_type].value_counts().tolist())}\")\n",
    "print(f\"\\nSum: {len(df['source'].value_counts().tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs word count for each source (i.e. text file) present in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_each_source_word_count(data):\n",
    "    \"\"\"Outputs word count for each source (i.e. text file) present in the data\n",
    "\n",
    "    Args:\n",
    "        data (pandas.core.frame.DataFrame): Pandas dataframe\n",
    "    \"\"\"\n",
    "    for text_type in text_types:\n",
    "        print(f\"{text_type}, sources: {len(data['source'][data['type'] == text_type].value_counts().tolist())}\")\n",
    "        display(data['source'][data['type'] == text_type].value_counts().head(5))\n",
    "        if len(data['source'][data['type'] == text_type].value_counts().tolist()) > 5:\n",
    "            display(data['source'][data['type'] == text_type].value_counts().tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_each_source_word_count(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data splitting<!-- ### Andmete jagamine treening-, test- ja valideerimishulkadesse -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='andmete_jagamine_treening_test_ja_valideerimishulkadesse'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the data used in training was based on the [statistics](https://github.com/UniversalDependencies/UD_Estonian-EDT/blob/master/stats.xml) found in [Estonian UD EDT treebank](https://github.com/UniversalDependencies/UD_Estonian-EDT). Data is gathered in such a way that all text types have more or less proportionate numbers of words.\n",
    "\n",
    "|Text type|Word count|\n",
    "|---|---|\n",
    "|Fiction|             115847|\n",
    "|Blogs and forums|    115719|\n",
    "|Science|             115347|\n",
    "|Periodicals|         115050|\n",
    "|Wikipedia|           115014|\n",
    "|**Sum**|             576977|\n",
    "\n",
    "The tag that the model starts to predict consists of a concatenation of columns `form` and `pos` connected with underscore _ symbol (e.g. `form` s and `pos` V will create s_V). In the data those concatenations will be stored in the column `labels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grouping data by text type <!-- #### Andmete grupeerimine tekstiliigiti -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='andmete_grupeerimine_tekstiliigiti'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file\n",
    "csv_file = \"andmestik.csv\"\n",
    "df = pd.read_csv(csv_file, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#pos_labels = ['A', 'C', 'D', 'G', 'H', 'I', 'J', 'K', 'N', 'O', 'P', 'S', 'U', 'V', 'X', 'Y', 'Z']\n",
    "#form_labels = ['ab', 'abl', 'ad', 'adt', 'all', 'el', 'es', 'g', 'ill', 'in', 'kom', 'n', 'p', 'pl', 'sg', 'ter', 'tr', 'b', 'd', 'da', 'des', 'ge', 'gem', 'gu', 'ks', 'ksid', 'ksime', 'ksin', 'ksite', 'ma', 'maks', 'mas', 'mast', 'mata', 'me', 'n', 'neg', 'neg ge', 'neg gem', 'neg gu', 'neg ks', 'neg me', 'neg nud', 'neg nuks', 'neg o', 'neg vat', 'neg tud', 'nud', 'nuks', 'nuksid', 'nuksime', 'nuksin', 'nuksite', 'nuvat', 'o', 's', 'sid', 'sime', 'sin', 'site', 'ta', 'tagu', 'taks', 'takse', 'tama', 'tav', 'tavat', 'te', 'ti', 'tud', 'tuks', 'tuvat', 'v', 'vad', 'vat']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels that have '?' in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['?_Y', '?_N', '?_O', '?_H', '?_S', '?_A', '?_Z'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_mark_labels = df[df['labels'].str.contains(r\"\\?\")]\n",
    "question_mark_labels['labels'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating column 'labels'\n",
      "Column 'labels' created\n",
      "List of unique labels created\n"
     ]
    }
   ],
   "source": [
    "unique_labels = get_unique_labels()\n",
    "# display(unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving unique labels for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./unique_labels.json\"):\n",
    "    with open(\"unique_labels.json\", 'w') as f:\n",
    "        json.dump(unique_labels, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gather_rows_for_text_type(df, 115000) #115000 words per text type for NER model\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "fiction             115847\n",
       "blogs_and_forums    115719\n",
       "science             115347\n",
       "periodicals         115050\n",
       "wikipedia           115014\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 576977\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "display(data['type'].value_counts())\n",
    "print(f\"Sum: {data['words'].size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathers text types for evaluating model by text type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_types = data['type'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving model data to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(path_or_buf='model_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting data to sets<!-- #### Andmete jagamine hulkadesse -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='andmete_jagamine_hulkadesse'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file\n",
    "data = pd.read_csv(\"model_data.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping data by filename to preserve the integrity of texts<!-- Andmete grupeerimine failinimede kaupa, et säilitada tekstide terviklikkus -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby('source')\n",
    "\n",
    "groups = list(grouped.groups.keys())\n",
    "train_groups, test_groups = sk.model_selection.train_test_split(groups, test_size=0.2, random_state=42)\n",
    "\n",
    "def filter_by_group(df, groups):\n",
    "    return df[df['source'].isin(groups)]\n",
    "\n",
    "# Splitting dataframe\n",
    "train_df = filter_by_group(data, train_groups)\n",
    "test_df = filter_by_group(data, test_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unnecessary columns for the model<!-- Mudelile ebavajalike veergude eemaldamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(labels=['type', 'source'], axis=1)\n",
    "test_df = test_df.drop(labels=['type', 'source'], axis=1)\n",
    "# display(train_df)\n",
    "# display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token classification model (for assigning morpological categories)<!-- ### NER Mudel -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ner_mudel'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unique_labels.json\", 'r') as f:\n",
    "    unique_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model<!-- Mudeli ülesehitamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at EMBEDDIA/est-roberta and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger('simpletransformers.ner.ner_model')\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Suppress specific warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning) # For warning message \"FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated.\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # For warnings from seqeval.metrics like \"UserWarning: <tag> seems not to be NE tag.\"\n",
    "\n",
    "# Configurations\n",
    "model_args = NERArgs()\n",
    "model_args.train_batch_size = 8\n",
    "model_args.evaluate_during_training = False\n",
    "model_args.learning_rate = 5e-5\n",
    "model_args.num_train_epochs = 10\n",
    "model_args.use_early_stopping = True\n",
    "model_args.use_cuda = torch.cuda.is_available()  # Use GPU if available\n",
    "model_args.save_eval_checkpoints = False\n",
    "model_args.save_model_every_epoch = False # Takes a lot of storage space\n",
    "model_args.save_steps = -1\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.cache_dir = 'NER_mudel/cache'\n",
    "model_args.best_model_dir = 'NER_mudel/best_model'\n",
    "model_args.output_dir = 'NER_mudel'\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.silent = True\n",
    "\n",
    "# Initialization\n",
    "model = NERModel(\"camembert\", \"EMBEDDIA/est-roberta\", args=model_args, labels=unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model<!-- Mudeli treenimine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\".\\\\NER_mudel\\\\config.json\"):\n",
    "    # Train model\n",
    "    print(\"Training model\")\n",
    "    model.train_model(train_df, eval_data=test_df)\n",
    "else:\n",
    "    model = NERModel(\"camembert\", \"NER_mudel\", args=model_args, labels=unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model<!-- Mudeli hindamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:50<00:00, 50.58s/it]\n",
      "Running Evaluation:   0%|          | 0/4 [00:00<?, ?it/s]e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:1303: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n",
      "Running Evaluation: 100%|██████████| 4/4 [00:00<00:00,  5.77it/s]\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: nud_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: s_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: tud_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg abl_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: Z seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl all_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ?_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: K seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sid_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: J seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg es_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ill_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: b_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ?_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: da_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: neg_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: o_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: vat_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: adt_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg es_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: vad_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ma_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl kom_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ti_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ksid_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl all_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ?_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ill_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg abl_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: takse_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: G seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: n_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ks_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ab_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: neg nud_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ad_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: neg o_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sime_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg abl_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg abl_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: X seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ?_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: adt_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: des_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sin_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: tama_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ksime_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ad_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: me_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl es_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl abl_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl all_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ad_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl es_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: mast_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ill_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: adt_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: mata_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ill_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: nuks_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl tr_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: site_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ill_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ta_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl abl_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: neg ks_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ksin_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: gem_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ill_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: d_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ill_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: te_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: gu_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg es_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: mas_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl kom_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl all_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg es_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: maks_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: taks_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ksite_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ter_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ter_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ill_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ?_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl abl_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl tr_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl tr_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: tav_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ge_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl tr_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl kom_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl tr_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg es_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl all_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg abl_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ab_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ab_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1253: {'eval_loss': 0.24354228004813194, 'precision': 0.9536447592575988, 'recall': 0.9518951089631718, 'f1_score': 0.95276913085347}\n",
      "{'eval_loss': 0.24354228004813194, 'precision': 0.9536447592575988, 'recall': 0.9518951089631718, 'f1_score': 0.95276913085347}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, preds_list = model.eval_model(test_df)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluation Loss:{result['eval_loss']:.4f}\")\n",
    "print(f\"Precision: \\t{result['precision']:.4f}\")\n",
    "print(f\"Recall: \\t{result['recall']:.4f}\")\n",
    "print(f\"F1 Score: \\t{result['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see a lot of UserWarnings are logged in the output. This does not impose a problem because the model can still predict these tags. This is now fixed by configuring the logger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model by text type<!-- Mudeli hindamine tekstiliigiti -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_type in text_types:\n",
    "\n",
    "    print(f\"Text type:\\t{text_type}\")\n",
    "\n",
    "    text_type_data = data.loc[data['type'] == text_type]\n",
    "\n",
    "    print(f\"Token count: {len(text_type_data)}\")\n",
    "\n",
    "    print(f\"Starting evaluation\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    result, model_outputs, preds_list = model.eval_model(text_type_data)\n",
    "    print(result)\n",
    "    print(f\"Evaluation Loss:{result['eval_loss']:.4f}\")\n",
    "    print(f\"Precision: \\t{result['precision']:.4f}\")\n",
    "    print(f\"Recall: \\t{result['recall']:.4f}\")\n",
    "    print(f\"F1 Score: \\t{result['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions with the model <!-- Mudelil ennustamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'A. H. Tammsaare': 'sg n_H'}, {'oli': 's_V'}, {'eesti': 'G'}, {'kirjanik': 'sg n_S'}, {',': 'Z'}, {'esseist': 'sg n_S'}, {',': 'Z'}, {'kultuurifilosoof': 'sg n_S'}, {'ja': 'J'}, {'tõlkija': 'sg n_S'}, {'.': 'Z'}]]\n"
     ]
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict([['A. H. Tammsaare',\n",
    " 'oli',\n",
    " 'eesti',\n",
    " 'kirjanik',\n",
    " ',',\n",
    " 'esseist',\n",
    " ',',\n",
    " 'kultuurifilosoof',\n",
    " 'ja',\n",
    " 'tõlkija',\n",
    " '.']], split_on_space=False)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'A. H. Tammsaare': 'sg n_H'}, {'oli': 's_V'}, {'eesti': 'G'}, {'kirjanik': 'sg n_S'}, {',': 'Z'}, {'esseist': 'sg n_S'}, {',': 'Z'}, {'kultuurifilosoof': 'sg n_S'}, {'ja': 'J'}, {'tõlkija': 'sg n_S'}, {'.': 'Z'}], [{'Üksnes': 'D'}, {'autorihüvitis': 'sg n_S'}, {'oli': 's_V'}, {'12 431': '?_N'}, {'krooni': 'sg p_S'}, {'.': 'Z'}]]\n"
     ]
    }
   ],
   "source": [
    "test_text = estnltk.Text(\"A. H. Tammsaare oli eesti kirjanik, esseist, kultuurifilosoof ja tõlkija. Üksnes autorihüvitis oli 12 431 krooni.\")\n",
    "test_text.tag_layer(\"morph_analysis\")\n",
    "test_text.morph_analysis;\n",
    "sentences = [s.text for s in test_text.sentences]\n",
    "\n",
    "# Predict tags\n",
    "predictions, raw_outputs = model.predict(sentences, split_on_space=False)\n",
    "\n",
    "# Output of predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top n tags prediction<!-- ### Top n märgendi ennustamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: <s>\n",
      "  Tag 1: Z (Probability: 0.9998)\n",
      "  Tag 2: J (Probability: 0.0000)\n",
      "  Tag 3: ?_Y (Probability: 0.0000)\n",
      "Token: A\n",
      "  Tag 1: sg n_H (Probability: 0.9954)\n",
      "  Tag 2: sg g_H (Probability: 0.0024)\n",
      "  Tag 3: ?_Y (Probability: 0.0004)\n",
      "Token: .\n",
      "  Tag 1: Z (Probability: 0.9995)\n",
      "  Tag 2: ?_Y (Probability: 0.0001)\n",
      "  Tag 3: J (Probability: 0.0001)\n",
      "Token: H\n",
      "  Tag 1: sg n_H (Probability: 0.9968)\n",
      "  Tag 2: ?_Y (Probability: 0.0010)\n",
      "  Tag 3: sg g_H (Probability: 0.0004)\n",
      "Token: .\n",
      "  Tag 1: Z (Probability: 0.9993)\n",
      "  Tag 2: J (Probability: 0.0002)\n",
      "  Tag 3: ?_Y (Probability: 0.0001)\n",
      "Token: Tammsaare\n",
      "  Tag 1: sg n_H (Probability: 0.9994)\n",
      "  Tag 2: sg g_H (Probability: 0.0002)\n",
      "  Tag 3: sg n_S (Probability: 0.0001)\n",
      "Token: oli\n",
      "  Tag 1: s_V (Probability: 0.9999)\n",
      "  Tag 2: sg n_S (Probability: 0.0000)\n",
      "  Tag 3: A (Probability: 0.0000)\n",
      "Token: eesti\n",
      "  Tag 1: G (Probability: 0.9958)\n",
      "  Tag 2: sg g_S (Probability: 0.0004)\n",
      "  Tag 3: sg g_H (Probability: 0.0003)\n",
      "Token: kirjanik\n",
      "  Tag 1: sg n_S (Probability: 0.9997)\n",
      "  Tag 2: sg n_A (Probability: 0.0000)\n",
      "  Tag 3: sg in_S (Probability: 0.0000)\n",
      "Token: ,\n",
      "  Tag 1: Z (Probability: 0.9999)\n",
      "  Tag 2: J (Probability: 0.0000)\n",
      "  Tag 3: b_V (Probability: 0.0000)\n",
      "Token: es\n",
      "  Tag 1: sg n_S (Probability: 0.9996)\n",
      "  Tag 2: sg n_A (Probability: 0.0001)\n",
      "  Tag 3: sg in_S (Probability: 0.0000)\n",
      "Token: se\n",
      "  Tag 1: sg n_S (Probability: 0.9996)\n",
      "  Tag 2: sg n_A (Probability: 0.0001)\n",
      "  Tag 3: sg in_S (Probability: 0.0000)\n",
      "Token: ist\n",
      "  Tag 1: sg n_S (Probability: 0.9996)\n",
      "  Tag 2: sg n_A (Probability: 0.0001)\n",
      "  Tag 3: sg in_S (Probability: 0.0000)\n",
      "Token: ,\n",
      "  Tag 1: Z (Probability: 0.9999)\n",
      "  Tag 2: J (Probability: 0.0000)\n",
      "  Tag 3: b_V (Probability: 0.0000)\n",
      "Token: kultuuri\n",
      "  Tag 1: sg n_S (Probability: 0.9996)\n",
      "  Tag 2: sg n_A (Probability: 0.0001)\n",
      "  Tag 3: sg g_S (Probability: 0.0000)\n",
      "Token: filosoof\n",
      "  Tag 1: sg n_S (Probability: 0.9997)\n",
      "  Tag 2: sg in_S (Probability: 0.0000)\n",
      "  Tag 3: sg n_A (Probability: 0.0000)\n",
      "Token: ja\n",
      "  Tag 1: J (Probability: 0.9999)\n",
      "  Tag 2: Z (Probability: 0.0000)\n",
      "  Tag 3: o_V (Probability: 0.0000)\n",
      "Token: tõlkija\n",
      "  Tag 1: sg n_S (Probability: 0.9993)\n",
      "  Tag 2: sg g_S (Probability: 0.0001)\n",
      "  Tag 3: sg n_A (Probability: 0.0001)\n",
      "Token: .\n",
      "  Tag 1: Z (Probability: 1.0000)\n",
      "  Tag 2: J (Probability: 0.0000)\n",
      "  Tag 3: b_V (Probability: 0.0000)\n",
      "Token: </s>\n",
      "  Tag 1: sg n_H (Probability: 0.5349)\n",
      "  Tag 2: Z (Probability: 0.2840)\n",
      "  Tag 3: sg g_H (Probability: 0.0472)\n"
     ]
    }
   ],
   "source": [
    "# Top n tags prediction for tokens\n",
    "# # Determine the device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Ensure model is on the right device\n",
    "# model.model.to(device)\n",
    "\n",
    "# # Sample input sentence\n",
    "# sentence = \"A. H. Tammsaare oli eesti kirjanik, esseist, kultuurifilosoof ja tõlkija.\"\n",
    "\n",
    "# # Tokenize and predict (This is normally done internally by simpletransformers)\n",
    "# inputs = model.tokenizer(sentence, return_tensors=\"pt\")\n",
    "# inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "\n",
    "# # Get the logits from the model\n",
    "# model_output = model.model(**inputs)\n",
    "# logits = model_output.logits  # Ensure logits are on the same device\n",
    "\n",
    "# # Apply softmax to logits to get probabilities\n",
    "# probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "# # Get top n tags for each token\n",
    "# top_n = 3\n",
    "# top_n_probs, top_n_indices = torch.topk(probs, top_n, dim=-1)\n",
    "\n",
    "# # Decode the tags\n",
    "# for token_idx in range(len(inputs[\"input_ids\"][0])):\n",
    "#     token = model.tokenizer.decode([inputs[\"input_ids\"][0][token_idx]])\n",
    "#     print(f\"Token: {token}\")\n",
    "#     for i in range(top_n):\n",
    "#         label_idx = top_n_indices[0][token_idx][i].item()\n",
    "#         label_prob = top_n_probs[0][token_idx][i].item()\n",
    "#         label = model.args.labels_list[label_idx]\n",
    "#         print(f\"  Tag {i+1}: {label} (Probability: {label_prob:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: A. H. Tammsaare\n",
      "\tTag: sg n_H \t Confidence: 0.9976\n",
      "\tTag: sg g_H \t Confidence: 0.0012\n",
      "\tTag: ?_Y \t Confidence: 0.0001\n",
      "Word: oli\n",
      "\tTag: s_V \t Confidence: 1.0000\n",
      "\tTag: sg n_S \t Confidence: 0.0000\n",
      "\tTag: A \t Confidence: 0.0000\n",
      "Word: eesti\n",
      "\tTag: G \t Confidence: 0.9956\n",
      "\tTag: sg g_S \t Confidence: 0.0004\n",
      "\tTag: sg g_H \t Confidence: 0.0002\n",
      "Word: kirjanik\n",
      "\tTag: sg n_S \t Confidence: 0.9995\n",
      "\tTag: sg in_S \t Confidence: 0.0000\n",
      "\tTag: sg g_S \t Confidence: 0.0000\n",
      "Word: ,\n",
      "\tTag: Z \t Confidence: 1.0000\n",
      "\tTag: J \t Confidence: 0.0000\n",
      "\tTag: A \t Confidence: 0.0000\n",
      "Word: esseist\n",
      "\tTag: sg n_S \t Confidence: 0.9995\n",
      "\tTag: sg n_A \t Confidence: 0.0001\n",
      "\tTag: sg in_S \t Confidence: 0.0000\n",
      "Word: ,\n",
      "\tTag: Z \t Confidence: 1.0000\n",
      "\tTag: J \t Confidence: 0.0000\n",
      "\tTag: s_V \t Confidence: 0.0000\n",
      "Word: kultuurifilosoof\n",
      "\tTag: sg n_S \t Confidence: 0.9995\n",
      "\tTag: sg n_A \t Confidence: 0.0000\n",
      "\tTag: sg in_S \t Confidence: 0.0000\n",
      "Word: ja\n",
      "\tTag: J \t Confidence: 1.0000\n",
      "\tTag: Z \t Confidence: 0.0000\n",
      "\tTag: o_V \t Confidence: 0.0000\n",
      "Word: tõlkija\n",
      "\tTag: sg n_S \t Confidence: 0.9995\n",
      "\tTag: sg g_S \t Confidence: 0.0001\n",
      "\tTag: sg n_A \t Confidence: 0.0001\n",
      "Word: .\n",
      "\tTag: Z \t Confidence: 1.0000\n",
      "\tTag: J \t Confidence: 0.0000\n",
      "\tTag: A \t Confidence: 0.0000\n"
     ]
    }
   ],
   "source": [
    "predict_top_n_tags(model, [['A. H. Tammsaare',\n",
    " 'oli',\n",
    " 'eesti',\n",
    " 'kirjanik',\n",
    " ',',\n",
    " 'esseist',\n",
    " ',',\n",
    " 'kultuurifilosoof',\n",
    " 'ja',\n",
    " 'tõlkija',\n",
    " '.']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token classification model (for assigning morpological categories) NER model by text type<!-- ### NER mudel tekstiliigiti -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ner_mudel_tekstiliigiti'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tekstiliik:\tblogs_and_forums\n",
      "Suurus: 115719 sõna\n",
      "Alustan hindamisega\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.49s/it]\n",
      "Running Evaluation:   0%|          | 0/9 [00:00<?, ?it/s]e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:1303: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n",
      "Running Evaluation: 100%|██████████| 9/9 [00:01<00:00,  6.79it/s]\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: b_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: Z seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ?_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: K seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: des_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: J seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: vad_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: takse_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl all_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: da_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ?_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ?_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sin_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ab_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: neg_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: nud_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: s_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ad_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: n_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ill_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: o_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: me_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: adt_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: neg o_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl kom_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ksin_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl abl_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ad_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sid_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: tud_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl all_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ks_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ma_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl all_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ill_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: mas_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ta_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg abl_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg abl_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: adt_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: neg nud_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ill_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: d_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ?_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg es_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: G seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: vat_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ter_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ad_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: neg ks_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl abl_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg abl_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ter_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl es_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sime_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ti_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ill_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ter_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: adt_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ge_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: nuks_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: adt_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ?_Z seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ad_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl tr_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: adt_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl kom_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: tama_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ksid_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg es_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ill_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: mast_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ksime_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ter_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: te_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: mata_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl tr_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: gu_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: maks_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg abl_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl tr_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl all_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg n_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg g_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl es_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ?_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ab_S seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: X seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: taks_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ad_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl all_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: neg ge_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ill_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: adt_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg es_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl abl_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ill_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg es_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: tavat_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: tav_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg in_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ad_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ad_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl g_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg p_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ter_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg es_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ter_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ab_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: gem_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ter_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl kom_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ill_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl tr_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl kom_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl all_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl es_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ad_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ab_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1253: {'eval_loss': 0.08652005354977316, 'precision': 0.9740840914679124, 'recall': 0.9738446411012782, 'f1_score': 0.9739643515673017}\n",
      "{'eval_loss': 0.08652005354977316, 'precision': 0.9740840914679124, 'recall': 0.9738446411012782, 'f1_score': 0.9739643515673017}\n",
      "Tekstiliik:\tfiction\n",
      "Suurus: 115847 sõna\n",
      "Alustan hindamisega\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:18<00:00,  1.24s/it]\n",
      "Running Evaluation: 100%|██████████| 75/75 [00:12<00:00,  6.13it/s]\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ill_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg all_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: adt_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ill_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ter_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl kom_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: tagu_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ill_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: neg vat_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: neg gu_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ab_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg abl_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl n_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl kom_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg el_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg tr_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg es_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl p_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: site_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg es_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl tr_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1253: {'eval_loss': 0.0043835937487892804, 'precision': 0.9991472669907052, 'recall': 0.9991046685141761, 'f1_score': 0.9991259672983863}\n",
      "{'eval_loss': 0.0043835937487892804, 'precision': 0.9991472669907052, 'recall': 0.9991046685141761, 'f1_score': 0.9991259672983863}\n",
      "Tekstiliik:\tperiodicals\n",
      "Suurus: 115050 sõna\n",
      "Alustan hindamisega\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.00s/it]\n",
      "Running Evaluation: 100%|██████████| 3/3 [00:00<00:00,  6.96it/s]\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg abl_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ksite_V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1253: {'eval_loss': 0.11051202068726222, 'precision': 0.9810868635002139, 'recall': 0.9799128130609454, 'f1_score': 0.9804994868286008}\n",
      "{'eval_loss': 0.11051202068726222, 'precision': 0.9810868635002139, 'recall': 0.9799128130609454, 'f1_score': 0.9804994868286008}\n",
      "Tekstiliik:\tscience\n",
      "Suurus: 115347 sõna\n",
      "Alustan hindamisega\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.86s/it]\n",
      "Running Evaluation: 100%|██████████| 4/4 [00:00<00:00,  6.16it/s]\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl kom_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ill_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg kom_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl all_O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl ab_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: sg ter_Y seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl tr_H seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl in_U seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1253: {'eval_loss': 0.1578108500689268, 'precision': 0.9701866365031914, 'recall': 0.971324570273003, 'f1_score': 0.9707552699124881}\n",
      "{'eval_loss': 0.1578108500689268, 'precision': 0.9701866365031914, 'recall': 0.971324570273003, 'f1_score': 0.9707552699124881}\n",
      "Tekstiliik:\twikipedia\n",
      "Suurus: 115014 sõna\n",
      "Alustan hindamisega\n",
      "INFO:ner_model.py:1884:  Converting to features started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.05s/it]\n",
      "Running Evaluation: 100%|██████████| 4/4 [00:00<00:00,  8.53it/s]\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ?_A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl es_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: pl el_N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:ner_model.py:1253: {'eval_loss': 0.32173459976911545, 'precision': 0.9479583095374072, 'recall': 0.9465393114263312, 'f1_score': 0.9472482790598138}\n",
      "{'eval_loss': 0.32173459976911545, 'precision': 0.9479583095374072, 'recall': 0.9465393114263312, 'f1_score': 0.9472482790598138}\n"
     ]
    }
   ],
   "source": [
    "for text_type in text_types:\n",
    "\n",
    "    print(f\"Text type:\\t{text_type}\")\n",
    "\n",
    "    text_type_data = data.loc[data['type'] == text_type]\n",
    "\n",
    "    print(f\"Token count: {len(text_type_data)}\")\n",
    "\n",
    "    grouped = data.groupby('source')\n",
    "\n",
    "    groups = list(grouped.groups.keys())\n",
    "    train_groups, test_groups = sk.model_selection.train_test_split(groups, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Split the dataframe\n",
    "    train_df = df[df['source'].isin(train_groups)]\n",
    "    test_df = df[df['source'].isin(test_groups)]\n",
    "\n",
    "    print(f\"Train- and testset created.\\Beginning to train\")\n",
    "\n",
    "    # Model\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    transformers_logger = logging.getLogger(\"transformers\")\n",
    "    transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "    # Configure the model\n",
    "    model_args = NERArgs()\n",
    "    model_args.train_batch_size = 8\n",
    "    model_args.evaluate_during_training = False\n",
    "    model_args.learning_rate = 5e-5\n",
    "    model_args.num_train_epochs = 10\n",
    "    model_args.use_early_stopping = True\n",
    "    model_args.use_cuda = torch.cuda.is_available()  # Use GPU if available\n",
    "    model_args.overwrite_output_dir = True\n",
    "    model_args.save_eval_checkpoints = False\n",
    "    model_args.save_model_every_epoch = False\n",
    "    model_args.save_steps = -1\n",
    "    model_args.cache_dir = text_type + '_NER_mudel/cache'\n",
    "    model_args.best_model_dir = text_type + '_NER_mudel/best_model'\n",
    "    model_args.output_dir = text_type + '_NER_mudel'\n",
    "\n",
    "    # Initialize the model\n",
    "    model = NERModel(\"camembert\", \"EMBEDDIA/est-roberta\", args=model_args, labels=unique_labels)\n",
    "\n",
    "    # Train the model\n",
    "    model.train_model(train_df, eval_data=test_df)\n",
    "\n",
    "    print(f\"Starting evaluation\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    result, model_outputs, preds_list = model.eval_model(text_type_data)\n",
    "    print(result)\n",
    "    print(f\"Evaluation Loss:{result['eval_loss']:.4f}\")\n",
    "    print(f\"Precision: \\t{result['precision']:.4f}\")\n",
    "    print(f\"Recall: \\t{result['recall']:.4f}\")\n",
    "    print(f\"F1 Score: \\t{result['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New trained and evaluated NER models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Text type         | Token count | eval_loss            | precision          | recall             | f1_score           |\n",
    "|-------------------|-------------|----------------------|--------------------|--------------------|--------------------|\n",
    "| blogs_and_forums  | 115719      | 0.23976334929466248  | 0.9538220130015691 | 0.9520293551707164 | 0.9529248409925647 |\n",
    "| fiction           | 115847      | 0.2396537810564041   | 0.9538792523867151 | 0.952342596321654  | 0.9531103049845493 |\n",
    "| periodicals       | 115050      | 0.24317898601293564  | 0.9550244383659926 | 0.9530585760952253 | 0.9540404945350295 |\n",
    "| science           | 115347      | 0.2376849427819252   | 0.9557577659240665 | 0.9541325457555824 | 0.9549444643496955 |\n",
    "| wikipedia         | 115014      | 0.23429438471794128  | 0.9547409930094999 | 0.953416565982011  | 0.9540783198620781 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General NER model evaluations by text type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Text type         | Token count | eval_loss            | precision          | recall             | f1_score           |\n",
    "|-------------------|-------------|----------------------|--------------------|--------------------|--------------------|\n",
    "| blogs_and_forums  | 115719      | 0.08935269061475992  | 0.974046101995129  | 0.9733038348082595 | 0.9736748269375007 |\n",
    "| fiction           | 115847      | 0.004841572717996314 | 0.9990726034260374 | 0.9989767640162013 | 0.9990246814225794 |\n",
    "| periodicals       | 115050      | 0.09677119304736455  | 0.9807560725282244 | 0.9801692452346354 | 0.9804625710743448 |\n",
    "| science           | 115347      | 0.1642883848398924   | 0.9677601809954751 | 0.9688169868554095 | 0.9682882955716798 |\n",
    "| wikipedia         | 115014      | 0.2906533405184746   | 0.9479240975888144 | 0.9471808396892152 | 0.9475523228865832 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering unused data and comparing Vabamorf / NER model <!-- Kasutamata andmete kogumine ja Vabamorfi ning NER mudeli võrdlemine -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kasutamata_andmete_kogumine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gathering unused data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file\n",
    "df = pd.read_csv(\"andmestik.csv\", keep_default_na=False)\n",
    "model_df = pd.read_csv(\"model_data.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both DataFrames have the same columns for comparison\n",
    "common_columns = df.columns.intersection(model_df.columns)\n",
    "# Perform a left anti-join to get the unused data\n",
    "unused_data = df.merge(model_df, on=common_columns.tolist(), how='left', indicator=True)\n",
    "unused_data = unused_data[unused_data['_merge'] == 'left_only'].drop(columns=['_merge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>form</th>\n",
       "      <th>pos</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131164</th>\n",
       "      <td>0</td>\n",
       "      <td>BAGDAD</td>\n",
       "      <td>sg n</td>\n",
       "      <td>H</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642042.json</td>\n",
       "      <td>sg n_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131165</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642042.json</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131166</th>\n",
       "      <td>0</td>\n",
       "      <td>29.</td>\n",
       "      <td>?</td>\n",
       "      <td>O</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642042.json</td>\n",
       "      <td>?_O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131167</th>\n",
       "      <td>0</td>\n",
       "      <td>november</td>\n",
       "      <td>sg n</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642042.json</td>\n",
       "      <td>sg n_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131168</th>\n",
       "      <td>0</td>\n",
       "      <td>(</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642042.json</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555188</th>\n",
       "      <td>2</td>\n",
       "      <td>tulemusena</td>\n",
       "      <td>sg es</td>\n",
       "      <td>S</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99964_x.json</td>\n",
       "      <td>sg es_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555189</th>\n",
       "      <td>2</td>\n",
       "      <td>[</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99964_x.json</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555190</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>?</td>\n",
       "      <td>N</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99964_x.json</td>\n",
       "      <td>?_N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555191</th>\n",
       "      <td>2</td>\n",
       "      <td>]</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99964_x.json</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555192</th>\n",
       "      <td>2</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99964_x.json</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9813158 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sentence_id       words   form pos         type  \\\n",
       "131164              0      BAGDAD   sg n   H  periodicals   \n",
       "131165              0           ,          Z  periodicals   \n",
       "131166              0         29.      ?   O  periodicals   \n",
       "131167              0    november   sg n   S  periodicals   \n",
       "131168              0           (          Z  periodicals   \n",
       "...               ...         ...    ...  ..          ...   \n",
       "10555188            2  tulemusena  sg es   S    wikipedia   \n",
       "10555189            2           [          Z    wikipedia   \n",
       "10555190            2           2      ?   N    wikipedia   \n",
       "10555191            2           ]          Z    wikipedia   \n",
       "10555192            2           .          Z    wikipedia   \n",
       "\n",
       "                        source   labels  \n",
       "131164    nc_10532_642042.json   sg n_H  \n",
       "131165    nc_10532_642042.json        Z  \n",
       "131166    nc_10532_642042.json      ?_O  \n",
       "131167    nc_10532_642042.json   sg n_S  \n",
       "131168    nc_10532_642042.json        Z  \n",
       "...                        ...      ...  \n",
       "10555188   wiki17_99964_x.json  sg es_S  \n",
       "10555189   wiki17_99964_x.json        Z  \n",
       "10555190   wiki17_99964_x.json      ?_N  \n",
       "10555191   wiki17_99964_x.json        Z  \n",
       "10555192   wiki17_99964_x.json        Z  \n",
       "\n",
       "[9813158 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print or save the result\n",
    "display(unused_data)\n",
    "if not os.path.exists(\"./unused_data.csv\"):\n",
    "    unused_data.to_csv(\"unused_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_data = pd.read_csv(\"unused_data.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = gather_rows_for_text_type(unused_data, 600000, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./comparison_data.csv\"):\n",
    "    comparison_data.to_csv(\"comparison_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = pd.read_csv(\"comparison_data.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>form</th>\n",
       "      <th>pos</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Koju</td>\n",
       "      <td>adt</td>\n",
       "      <td>S</td>\n",
       "      <td>blogs_and_forums</td>\n",
       "      <td>web13_274106_x.json</td>\n",
       "      <td>adt_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>jõudes</td>\n",
       "      <td>des</td>\n",
       "      <td>V</td>\n",
       "      <td>blogs_and_forums</td>\n",
       "      <td>web13_274106_x.json</td>\n",
       "      <td>des_V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ootas</td>\n",
       "      <td>s</td>\n",
       "      <td>V</td>\n",
       "      <td>blogs_and_forums</td>\n",
       "      <td>web13_274106_x.json</td>\n",
       "      <td>s_V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ees</td>\n",
       "      <td></td>\n",
       "      <td>D</td>\n",
       "      <td>blogs_and_forums</td>\n",
       "      <td>web13_274106_x.json</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>üllatus</td>\n",
       "      <td>sg n</td>\n",
       "      <td>S</td>\n",
       "      <td>blogs_and_forums</td>\n",
       "      <td>web13_274106_x.json</td>\n",
       "      <td>sg n_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056351</th>\n",
       "      <td>17</td>\n",
       "      <td>aastaks</td>\n",
       "      <td>sg tr</td>\n",
       "      <td>S</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_85786_x.json</td>\n",
       "      <td>sg tr_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056352</th>\n",
       "      <td>17</td>\n",
       "      <td>oli</td>\n",
       "      <td>s</td>\n",
       "      <td>V</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_85786_x.json</td>\n",
       "      <td>s_V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056353</th>\n",
       "      <td>17</td>\n",
       "      <td>ta</td>\n",
       "      <td>sg n</td>\n",
       "      <td>P</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_85786_x.json</td>\n",
       "      <td>sg n_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056354</th>\n",
       "      <td>17</td>\n",
       "      <td>surnud</td>\n",
       "      <td></td>\n",
       "      <td>A</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_85786_x.json</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056355</th>\n",
       "      <td>17</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_85786_x.json</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3056356 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id    words   form pos              type  \\\n",
       "0                  0     Koju    adt   S  blogs_and_forums   \n",
       "1                  0   jõudes    des   V  blogs_and_forums   \n",
       "2                  0    ootas      s   V  blogs_and_forums   \n",
       "3                  0      ees          D  blogs_and_forums   \n",
       "4                  0  üllatus   sg n   S  blogs_and_forums   \n",
       "...              ...      ...    ...  ..               ...   \n",
       "3056351           17  aastaks  sg tr   S         wikipedia   \n",
       "3056352           17      oli      s   V         wikipedia   \n",
       "3056353           17       ta   sg n   P         wikipedia   \n",
       "3056354           17   surnud          A         wikipedia   \n",
       "3056355           17        .          Z         wikipedia   \n",
       "\n",
       "                      source   labels  \n",
       "0        web13_274106_x.json    adt_S  \n",
       "1        web13_274106_x.json    des_V  \n",
       "2        web13_274106_x.json      s_V  \n",
       "3        web13_274106_x.json        D  \n",
       "4        web13_274106_x.json   sg n_S  \n",
       "...                      ...      ...  \n",
       "3056351  wiki17_85786_x.json  sg tr_S  \n",
       "3056352  wiki17_85786_x.json      s_V  \n",
       "3056353  wiki17_85786_x.json   sg n_P  \n",
       "3056354  wiki17_85786_x.json        A  \n",
       "3056355  wiki17_85786_x.json        Z  \n",
       "\n",
       "[3056356 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using own custom prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_groups = comparison_data.groupby(['source', 'sentence_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unique_labels.json\", 'r') as f:\n",
    "    unique_labels = json.load(f)\n",
    "\n",
    "model = initialize_model(\"NER_mudel\", unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_chunks = process_groups(model, comparison_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_comparison_data = pd.concat(predicted_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_comparison_data.to_csv(\"updated_comparison_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_comparison_data = pd.read_csv('./updated_comparison_data.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_differences = updated_comparison_data[updated_comparison_data['labels'] != updated_comparison_data['ner_labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_differences.to_csv(\"label_differences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>form</th>\n",
       "      <th>pos</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>labels</th>\n",
       "      <th>ner_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "      <td>sg g</td>\n",
       "      <td>H</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642051.json</td>\n",
       "      <td>sg g_H</td>\n",
       "      <td>sg n_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Reuters-EPLO</td>\n",
       "      <td>?</td>\n",
       "      <td>Y</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642051.json</td>\n",
       "      <td>?_Y</td>\n",
       "      <td>sg n_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>sg g</td>\n",
       "      <td>H</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642051.json</td>\n",
       "      <td>sg g_H</td>\n",
       "      <td>sg n_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>sajalases</td>\n",
       "      <td>sg in</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642051.json</td>\n",
       "      <td>sg in_S</td>\n",
       "      <td>sg in_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2</td>\n",
       "      <td>Timothy</td>\n",
       "      <td>sg g</td>\n",
       "      <td>H</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_10532_642051.json</td>\n",
       "      <td>sg g_H</td>\n",
       "      <td>sg n_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056133</th>\n",
       "      <td>20</td>\n",
       "      <td>maini</td>\n",
       "      <td>pl ter</td>\n",
       "      <td>S</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99902_x.json</td>\n",
       "      <td>pl ter_S</td>\n",
       "      <td>sg ter_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056235</th>\n",
       "      <td>6</td>\n",
       "      <td>Kloogaranna</td>\n",
       "      <td>sg n</td>\n",
       "      <td>H</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99953_x.json</td>\n",
       "      <td>sg n_H</td>\n",
       "      <td>sg g_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056247</th>\n",
       "      <td>7</td>\n",
       "      <td>ehitatud</td>\n",
       "      <td>tud</td>\n",
       "      <td>V</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99953_x.json</td>\n",
       "      <td>tud_V</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056303</th>\n",
       "      <td>9</td>\n",
       "      <td>teise</td>\n",
       "      <td>adt</td>\n",
       "      <td>P</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99953_x.json</td>\n",
       "      <td>adt_P</td>\n",
       "      <td>sg g_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056315</th>\n",
       "      <td>0</td>\n",
       "      <td>bioloogia</td>\n",
       "      <td>sg n</td>\n",
       "      <td>S</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wiki17_99964_x.json</td>\n",
       "      <td>sg n_S</td>\n",
       "      <td>sg g_S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139917 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id         words    form pos         type  \\\n",
       "0                  0      OKLAHOMA    sg g   H  periodicals   \n",
       "5                  0  Reuters-EPLO       ?   Y  periodicals   \n",
       "8                  0      Oklahoma    sg g   H  periodicals   \n",
       "47                 1     sajalases   sg in   S  periodicals   \n",
       "61                 2       Timothy    sg g   H  periodicals   \n",
       "...              ...           ...     ...  ..          ...   \n",
       "3056133           20         maini  pl ter   S    wikipedia   \n",
       "3056235            6   Kloogaranna    sg n   H    wikipedia   \n",
       "3056247            7      ehitatud     tud   V    wikipedia   \n",
       "3056303            9         teise     adt   P    wikipedia   \n",
       "3056315            0     bioloogia    sg n   S    wikipedia   \n",
       "\n",
       "                       source    labels ner_labels  \n",
       "0        nc_10532_642051.json    sg g_H     sg n_H  \n",
       "5        nc_10532_642051.json       ?_Y     sg n_H  \n",
       "8        nc_10532_642051.json    sg g_H     sg n_H  \n",
       "47       nc_10532_642051.json   sg in_S    sg in_A  \n",
       "61       nc_10532_642051.json    sg g_H     sg n_H  \n",
       "...                       ...       ...        ...  \n",
       "3056133   wiki17_99902_x.json  pl ter_S   sg ter_S  \n",
       "3056235   wiki17_99953_x.json    sg n_H     sg g_H  \n",
       "3056247   wiki17_99953_x.json     tud_V          A  \n",
       "3056303   wiki17_99953_x.json     adt_P     sg g_P  \n",
       "3056315   wiki17_99964_x.json    sg n_S     sg g_S  \n",
       "\n",
       "[139917 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(label_differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences count: 139917 / 3056356 (4.578%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Differences count: {len(label_differences)} / {len(updated_comparison_data)} ({round(len(label_differences) / len(updated_comparison_data) * 100, 3)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using EstNLTK Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = pd.read_csv(\"comparison_data.csv\", keep_default_na=False)\n",
    "in_dir = '_plain_texts_json'\n",
    "jsons = comparison_data['source'].unique().tolist()\n",
    "morph_tagger = BertMorphTagger('./NER_mudel/', get_top_n_predictions=1, token_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_diff_finder = MorphDiffFinder( \n",
    "                                    'morph_analysis', \n",
    "                                    'bert_morph_tagging', \n",
    "                                    diff_attribs  = ['partofspeech', 'form'], \n",
    "                                    focus_attribs = ['word', 'ending', 'clitic', 'partofspeech', 'form'] )\n",
    "morph_diff_summarizer = MorphDiffSummarizer('morph_analysis', 'bert_morph_tagging' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning tokenization file by file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 341/5778 [00:55<38:51,  2.33it/s] c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_A', 'probabilities': 0.99956}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  6%|▌         | 349/5778 [01:28<1:01:54,  1.46it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'D', 'probabilities': 0.9998}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  6%|▋         | 364/5778 [02:27<3:20:49,  2.23s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_S', 'probabilities': 0.99034}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_S', 'probabilities': 0.99654}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_S', 'probabilities': 0.99872}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  6%|▋         | 370/5778 [02:41<2:39:02,  1.76s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'D', 'probabilities': 0.99688}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_S', 'probabilities': 0.99934}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  7%|▋         | 378/5778 [02:55<2:24:52,  1.61s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98986}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99365}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'b_V', 'probabilities': 0.99993}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  7%|▋         | 392/5778 [04:38<6:50:33,  4.57s/it] c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl el_S', 'probabilities': 0.99907}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  8%|▊         | 445/5778 [06:47<2:15:46,  1.53s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl el_S', 'probabilities': 0.99905}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  8%|▊         | 448/5778 [07:42<13:54:52,  9.40s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'gu_V', 'probabilities': 0.99715}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  8%|▊         | 453/5778 [08:10<10:10:27,  6.88s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl in_S', 'probabilities': 0.99845}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  8%|▊         | 466/5778 [08:35<2:02:36,  1.38s/it] c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl kom_S', 'probabilities': 0.99909}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl kom_S', 'probabilities': 0.99908}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  8%|▊         | 467/5778 [08:40<3:24:20,  2.31s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99334}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  8%|▊         | 474/5778 [08:53<3:18:17,  2.24s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.89421}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  8%|▊         | 477/5778 [09:00<2:47:05,  1.89s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg tr_A', 'probabilities': 0.9991}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  9%|▉         | 517/5778 [11:01<2:09:34,  1.48s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.65926}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl ad_S', 'probabilities': 0.99925}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "  9%|▉         | 548/5778 [12:38<3:26:47,  2.37s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.96384}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 10%|▉         | 552/5778 [12:52<5:01:33,  3.46s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_S', 'probabilities': 0.99852}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 11%|█         | 612/5778 [15:09<3:51:31,  2.69s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'ma_V', 'probabilities': 0.88383}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 11%|█         | 613/5778 [15:18<6:20:26,  4.42s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg tr_S', 'probabilities': 0.63454}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 11%|█         | 634/5778 [16:10<2:46:54,  1.95s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl kom_S', 'probabilities': 0.99906}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_S', 'probabilities': 0.99945}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 11%|█         | 650/5778 [16:46<3:04:55,  2.16s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl el_S', 'probabilities': 0.99879}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg tr_S', 'probabilities': 0.99881}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl p_S', 'probabilities': 0.99923}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg tr_S', 'probabilities': 0.99839}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99972}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 11%|█▏        | 653/5778 [16:53<3:22:58,  2.38s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99958}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 12%|█▏        | 669/5778 [17:32<3:27:12,  2.43s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99459}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 12%|█▏        | 674/5778 [17:36<1:34:00,  1.11s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'D', 'probabilities': 0.89234}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 12%|█▏        | 694/5778 [18:12<1:37:51,  1.15s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.54554}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.9988}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.91177}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.97805}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 12%|█▏        | 700/5778 [18:26<4:12:07,  2.98s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_S', 'probabilities': 0.99953}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 12%|█▏        | 715/5778 [18:54<2:02:59,  1.46s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'neg_V', 'probabilities': 0.37614}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 13%|█▎        | 751/5778 [20:15<2:47:08,  1.99s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl ad_S', 'probabilities': 0.99911}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 13%|█▎        | 753/5778 [20:39<8:42:41,  6.24s/it] c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.96847}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 13%|█▎        | 760/5778 [20:51<3:02:04,  2.18s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_S', 'probabilities': 0.9993}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 13%|█▎        | 780/5778 [21:35<1:25:05,  1.02s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.87497}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 14%|█▎        | 782/5778 [21:56<6:51:46,  4.95s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl g_S', 'probabilities': 0.99908}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 15%|█▍        | 853/5778 [24:49<2:00:12,  1.46s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'D', 'probabilities': 0.4236}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 15%|█▍        | 862/5778 [25:25<5:14:38,  3.84s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl p_S', 'probabilities': 0.99963}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_S', 'probabilities': 0.9992}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 15%|█▌        | 867/5778 [25:40<3:52:46,  2.84s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98709}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 16%|█▌        | 919/5778 [28:08<2:34:07,  1.90s/it] c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.6029}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.60108}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.90617}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg abl_S', 'probabilities': 0.99735}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 16%|█▌        | 923/5778 [28:29<5:07:06,  3.80s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.9994}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 16%|█▌        | 925/5778 [30:10<42:54:59, 31.84s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99974}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_A', 'probabilities': 0.99975}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.83192}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl g_S', 'probabilities': 0.99943}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg tr_A', 'probabilities': 0.99918}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_A', 'probabilities': 0.99963}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99976}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_C', 'probabilities': 0.99851}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_S', 'probabilities': 0.99964}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_S', 'probabilities': 0.9994}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg all_A', 'probabilities': 0.9973}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.89095}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_A', 'probabilities': 0.99976}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 16%|█▌        | 926/5778 [38:48<239:27:36, 177.67s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99948}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 16%|█▌        | 927/5778 [40:26<206:53:44, 153.54s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl kom_S', 'probabilities': 0.99907}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl el_S', 'probabilities': 0.99911}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 16%|█▌        | 928/5778 [43:40<223:07:50, 165.62s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_S', 'probabilities': 0.997}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg kom_S', 'probabilities': 0.99969}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_S', 'probabilities': 0.9944}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_S', 'probabilities': 0.93021}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.97399}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.98724}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 16%|█▌        | 929/5778 [51:31<346:30:28, 257.25s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl g_A', 'probabilities': 0.99892}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99789}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_S', 'probabilities': 0.99958}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl el_A', 'probabilities': 0.994}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_A', 'probabilities': 0.9997}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'D', 'probabilities': 0.99987}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_A', 'probabilities': 0.95289}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_C', 'probabilities': 0.99149}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'D', 'probabilities': 0.99986}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_S', 'probabilities': 0.99887}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl p_A', 'probabilities': 0.99928}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99962}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_A', 'probabilities': 0.9996}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'da_V', 'probabilities': 0.99986}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl p_S', 'probabilities': 0.9993}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'D', 'probabilities': 0.99988}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_S', 'probabilities': 0.99959}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_C', 'probabilities': 0.99872}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_A', 'probabilities': 0.99932}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99963}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_C', 'probabilities': 0.99221}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_S', 'probabilities': 0.99948}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_A', 'probabilities': 0.99957}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_A', 'probabilities': 0.99938}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_A', 'probabilities': 0.99971}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_A', 'probabilities': 0.99944}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_A', 'probabilities': 0.99909}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_A', 'probabilities': 0.99943}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_S', 'probabilities': 0.99965}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_A', 'probabilities': 0.99959}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_A', 'probabilities': 0.99958}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_S', 'probabilities': 0.99963}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.9997}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_A', 'probabilities': 0.99905}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl all_A', 'probabilities': 0.9915}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_A', 'probabilities': 0.99972}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl p_A', 'probabilities': 0.99942}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg all_A', 'probabilities': 0.99774}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_A', 'probabilities': 0.99973}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99973}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_A', 'probabilities': 0.99966}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_A', 'probabilities': 0.99792}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl el_A', 'probabilities': 0.99343}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 16%|█▌        | 931/5778 [1:00:59<334:01:10, 248.09s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl p_S', 'probabilities': 0.99951}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_S', 'probabilities': 0.99932}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl p_S', 'probabilities': 0.99931}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98929}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'nud_V', 'probabilities': 0.99947}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.9997}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.94774}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99975}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99925}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99579}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 16%|█▌        | 932/5778 [1:05:58<354:25:14, 263.29s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_S', 'probabilities': 0.99933}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 16%|█▋        | 939/5778 [1:11:30<45:28:02, 33.83s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98204}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 18%|█▊        | 1034/5778 [1:13:33<2:10:40,  1.65s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99845}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99649}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 18%|█▊        | 1058/5778 [1:14:00<1:14:16,  1.06it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.99476}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 19%|█▉        | 1125/5778 [1:15:09<56:37,  1.37it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'nud_V', 'probabilities': 0.99952}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 21%|██▏       | 1242/5778 [1:17:33<1:54:50,  1.52s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_A', 'probabilities': 0.99922}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 24%|██▍       | 1383/5778 [1:20:18<2:21:37,  1.93s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl ad_S', 'probabilities': 0.9992}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 24%|██▍       | 1411/5778 [1:20:46<47:22,  1.54it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg ad_A', 'probabilities': 0.99825}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_A', 'probabilities': 0.99933}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 25%|██▌       | 1467/5778 [1:21:41<41:07,  1.75it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.92264}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 26%|██▌       | 1483/5778 [1:22:03<2:00:32,  1.68s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_H', 'probabilities': 0.93321}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 26%|██▌       | 1516/5778 [1:22:45<1:11:07,  1.00s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_A', 'probabilities': 0.99946}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.99414}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 26%|██▋       | 1521/5778 [1:22:53<1:47:13,  1.51s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.99421}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 28%|██▊       | 1626/5778 [1:25:02<46:52,  1.48it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99969}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 28%|██▊       | 1627/5778 [1:25:02<36:45,  1.88it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.98921}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 29%|██▉       | 1669/5778 [1:26:31<43:41,  1.57it/s]   c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg kom_S', 'probabilities': 0.99967}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 31%|███       | 1797/5778 [1:29:34<1:43:13,  1.56s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99892}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99948}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 33%|███▎      | 1929/5778 [1:31:54<50:42,  1.26it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.81478}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 36%|███▌      | 2062/5778 [1:35:09<33:47,  1.83it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_S', 'probabilities': 0.99842}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl p_S', 'probabilities': 0.99942}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 36%|███▌      | 2068/5778 [1:35:18<56:38,  1.09it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.99435}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 36%|███▋      | 2100/5778 [1:35:55<1:04:57,  1.06s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'da_V', 'probabilities': 0.99984}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 37%|███▋      | 2156/5778 [1:37:00<1:23:30,  1.38s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg kom_S', 'probabilities': 0.99968}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 38%|███▊      | 2223/5778 [1:38:34<1:17:12,  1.30s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.67951}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 39%|███▊      | 2227/5778 [1:38:41<1:19:58,  1.35s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99819}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99915}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 39%|███▊      | 2238/5778 [1:38:52<1:19:33,  1.35s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99966}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 40%|████      | 2340/5778 [1:40:28<1:23:11,  1.45s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.91367}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 41%|████      | 2343/5778 [1:40:35<1:32:25,  1.61s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.99289}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.96227}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 41%|████      | 2372/5778 [1:43:57<5:16:48,  5.58s/it] c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.93541}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 42%|████▏     | 2403/5778 [1:44:47<56:26,  1.00s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_S', 'probabilities': 0.99545}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 42%|████▏     | 2438/5778 [1:45:20<48:57,  1.14it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl p_S', 'probabilities': 0.99965}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl kom_S', 'probabilities': 0.99001}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 43%|████▎     | 2483/5778 [1:46:26<42:54,  1.28it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl g_A', 'probabilities': 0.99888}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 44%|████▍     | 2548/5778 [1:47:44<1:43:31,  1.92s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_S', 'probabilities': 0.99614}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 47%|████▋     | 2732/5778 [1:51:37<1:19:21,  1.56s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.982}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99055}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.86218}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.98163}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.56466}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.9716}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_S', 'probabilities': 0.80384}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_S', 'probabilities': 0.70368}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_S', 'probabilities': 0.81319}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_S', 'probabilities': 0.73305}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 47%|████▋     | 2735/5778 [1:56:06<34:46:59, 41.15s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.84918}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.73839}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.92853}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9885}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 47%|████▋     | 2738/5778 [1:58:03<28:24:04, 33.63s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.69459}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.54419}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99959}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 47%|████▋     | 2743/5778 [2:08:15<55:57:21, 66.37s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99847}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 48%|████▊     | 2749/5778 [2:09:40<15:56:46, 18.95s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl g_S', 'probabilities': 0.99957}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl p_S', 'probabilities': 0.99946}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 48%|████▊     | 2750/5778 [2:09:53<14:14:39, 16.94s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98844}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg all_S', 'probabilities': 0.95952}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'adt_S', 'probabilities': 0.75628}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99775}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99692}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 48%|████▊     | 2751/5778 [2:10:00<11:57:31, 14.22s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg ill_S', 'probabilities': 0.20902}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 48%|████▊     | 2752/5778 [2:10:10<10:39:28, 12.68s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99955}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 48%|████▊     | 2753/5778 [2:11:46<31:47:00, 37.82s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99832}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.56498}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.98054}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.98975}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg tr_A', 'probabilities': 0.99919}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.65918}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99597}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99824}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99924}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 's_V', 'probabilities': 0.42624}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99952}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99954}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99955}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99775}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.84436}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.9995}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.9959}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.9985}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99955}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99945}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99849}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99946}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg abl_H', 'probabilities': 0.96357}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99935}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.85699}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg tr_A', 'probabilities': 0.99922}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.92224}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 48%|████▊     | 2763/5778 [2:18:39<9:43:30, 11.61s/it]   c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99575}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99782}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.98064}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 48%|████▊     | 2765/5778 [2:25:39<78:48:13, 94.16s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98702}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 48%|████▊     | 2767/5778 [2:25:49<40:47:58, 48.78s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.94616}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.89515}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.89001}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99873}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99934}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_S', 'probabilities': 0.50422}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.8619}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99893}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.73163}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_S', 'probabilities': 0.57082}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99744}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.92901}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.64276}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99949}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_S', 'probabilities': 0.55093}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99887}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.98622}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.96123}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.98297}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.60461}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.85062}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99753}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99078}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99882}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 's_V', 'probabilities': 0.99714}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99942}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.98114}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99738}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99753}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99601}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99902}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99804}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99911}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99587}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99119}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.97331}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99926}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.74129}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.98247}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.91471}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.92281}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.97877}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.88909}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99772}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.79723}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.98347}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.50468}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 48%|████▊     | 2771/5778 [2:31:41<44:01:37, 52.71s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97961}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 48%|████▊     | 2785/5778 [2:31:52<1:20:22,  1.61s/it] c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99956}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99962}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 48%|████▊     | 2799/5778 [2:32:03<30:18,  1.64it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98406}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98922}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98503}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98487}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99042}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.95234}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97556}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97889}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.96486}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.79316}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99439}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98664}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.78992}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 49%|████▉     | 2828/5778 [2:32:28<48:14,  1.02it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.96963}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 49%|████▉     | 2830/5778 [2:32:29<30:36,  1.61it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.96452}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 49%|████▉     | 2837/5778 [2:32:39<1:10:18,  1.43s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.68325}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.31397}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97971}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 49%|████▉     | 2839/5778 [2:32:39<42:40,  1.15it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg tr_A', 'probabilities': 0.99908}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 50%|████▉     | 2875/5778 [2:33:07<41:44,  1.16it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.87764}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.76039}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.78734}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 50%|████▉     | 2887/5778 [2:33:16<20:29,  2.35it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.33759}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 50%|█████     | 2892/5778 [2:33:18<17:34,  2.74it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.59746}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 50%|█████     | 2903/5778 [2:33:26<48:04,  1.00s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.94794}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 51%|█████     | 2928/5778 [2:33:43<30:50,  1.54it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99923}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 51%|█████     | 2929/5778 [2:33:51<2:10:29,  2.75s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.93247}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99731}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99894}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.88412}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.34374}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.97763}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 51%|█████     | 2932/5778 [2:33:53<1:09:40,  1.47s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.83093}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 51%|█████     | 2934/5778 [2:33:54<44:04,  1.08it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.93726}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97301}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 51%|█████     | 2952/5778 [2:34:07<50:44,  1.08s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99189}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 51%|█████▏    | 2973/5778 [2:34:27<40:56,  1.14it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99553}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 51%|█████▏    | 2974/5778 [2:34:36<2:29:10,  3.19s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_S', 'probabilities': 0.99437}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 52%|█████▏    | 2983/5778 [2:34:43<34:48,  1.34it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.93674}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98464}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_Y', 'probabilities': 0.08959}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.92747}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.7038}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97584}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.94993}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99786}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99725}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 52%|█████▏    | 2995/5778 [2:35:02<19:33,  2.37it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98043}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 52%|█████▏    | 2999/5778 [2:35:08<41:16,  1.12it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_H', 'probabilities': 0.90529}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 52%|█████▏    | 3007/5778 [2:35:12<16:29,  2.80it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99867}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99714}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.7016}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 52%|█████▏    | 3009/5778 [2:35:17<1:07:03,  1.45s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.42962}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.88955}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_H', 'probabilities': 0.68618}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 52%|█████▏    | 3011/5778 [2:35:17<39:17,  1.17it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99076}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 52%|█████▏    | 3013/5778 [2:35:18<32:59,  1.40it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.9909}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 52%|█████▏    | 3020/5778 [2:35:23<32:04,  1.43it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99555}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99502}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 52%|█████▏    | 3028/5778 [2:35:32<29:05,  1.58it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.90893}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 52%|█████▏    | 3030/5778 [2:35:35<1:05:33,  1.43s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_H', 'probabilities': 0.97132}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 53%|█████▎    | 3051/5778 [2:35:53<33:12,  1.37it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99473}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.96189}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.82095}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 53%|█████▎    | 3061/5778 [2:35:57<15:43,  2.88it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.86764}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 53%|█████▎    | 3067/5778 [2:36:06<1:10:40,  1.56s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.58134}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.36862}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 53%|█████▎    | 3069/5778 [2:36:07<45:14,  1.00s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.9528}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.39549}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 53%|█████▎    | 3071/5778 [2:36:07<28:41,  1.57it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.731}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99501}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 53%|█████▎    | 3080/5778 [2:36:11<15:57,  2.82it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99733}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.51706}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.9503}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 53%|█████▎    | 3088/5778 [2:36:20<30:30,  1.47it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.67353}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 54%|█████▍    | 3110/5778 [2:36:38<29:29,  1.51it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.9994}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99944}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99938}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 54%|█████▍    | 3111/5778 [2:36:43<1:32:35,  2.08s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.37819}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.95357}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 54%|█████▍    | 3112/5778 [2:36:44<1:07:09,  1.51s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99548}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 54%|█████▍    | 3113/5778 [2:36:45<1:06:04,  1.49s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99959}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 54%|█████▍    | 3115/5778 [2:36:46<47:09,  1.06s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99865}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.67107}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 54%|█████▍    | 3141/5778 [2:37:08<21:52,  2.01it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99686}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.23557}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99337}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 55%|█████▍    | 3154/5778 [2:37:13<07:38,  5.73it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99301}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.95116}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99234}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.36036}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.32592}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.97951}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.9566}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.97289}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.98649}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.9949}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.45474}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.85292}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 55%|█████▌    | 3179/5778 [2:37:29<11:48,  3.67it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98722}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 55%|█████▌    | 3188/5778 [2:37:33<11:11,  3.86it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.92641}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.96119}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97115}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 56%|█████▌    | 3214/5778 [2:37:43<22:24,  1.91it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.84117}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 56%|█████▌    | 3216/5778 [2:37:43<18:18,  2.33it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.59044}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 56%|█████▌    | 3233/5778 [2:37:55<11:50,  3.58it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97603}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98605}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 56%|█████▌    | 3240/5778 [2:38:00<28:25,  1.49it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9863}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97589}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 56%|█████▌    | 3246/5778 [2:38:01<11:31,  3.66it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.98041}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.83674}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.68015}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 57%|█████▋    | 3269/5778 [2:38:09<12:19,  3.39it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99896}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_H', 'probabilities': 0.94192}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 57%|█████▋    | 3271/5778 [2:38:18<1:26:51,  2.08s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.88386}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97804}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.8201}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.89955}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.69183}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.55227}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 57%|█████▋    | 3282/5778 [2:38:22<13:33,  3.07it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.96593}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.47776}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99144}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 57%|█████▋    | 3290/5778 [2:38:26<24:41,  1.68it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99593}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99616}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99356}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99409}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'vad_V', 'probabilities': 0.06393}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97494}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98846}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98178}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.94667}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.72337}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98551}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98431}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.94467}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.94294}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.92627}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9668}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.55725}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99625}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 57%|█████▋    | 3307/5778 [2:38:33<17:41,  2.33it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.56956}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_H', 'probabilities': 0.50832}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 57%|█████▋    | 3311/5778 [2:38:35<18:27,  2.23it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.32865}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 57%|█████▋    | 3315/5778 [2:38:40<31:04,  1.32it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99869}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99849}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 57%|█████▋    | 3317/5778 [2:38:48<1:15:13,  1.83s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.96358}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.98851}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.87737}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.97327}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.36541}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99914}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 57%|█████▋    | 3322/5778 [2:38:52<47:21,  1.16s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99146}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 58%|█████▊    | 3333/5778 [2:39:20<1:15:09,  1.84s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.98705}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99695}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 58%|█████▊    | 3368/5778 [2:39:43<34:26,  1.17it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.91093}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.96136}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 58%|█████▊    | 3370/5778 [2:39:43<22:51,  1.76it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99416}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 59%|█████▊    | 3382/5778 [2:39:57<40:17,  1.01s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97793}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 59%|█████▊    | 3385/5778 [2:39:58<19:44,  2.02it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99967}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_S', 'probabilities': 0.99932}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99965}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99964}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 59%|█████▉    | 3403/5778 [2:40:06<15:12,  2.60it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99666}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 59%|█████▉    | 3406/5778 [2:40:07<13:25,  2.95it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.94786}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99535}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99486}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97529}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 59%|█████▉    | 3411/5778 [2:40:12<25:13,  1.56it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99334}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9943}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.80396}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.84118}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.67231}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 59%|█████▉    | 3421/5778 [2:40:17<11:35,  3.39it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_H', 'probabilities': 0.98524}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_H', 'probabilities': 0.98911}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 59%|█████▉    | 3428/5778 [2:40:24<1:00:12,  1.54s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_H', 'probabilities': 0.98885}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 60%|█████▉    | 3453/5778 [2:40:39<38:33,  1.01it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.96397}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97246}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 60%|█████▉    | 3456/5778 [2:40:41<25:42,  1.51it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.25808}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 60%|██████    | 3488/5778 [2:40:54<33:34,  1.14it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.2822}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 60%|██████    | 3491/5778 [2:40:55<20:59,  1.82it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.92802}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.96549}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 61%|██████    | 3504/5778 [2:41:02<21:50,  1.74it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99579}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99791}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 61%|██████    | 3507/5778 [2:41:04<20:08,  1.88it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.32015}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.976}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.63919}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.92482}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 61%|██████    | 3528/5778 [2:41:22<53:18,  1.42s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99819}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.79853}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.98722}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 61%|██████    | 3534/5778 [2:41:38<1:23:56,  2.24s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.51389}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99558}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 61%|██████▏   | 3553/5778 [2:41:46<09:58,  3.72it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.81522}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.86517}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.58728}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.63293}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.7425}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.53486}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 62%|██████▏   | 3555/5778 [2:41:57<1:33:47,  2.53s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97223}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98363}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 62%|██████▏   | 3560/5778 [2:41:59<34:41,  1.07it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.44851}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 62%|██████▏   | 3563/5778 [2:42:00<21:22,  1.73it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99822}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_S', 'probabilities': 0.99892}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 62%|██████▏   | 3590/5778 [2:42:17<10:55,  3.34it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98628}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 62%|██████▏   | 3596/5778 [2:42:27<1:17:25,  2.13s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.93295}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.97783}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.72954}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.87943}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.85589}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.24544}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 62%|██████▏   | 3602/5778 [2:42:35<47:54,  1.32s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98992}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 62%|██████▏   | 3608/5778 [2:42:38<31:31,  1.15it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_S', 'probabilities': 0.99802}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.30441}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97636}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.67022}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.33985}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.69956}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.68448}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.44273}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.63092}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.95292}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 63%|██████▎   | 3622/5778 [2:42:45<16:28,  2.18it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99364}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 63%|██████▎   | 3639/5778 [2:43:03<53:56,  1.51s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.77556}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_A', 'probabilities': 0.26541}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99428}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97852}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 63%|██████▎   | 3648/5778 [2:43:12<34:21,  1.03it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99127}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 63%|██████▎   | 3658/5778 [2:43:23<34:49,  1.01it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl el_A', 'probabilities': 0.99426}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.26276}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.39452}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99968}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 64%|██████▎   | 3676/5778 [2:43:36<16:16,  2.15it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.93176}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.74556}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.43911}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_H', 'probabilities': 0.9877}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 64%|██████▍   | 3699/5778 [2:43:58<39:21,  1.14s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.91837}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.91842}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 64%|██████▍   | 3707/5778 [2:44:07<48:39,  1.41s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97646}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 64%|██████▍   | 3716/5778 [2:44:10<11:16,  3.05it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.389}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.73265}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.61014}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 64%|██████▍   | 3722/5778 [2:44:13<18:48,  1.82it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.97224}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 65%|██████▍   | 3734/5778 [2:44:21<30:00,  1.14it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99805}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99899}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99597}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99825}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99864}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 65%|██████▍   | 3743/5778 [2:44:25<09:48,  3.46it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99943}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.98134}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.96638}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.77408}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99929}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99921}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.96987}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99951}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99941}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99665}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.58002}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99744}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.45789}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 65%|██████▌   | 3758/5778 [2:44:36<14:34,  2.31it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.86959}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.98882}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 66%|██████▌   | 3786/5778 [2:44:56<17:54,  1.85it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99832}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 66%|██████▌   | 3791/5778 [2:44:59<13:53,  2.38it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99675}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.97488}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 66%|██████▌   | 3793/5778 [2:45:01<21:39,  1.53it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99931}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 66%|██████▌   | 3799/5778 [2:45:06<39:25,  1.20s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99842}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99829}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_H', 'probabilities': 0.9807}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98436}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99919}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99568}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99352}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99795}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 66%|██████▌   | 3810/5778 [2:45:19<24:32,  1.34it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9968}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 66%|██████▌   | 3812/5778 [2:45:30<1:29:57,  2.75s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.68941}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.96755}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 66%|██████▌   | 3817/5778 [2:45:37<47:50,  1.46s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.80691}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.98423}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 66%|██████▋   | 3839/5778 [2:45:53<28:31,  1.13it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.95971}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.87639}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98374}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98838}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9956}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.992}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.72552}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.91504}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 67%|██████▋   | 3862/5778 [2:46:13<35:16,  1.10s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_H', 'probabilities': 0.98853}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 67%|██████▋   | 3868/5778 [2:46:16<21:07,  1.51it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.53294}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 67%|██████▋   | 3887/5778 [2:46:26<13:37,  2.31it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.9922}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 67%|██████▋   | 3893/5778 [2:46:28<08:49,  3.56it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.52252}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.87247}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.92369}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_S', 'probabilities': 0.59854}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98649}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 68%|██████▊   | 3907/5778 [2:46:38<19:22,  1.61it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.76531}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.67417}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.52452}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.77302}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.87105}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.54206}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 68%|██████▊   | 3908/5778 [2:46:39<21:46,  1.43it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.5916}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99131}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 68%|██████▊   | 3930/5778 [2:46:57<22:33,  1.37it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl g_H', 'probabilities': 0.21594}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl g_H', 'probabilities': 0.14866}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.40579}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98941}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.51348}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.96227}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 68%|██████▊   | 3932/5778 [2:46:57<16:42,  1.84it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.991}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 68%|██████▊   | 3936/5778 [2:47:04<29:01,  1.06it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.92551}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.24188}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 68%|██████▊   | 3940/5778 [2:47:04<10:51,  2.82it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.94475}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.55032}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.50298}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.40323}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.92124}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 68%|██████▊   | 3949/5778 [2:47:17<20:53,  1.46it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.75657}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 69%|██████▉   | 3979/5778 [2:47:40<12:55,  2.32it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99957}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 69%|██████▉   | 3990/5778 [2:47:58<31:58,  1.07s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99391}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99913}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 69%|██████▉   | 3997/5778 [2:48:04<16:59,  1.75it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99329}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.31468}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.96345}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 69%|██████▉   | 4012/5778 [2:48:12<14:43,  2.00it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.53569}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.67383}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.9723}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 69%|██████▉   | 4014/5778 [2:48:14<21:17,  1.38it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.77758}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 70%|██████▉   | 4020/5778 [2:48:17<13:33,  2.16it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99122}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.28391}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.45022}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_S', 'probabilities': 0.18307}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.9981}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.76317}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.75733}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg ad_S', 'probabilities': 0.99726}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.91186}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.65926}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl g_S', 'probabilities': 0.39914}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99813}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99848}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 70%|██████▉   | 4028/5778 [2:48:25<16:29,  1.77it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.94226}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.89183}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 70%|██████▉   | 4037/5778 [2:48:28<16:06,  1.80it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.98609}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 70%|██████▉   | 4042/5778 [2:48:33<27:05,  1.07it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99669}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 70%|███████   | 4045/5778 [2:48:38<37:03,  1.28s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.5724}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.95145}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99276}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.78126}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.97844}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 70%|███████   | 4059/5778 [2:48:51<17:04,  1.68it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.84211}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.54197}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.87378}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.68927}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.58804}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.73183}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.98811}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.90684}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 71%|███████   | 4082/5778 [2:49:14<11:58,  2.36it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.64832}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.49145}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.37051}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 71%|███████   | 4090/5778 [2:49:17<09:25,  2.99it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.71715}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 71%|███████   | 4111/5778 [2:49:30<21:50,  1.27it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99148}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 71%|███████▏  | 4117/5778 [2:49:33<20:15,  1.37it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.95648}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 71%|███████▏  | 4124/5778 [2:49:52<43:03,  1.56s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'I', 'probabilities': 0.3151}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 71%|███████▏  | 4126/5778 [2:49:53<26:48,  1.03it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.86772}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.37617}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99192}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.97405}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99273}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99843}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99583}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.94106}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.94102}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99748}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.45473}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.63175}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99821}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.72125}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99927}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.98159}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.67458}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.98687}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99823}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.63227}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.49683}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.81367}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 71%|███████▏  | 4131/5778 [2:49:59<23:45,  1.16it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99374}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99457}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99863}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.9909}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 72%|███████▏  | 4142/5778 [2:50:06<15:48,  1.73it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.90929}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.93626}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.818}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.86723}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.9059}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 72%|███████▏  | 4153/5778 [2:50:14<18:43,  1.45it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.65718}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.54725}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99569}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 72%|███████▏  | 4166/5778 [2:50:21<17:37,  1.52it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.54078}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 72%|███████▏  | 4171/5778 [2:50:23<11:37,  2.30it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98368}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 73%|███████▎  | 4201/5778 [2:50:44<10:26,  2.52it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_A', 'probabilities': 0.99974}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 73%|███████▎  | 4207/5778 [2:50:57<49:02,  1.87s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.85005}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9823}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.96687}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99254}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.62428}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.88522}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.92093}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98477}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.90216}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.54783}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.52115}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99306}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9438}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 74%|███████▎  | 4250/5778 [2:51:24<09:54,  2.57it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.97862}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99574}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.5815}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 74%|███████▍  | 4302/5778 [2:51:49<17:59,  1.37it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_N', 'probabilities': 0.97423}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 74%|███████▍  | 4304/5778 [2:51:58<56:24,  2.30s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99169}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 75%|███████▍  | 4325/5778 [2:52:38<12:48,  1.89it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.95048}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97798}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 75%|███████▌  | 4345/5778 [2:52:51<12:36,  1.89it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98053}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 75%|███████▌  | 4355/5778 [2:53:06<16:51,  1.41it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_H', 'probabilities': 0.86422}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.91893}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99231}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_H', 'probabilities': 0.98541}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99915}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_H', 'probabilities': 0.79579}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99839}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98853}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.66106}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99812}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_H', 'probabilities': 0.99152}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 76%|███████▌  | 4381/5778 [2:53:55<34:37,  1.49s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.9807}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.67929}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 76%|███████▌  | 4395/5778 [2:54:10<26:15,  1.14s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99776}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 76%|███████▌  | 4399/5778 [2:54:12<13:56,  1.65it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.49451}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.37408}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99464}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.60766}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.64879}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 76%|███████▋  | 4407/5778 [2:54:17<17:13,  1.33it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.25803}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99443}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99126}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.47803}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.89644}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.97026}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99738}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.93747}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 76%|███████▋  | 4411/5778 [2:54:19<14:14,  1.60it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.95242}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 76%|███████▋  | 4413/5778 [2:54:20<11:22,  2.00it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.87262}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.9407}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.10258}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.98568}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 77%|███████▋  | 4434/5778 [2:54:35<07:49,  2.86it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.87462}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.9726}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99643}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 77%|███████▋  | 4439/5778 [2:54:38<10:29,  2.13it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98304}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97198}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98483}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 77%|███████▋  | 4448/5778 [2:54:47<43:54,  1.98s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99953}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 77%|███████▋  | 4460/5778 [2:54:54<07:39,  2.87it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.33302}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 77%|███████▋  | 4463/5778 [2:54:56<14:49,  1.48it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.8765}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98584}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99799}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99882}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99893}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.9985}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.95849}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 77%|███████▋  | 4467/5778 [2:55:00<14:41,  1.49it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99958}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.78427}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.61331}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.96214}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99405}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99012}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99262}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.72132}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.87191}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 77%|███████▋  | 4474/5778 [2:55:13<24:40,  1.14s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.81862}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 78%|███████▊  | 4488/5778 [2:55:37<36:41,  1.71s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.65655}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 78%|███████▊  | 4496/5778 [2:55:44<18:55,  1.13it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9579}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 78%|███████▊  | 4503/5778 [2:55:56<25:28,  1.20s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.75015}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 78%|███████▊  | 4525/5778 [2:56:10<09:19,  2.24it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.90922}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 79%|███████▊  | 4540/5778 [2:56:20<14:03,  1.47it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.9365}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 79%|███████▊  | 4548/5778 [2:56:26<14:05,  1.45it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'adt_S', 'probabilities': 0.98923}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 79%|███████▉  | 4570/5778 [2:56:39<15:25,  1.31it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.9974}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 79%|███████▉  | 4571/5778 [2:56:39<13:30,  1.49it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.80441}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99928}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 79%|███████▉  | 4575/5778 [2:56:45<16:52,  1.19it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99636}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.83626}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.46088}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.90699}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.54642}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99392}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.52649}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.65646}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 79%|███████▉  | 4578/5778 [2:56:46<10:08,  1.97it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.49113}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 79%|███████▉  | 4580/5778 [2:56:46<06:17,  3.18it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9433}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 80%|███████▉  | 4595/5778 [2:56:58<12:42,  1.55it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.9931}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99786}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.98676}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.60773}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_S', 'probabilities': 0.21829}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.40891}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.9858}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.24292}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.96793}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 80%|███████▉  | 4609/5778 [2:57:10<15:10,  1.28it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99777}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 80%|███████▉  | 4612/5778 [2:57:13<14:48,  1.31it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99887}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 80%|████████  | 4633/5778 [2:57:23<06:50,  2.79it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.57523}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.94353}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.51743}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99072}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.55515}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.36472}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99735}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99206}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.44048}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.84076}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 81%|████████▏ | 4707/5778 [2:58:41<08:24,  2.12it/s]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg tr_A', 'probabilities': 0.99912}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 82%|████████▏ | 4713/5778 [2:58:47<12:12,  1.45it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98612}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97083}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9873}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99024}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9935}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.25272}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.8992}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.86958}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98324}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99401}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99563}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98741}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98877}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 82%|████████▏ | 4732/5778 [2:59:05<13:07,  1.33it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99898}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 82%|████████▏ | 4746/5778 [2:59:13<09:04,  1.90it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.49097}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.33913}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.98137}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 82%|████████▏ | 4747/5778 [2:59:14<09:26,  1.82it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.40388}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.61658}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 82%|████████▏ | 4755/5778 [2:59:17<07:28,  2.28it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.88985}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.86072}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.90404}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 82%|████████▏ | 4763/5778 [2:59:21<08:24,  2.01it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.87278}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99734}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 83%|████████▎ | 4790/5778 [2:59:37<09:19,  1.77it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.33031}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.50175}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 83%|████████▎ | 4809/5778 [2:59:45<06:27,  2.50it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99682}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99322}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 84%|████████▎ | 4827/5778 [2:59:57<10:10,  1.56it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97178}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 84%|████████▎ | 4830/5778 [2:59:58<05:40,  2.78it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.84732}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'Z', 'probabilities': 0.51519}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 84%|████████▍ | 4842/5778 [3:00:06<11:14,  1.39it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.92156}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99784}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 84%|████████▍ | 4849/5778 [3:00:10<07:33,  2.05it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99391}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.95117}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 84%|████████▍ | 4865/5778 [3:00:39<19:43,  1.30s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.22778}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99523}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 84%|████████▍ | 4867/5778 [3:00:40<13:30,  1.12it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_S', 'probabilities': 0.99508}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 84%|████████▍ | 4869/5778 [3:00:45<21:31,  1.42s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99892}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 84%|████████▍ | 4878/5778 [3:00:52<06:32,  2.29it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99331}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99389}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99712}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 84%|████████▍ | 4882/5778 [3:00:57<19:49,  1.33s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.7375}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_H', 'probabilities': 0.64821}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_H', 'probabilities': 0.45104}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 85%|████████▍ | 4884/5778 [3:00:59<14:37,  1.02it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg abl_H', 'probabilities': 0.96262}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99942}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 85%|████████▍ | 4885/5778 [3:00:59<12:40,  1.17it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.92089}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97069}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 85%|████████▍ | 4889/5778 [3:01:00<05:21,  2.76it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99905}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99945}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99963}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_S', 'probabilities': 0.99951}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg p_S', 'probabilities': 0.99955}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_S', 'probabilities': 0.99875}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99938}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99892}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl p_S', 'probabilities': 0.99829}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 85%|████████▍ | 4902/5778 [3:01:18<13:37,  1.07it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.30141}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.98638}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 85%|████████▌ | 4918/5778 [3:01:33<15:05,  1.05s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.58331}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.93948}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99284}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 85%|████████▌ | 4924/5778 [3:01:35<06:19,  2.25it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.99397}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 86%|████████▌ | 4948/5778 [3:01:49<09:16,  1.49it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.93986}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99943}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 86%|████████▌ | 4955/5778 [3:01:52<06:26,  2.13it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.25692}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 86%|████████▌ | 4957/5778 [3:01:52<04:14,  3.22it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_S', 'probabilities': 0.99928}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_S', 'probabilities': 0.99948}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 86%|████████▌ | 4966/5778 [3:02:09<09:52,  1.37it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97255}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.95643}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.95646}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 86%|████████▌ | 4983/5778 [3:02:20<05:23,  2.45it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99117}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99544}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99779}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99757}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg kom_S', 'probabilities': 0.99963}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 87%|████████▋ | 5007/5778 [3:02:38<05:00,  2.57it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.9992}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99203}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'des_V', 'probabilities': 0.97873}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 87%|████████▋ | 5017/5778 [3:02:45<05:07,  2.48it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9952}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.95643}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 87%|████████▋ | 5019/5778 [3:02:45<03:57,  3.20it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.6019}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 87%|████████▋ | 5052/5778 [3:03:47<21:00,  1.74s/it]  c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl g_H', 'probabilities': 0.44737}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.40505}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.54049}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 88%|████████▊ | 5068/5778 [3:03:54<05:01,  2.36it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.99447}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 88%|████████▊ | 5089/5778 [3:04:07<04:10,  2.75it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.86516}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 88%|████████▊ | 5112/5778 [3:04:15<03:28,  3.20it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99939}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 88%|████████▊ | 5113/5778 [3:04:16<04:08,  2.67it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99819}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.9976}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg ill_H', 'probabilities': 0.96578}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_H', 'probabilities': 0.90666}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 89%|████████▊ | 5120/5778 [3:04:24<07:35,  1.44it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.90296}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 89%|████████▉ | 5128/5778 [3:04:28<03:51,  2.81it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98493}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.31485}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.92786}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98392}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97278}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.75784}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 89%|████████▉ | 5129/5778 [3:04:29<03:55,  2.76it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.84978}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.26753}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 89%|████████▉ | 5162/5778 [3:04:58<11:13,  1.09s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99763}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.98786}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 89%|████████▉ | 5164/5778 [3:05:00<09:48,  1.04it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.93688}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 90%|████████▉ | 5174/5778 [3:05:03<03:17,  3.06it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99795}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99947}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.84865}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.80836}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 90%|████████▉ | 5193/5778 [3:05:20<11:44,  1.20s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99937}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 90%|████████▉ | 5198/5778 [3:05:25<10:29,  1.09s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99639}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_A', 'probabilities': 0.99964}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.95626}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 90%|████████▉ | 5200/5778 [3:05:31<18:30,  1.92s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98658}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98109}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 90%|█████████ | 5206/5778 [3:05:42<28:20,  2.97s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99584}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 90%|█████████ | 5210/5778 [3:05:52<17:19,  1.83s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.77746}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.94716}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.98727}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9967}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.25603}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.9788}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.66948}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 90%|█████████ | 5212/5778 [3:05:54<10:54,  1.16s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg in_H', 'probabilities': 0.98507}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 90%|█████████ | 5224/5778 [3:06:02<08:27,  1.09it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg tr_A', 'probabilities': 0.99914}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 91%|█████████ | 5251/5778 [3:06:27<03:40,  2.39it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.95636}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 91%|█████████ | 5252/5778 [3:06:34<21:12,  2.42s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.59765}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 91%|█████████ | 5261/5778 [3:06:37<03:20,  2.58it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.91285}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.8603}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.9892}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 91%|█████████ | 5270/5778 [3:06:46<07:09,  1.18it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.75618}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 91%|█████████▏| 5278/5778 [3:06:52<05:34,  1.50it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99157}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.8558}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.58032}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 92%|█████████▏| 5295/5778 [3:06:59<02:41,  2.98it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99348}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99485}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 92%|█████████▏| 5301/5778 [3:07:02<02:56,  2.70it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.98635}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 92%|█████████▏| 5305/5778 [3:07:03<01:43,  4.59it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.40367}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 92%|█████████▏| 5306/5778 [3:07:03<01:51,  4.24it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.44204}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.77884}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 92%|█████████▏| 5307/5778 [3:07:03<02:04,  3.77it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_H', 'probabilities': 0.98055}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 92%|█████████▏| 5310/5778 [3:07:05<02:56,  2.64it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl g_S', 'probabilities': 0.99911}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_S', 'probabilities': 0.99958}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl g_S', 'probabilities': 0.99946}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl g_S', 'probabilities': 0.99925}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg kom_H', 'probabilities': 0.98509}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 92%|█████████▏| 5315/5778 [3:07:09<04:53,  1.57it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.97552}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 92%|█████████▏| 5333/5778 [3:07:30<04:41,  1.58it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.99904}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl g_S', 'probabilities': 0.99915}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 92%|█████████▏| 5343/5778 [3:07:38<03:02,  2.39it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.95177}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 93%|█████████▎| 5374/5778 [3:07:54<03:13,  2.09it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99202}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.9829}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 93%|█████████▎| 5375/5778 [3:07:58<11:41,  1.74s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.95957}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 93%|█████████▎| 5379/5778 [3:08:01<05:19,  1.25it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.92907}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99725}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 93%|█████████▎| 5388/5778 [3:08:11<05:51,  1.11it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg ad_H', 'probabilities': 0.96105}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg ad_H', 'probabilities': 0.9632}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 93%|█████████▎| 5394/5778 [3:08:18<05:35,  1.14it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99669}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 94%|█████████▎| 5408/5778 [3:08:25<04:04,  1.52it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.95661}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97988}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 94%|█████████▎| 5409/5778 [3:08:26<03:04,  2.00it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.9477}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 94%|█████████▍| 5431/5778 [3:08:36<02:34,  2.25it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99861}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 94%|█████████▍| 5432/5778 [3:08:37<02:20,  2.47it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.37208}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.81809}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 94%|█████████▍| 5458/5778 [3:09:14<02:08,  2.50it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.3658}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.7516}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99933}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.79719}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99507}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 95%|█████████▍| 5474/5778 [3:09:20<01:35,  3.19it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.97754}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.96248}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.51574}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 95%|█████████▍| 5476/5778 [3:09:20<01:18,  3.86it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99055}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 95%|█████████▍| 5486/5778 [3:09:25<02:45,  1.77it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_H', 'probabilities': 0.34215}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.7898}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 95%|█████████▌| 5500/5778 [3:09:34<02:14,  2.06it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.59988}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 95%|█████████▌| 5507/5778 [3:09:36<01:21,  3.34it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.46834}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99935}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 95%|█████████▌| 5512/5778 [3:09:37<01:00,  4.43it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.81808}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 95%|█████████▌| 5514/5778 [3:09:38<01:42,  2.57it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'adt_S', 'probabilities': 0.99539}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 96%|█████████▌| 5520/5778 [3:09:43<03:48,  1.13it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.99684}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 96%|█████████▌| 5538/5778 [3:10:00<01:43,  2.32it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.8557}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99165}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99285}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99335}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 96%|█████████▌| 5543/5778 [3:10:04<03:11,  1.23it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.98259}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.90942}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 96%|█████████▌| 5548/5778 [3:10:08<02:46,  1.38it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.98757}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.98933}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.77832}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99912}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 96%|█████████▋| 5571/5778 [3:10:23<01:16,  2.69it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl n_S', 'probabilities': 0.99952}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 97%|█████████▋| 5578/5778 [3:10:27<01:39,  2.01it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99548}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99585}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 97%|█████████▋| 5585/5778 [3:10:31<01:31,  2.11it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'pl all_S', 'probabilities': 0.28028}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 97%|█████████▋| 5604/5778 [3:10:47<03:13,  1.11s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.94161}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 97%|█████████▋| 5619/5778 [3:11:02<01:43,  1.54it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.8595}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.70479}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 97%|█████████▋| 5629/5778 [3:11:08<01:21,  1.83it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.92085}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.72892}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 98%|█████████▊| 5651/5778 [3:11:54<09:36,  4.54s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.8518}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.96757}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 98%|█████████▊| 5688/5778 [3:12:47<05:25,  3.62s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': '?_Y', 'probabilities': 0.99237}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 99%|█████████▊| 5694/5778 [3:12:49<01:12,  1.16it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.36782}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.99374}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 99%|█████████▉| 5710/5778 [3:12:59<00:51,  1.32it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_S', 'probabilities': 0.99966}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg el_S', 'probabilities': 0.99891}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 99%|█████████▉| 5742/5778 [3:13:24<00:25,  1.42it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.95067}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.98654}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_H', 'probabilities': 0.70111}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      " 99%|█████████▉| 5744/5778 [3:13:25<00:23,  1.47it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'ma_V', 'probabilities': 0.59673}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "100%|█████████▉| 5751/5778 [3:13:45<01:46,  3.93s/it]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg g_H', 'probabilities': 0.91467}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.54402}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "100%|█████████▉| 5767/5778 [3:13:54<00:06,  1.61it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'G', 'probabilities': 0.99391}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "100%|█████████▉| 5773/5778 [3:13:58<00:03,  1.47it/s]c:\\Users\\Admin\\OneDrive\\TU\\Praktika\\EstNLTK\\morf_yhestaja\\bert_tokens_to_words_rewriter.py:137: UserWarning: (!) No matching words span for bert token Span(' ', [{'bert_tokens': '▁', 'morph_labels': 'sg n_S', 'probabilities': 0.95692}]).\n",
      "  warnings.warn(f\"(!) No matching {words_layer.name} span for bert token {bert_tokens_layer[i]}.\")\n",
      "100%|██████████| 5778/5778 [3:14:02<00:00,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_json_file_by_file_enc2017(jsons, in_dir, '_diff_morph_texts_json', True, bert_morph_tagger=morph_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing BertMorphTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"A. H. Tammsaare oli eesti kirjanik, esseist, kultuurifilosoof ja tõlkija. Üksnes autorihüvitis oli 12 431 krooni.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_obj = estnltk.Text(input_str)\n",
    "# text_obj.tag_layer(\"morph_analysis\");\n",
    "# text_obj.tag_layer(\"sentences\");\n",
    "text_obj = estnltk.converters.json_to_text(file=os.path.join('./_bert_morph_texts_json/', jsons[27]))\n",
    "# text_obj = estnltk.converters.json_to_text(file=os.path.join('./_bert_morph_texts_json/', jsons[931]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_abc_impl',\n",
       " '_span',\n",
       " 'clitic',\n",
       " 'end',\n",
       " 'ending',\n",
       " 'form',\n",
       " 'get',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'label',\n",
       " 'layer',\n",
       " 'legal_attribute_names',\n",
       " 'lemma',\n",
       " 'normalized_text',\n",
       " 'partofspeech',\n",
       " 'root',\n",
       " 'root_tokens',\n",
       " 'span',\n",
       " 'start',\n",
       " 'text',\n",
       " 'text_object',\n",
       " 'values']"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(text_obj.morph_analysis.spans[0].annotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_obj.morph_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tagger to the text object\n",
    "text_obj.add_layer(morph_tagger.make_layer(text_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "\n",
       "\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>bert_morph_tagging</td>\n",
       "      <td>bert_tokens, morph_labels, probabilities</td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_2b4ed\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_2b4ed_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "      <th id=\"T_2b4ed_level0_col1\" class=\"col_heading level0 col1\" >bert_tokens</th>\n",
       "      <th id=\"T_2b4ed_level0_col2\" class=\"col_heading level0 col2\" >morph_labels</th>\n",
       "      <th id=\"T_2b4ed_level0_col3\" class=\"col_heading level0 col3\" >probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row0_col0\" class=\"data row0 col0\" >['Tuvastamata']</td>\n",
       "      <td id=\"T_2b4ed_row0_col1\" class=\"data row0 col1\" >['▁Tu', 'vasta', 'mata']</td>\n",
       "      <td id=\"T_2b4ed_row0_col2\" class=\"data row0 col2\" >['A']</td>\n",
       "      <td id=\"T_2b4ed_row0_col3\" class=\"data row0 col3\" >[0.95848]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row1_col0\" class=\"data row1 col0\" >['Kasutaja']</td>\n",
       "      <td id=\"T_2b4ed_row1_col1\" class=\"data row1 col1\" >['▁Kasutaja']</td>\n",
       "      <td id=\"T_2b4ed_row1_col2\" class=\"data row1 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row1_col3\" class=\"data row1 col3\" >[0.99825]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row2_col0\" class=\"data row2 col0\" >['27.04.2009']</td>\n",
       "      <td id=\"T_2b4ed_row2_col1\" class=\"data row2 col1\" >['▁27', '.04.2009']</td>\n",
       "      <td id=\"T_2b4ed_row2_col2\" class=\"data row2 col2\" >['?_N']</td>\n",
       "      <td id=\"T_2b4ed_row2_col3\" class=\"data row2 col3\" >[0.99323]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row3_col0\" class=\"data row3 col0\" >['14:42']</td>\n",
       "      <td id=\"T_2b4ed_row3_col1\" class=\"data row3 col1\" >['▁14', ':', '42']</td>\n",
       "      <td id=\"T_2b4ed_row3_col2\" class=\"data row3 col2\" >['?_N']</td>\n",
       "      <td id=\"T_2b4ed_row3_col3\" class=\"data row3 col3\" >[0.9991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row4_col0\" class=\"data row4 col0\" >['Venemaa']</td>\n",
       "      <td id=\"T_2b4ed_row4_col1\" class=\"data row4 col1\" >['▁Venemaa']</td>\n",
       "      <td id=\"T_2b4ed_row4_col2\" class=\"data row4 col2\" >['sg n_H']</td>\n",
       "      <td id=\"T_2b4ed_row4_col3\" class=\"data row4 col3\" >[0.99944]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row5_col0\" class=\"data row5 col0\" >['käitub']</td>\n",
       "      <td id=\"T_2b4ed_row5_col1\" class=\"data row5 col1\" >['▁käitub']</td>\n",
       "      <td id=\"T_2b4ed_row5_col2\" class=\"data row5 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row5_col3\" class=\"data row5 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row6_col0\" class=\"data row6 col0\" >['nii']</td>\n",
       "      <td id=\"T_2b4ed_row6_col1\" class=\"data row6 col1\" >['▁nii']</td>\n",
       "      <td id=\"T_2b4ed_row6_col2\" class=\"data row6 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row6_col3\" class=\"data row6 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row7_col0\" class=\"data row7 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row7_col1\" class=\"data row7 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row7_col2\" class=\"data row7 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row7_col3\" class=\"data row7 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row8_col0\" class=\"data row8 col0\" >['nagu']</td>\n",
       "      <td id=\"T_2b4ed_row8_col1\" class=\"data row8 col1\" >['▁nagu']</td>\n",
       "      <td id=\"T_2b4ed_row8_col2\" class=\"data row8 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row8_col3\" class=\"data row8 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row9_col0\" class=\"data row9 col0\" >['oleks']</td>\n",
       "      <td id=\"T_2b4ed_row9_col1\" class=\"data row9 col1\" >['▁oleks']</td>\n",
       "      <td id=\"T_2b4ed_row9_col2\" class=\"data row9 col2\" >['ks_V']</td>\n",
       "      <td id=\"T_2b4ed_row9_col3\" class=\"data row9 col3\" >[0.9997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row10_col0\" class=\"data row10 col0\" >['võit']</td>\n",
       "      <td id=\"T_2b4ed_row10_col1\" class=\"data row10 col1\" >['▁võit']</td>\n",
       "      <td id=\"T_2b4ed_row10_col2\" class=\"data row10 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row10_col3\" class=\"data row10 col3\" >[0.99979]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row11_col0\" class=\"data row11 col0\" >['natsismi']</td>\n",
       "      <td id=\"T_2b4ed_row11_col1\" class=\"data row11 col1\" >['▁natsis', 'mi']</td>\n",
       "      <td id=\"T_2b4ed_row11_col2\" class=\"data row11 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row11_col3\" class=\"data row11 col3\" >[0.99961]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row12_col0\" class=\"data row12 col0\" >['üle']</td>\n",
       "      <td id=\"T_2b4ed_row12_col1\" class=\"data row12 col1\" >['▁üle']</td>\n",
       "      <td id=\"T_2b4ed_row12_col2\" class=\"data row12 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row12_col3\" class=\"data row12 col3\" >[0.99936]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row13_col0\" class=\"data row13 col0\" >['vaid']</td>\n",
       "      <td id=\"T_2b4ed_row13_col1\" class=\"data row13 col1\" >['▁vaid']</td>\n",
       "      <td id=\"T_2b4ed_row13_col2\" class=\"data row13 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row13_col3\" class=\"data row13 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row14_col0\" class=\"data row14 col0\" >['tema']</td>\n",
       "      <td id=\"T_2b4ed_row14_col1\" class=\"data row14 col1\" >['▁tema']</td>\n",
       "      <td id=\"T_2b4ed_row14_col2\" class=\"data row14 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row14_col3\" class=\"data row14 col3\" >[0.99969]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row15_col0\" class=\"data row15 col0\" >['teene']</td>\n",
       "      <td id=\"T_2b4ed_row15_col1\" class=\"data row15 col1\" >['▁teene']</td>\n",
       "      <td id=\"T_2b4ed_row15_col2\" class=\"data row15 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row15_col3\" class=\"data row15 col3\" >[0.99976]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row16_col0\" class=\"data row16 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row16_col1\" class=\"data row16 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row16_col2\" class=\"data row16 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row16_col3\" class=\"data row16 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row17_col0\" class=\"data row17 col0\" >['Ometi']</td>\n",
       "      <td id=\"T_2b4ed_row17_col1\" class=\"data row17 col1\" >['▁Ometi']</td>\n",
       "      <td id=\"T_2b4ed_row17_col2\" class=\"data row17 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row17_col3\" class=\"data row17 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row18_col0\" class=\"data row18 col0\" >['olid']</td>\n",
       "      <td id=\"T_2b4ed_row18_col1\" class=\"data row18 col1\" >['▁olid']</td>\n",
       "      <td id=\"T_2b4ed_row18_col2\" class=\"data row18 col2\" >['sid_V']</td>\n",
       "      <td id=\"T_2b4ed_row18_col3\" class=\"data row18 col3\" >[0.99983]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row19_col0\" class=\"data row19 col0\" >['tal']</td>\n",
       "      <td id=\"T_2b4ed_row19_col1\" class=\"data row19 col1\" >['▁tal']</td>\n",
       "      <td id=\"T_2b4ed_row19_col2\" class=\"data row19 col2\" >['sg ad_P']</td>\n",
       "      <td id=\"T_2b4ed_row19_col3\" class=\"data row19 col3\" >[0.99972]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row20_col0\" class=\"data row20 col0\" >['ju']</td>\n",
       "      <td id=\"T_2b4ed_row20_col1\" class=\"data row20 col1\" >['▁ju']</td>\n",
       "      <td id=\"T_2b4ed_row20_col2\" class=\"data row20 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row20_col3\" class=\"data row20 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row21_col0\" class=\"data row21 col0\" >['liitlased']</td>\n",
       "      <td id=\"T_2b4ed_row21_col1\" class=\"data row21 col1\" >['▁liitlased']</td>\n",
       "      <td id=\"T_2b4ed_row21_col2\" class=\"data row21 col2\" >['pl n_S']</td>\n",
       "      <td id=\"T_2b4ed_row21_col3\" class=\"data row21 col3\" >[0.99958]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row22_col0\" class=\"data row22 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row22_col1\" class=\"data row22 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row22_col2\" class=\"data row22 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row22_col3\" class=\"data row22 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row23_col0\" class=\"data row23 col0\" >['see']</td>\n",
       "      <td id=\"T_2b4ed_row23_col1\" class=\"data row23 col1\" >['▁see']</td>\n",
       "      <td id=\"T_2b4ed_row23_col2\" class=\"data row23 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row23_col3\" class=\"data row23 col3\" >[0.99986]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row24_col0\" class=\"data row24 col0\" >['võit']</td>\n",
       "      <td id=\"T_2b4ed_row24_col1\" class=\"data row24 col1\" >['▁võit']</td>\n",
       "      <td id=\"T_2b4ed_row24_col2\" class=\"data row24 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row24_col3\" class=\"data row24 col3\" >[0.99978]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row25_col0\" class=\"data row25 col0\" >['oli']</td>\n",
       "      <td id=\"T_2b4ed_row25_col1\" class=\"data row25 col1\" >['▁oli']</td>\n",
       "      <td id=\"T_2b4ed_row25_col2\" class=\"data row25 col2\" >['s_V']</td>\n",
       "      <td id=\"T_2b4ed_row25_col3\" class=\"data row25 col3\" >[0.99992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row26_col0\" class=\"data row26 col0\" >['ikkagi']</td>\n",
       "      <td id=\"T_2b4ed_row26_col1\" class=\"data row26 col1\" >['▁ikkagi']</td>\n",
       "      <td id=\"T_2b4ed_row26_col2\" class=\"data row26 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row26_col3\" class=\"data row26 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row27_col0\" class=\"data row27 col0\" >['rahvusvaheline']</td>\n",
       "      <td id=\"T_2b4ed_row27_col1\" class=\"data row27 col1\" >['▁rahvusvaheline']</td>\n",
       "      <td id=\"T_2b4ed_row27_col2\" class=\"data row27 col2\" >['sg n_A']</td>\n",
       "      <td id=\"T_2b4ed_row27_col3\" class=\"data row27 col3\" >[0.99976]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row28_col0\" class=\"data row28 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row28_col1\" class=\"data row28 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row28_col2\" class=\"data row28 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row28_col3\" class=\"data row28 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row29_col0\" class=\"data row29 col0\" >['Nüüd']</td>\n",
       "      <td id=\"T_2b4ed_row29_col1\" class=\"data row29 col1\" >['▁Nüüd']</td>\n",
       "      <td id=\"T_2b4ed_row29_col2\" class=\"data row29 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row29_col3\" class=\"data row29 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row30_col0\" class=\"data row30 col0\" >['aga']</td>\n",
       "      <td id=\"T_2b4ed_row30_col1\" class=\"data row30 col1\" >['▁aga']</td>\n",
       "      <td id=\"T_2b4ed_row30_col2\" class=\"data row30 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row30_col3\" class=\"data row30 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row31_col0\" class=\"data row31 col0\" >['surutakse']</td>\n",
       "      <td id=\"T_2b4ed_row31_col1\" class=\"data row31 col1\" >['▁surutakse']</td>\n",
       "      <td id=\"T_2b4ed_row31_col2\" class=\"data row31 col2\" >['takse_V']</td>\n",
       "      <td id=\"T_2b4ed_row31_col3\" class=\"data row31 col3\" >[0.9992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row32_col0\" class=\"data row32 col0\" >['lausa']</td>\n",
       "      <td id=\"T_2b4ed_row32_col1\" class=\"data row32 col1\" >['▁lausa']</td>\n",
       "      <td id=\"T_2b4ed_row32_col2\" class=\"data row32 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row32_col3\" class=\"data row32 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row33_col0\" class=\"data row33 col0\" >['kriminaalkoodeksi']</td>\n",
       "      <td id=\"T_2b4ed_row33_col1\" class=\"data row33 col1\" >['▁kriminaal', 'koodeksi']</td>\n",
       "      <td id=\"T_2b4ed_row33_col2\" class=\"data row33 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row33_col3\" class=\"data row33 col3\" >[0.99974]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row34_col0\" class=\"data row34 col0\" >['abil']</td>\n",
       "      <td id=\"T_2b4ed_row34_col1\" class=\"data row34 col1\" >['▁abil']</td>\n",
       "      <td id=\"T_2b4ed_row34_col2\" class=\"data row34 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row34_col3\" class=\"data row34 col3\" >[0.99967]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row35_col0\" class=\"data row35 col0\" >['peale']</td>\n",
       "      <td id=\"T_2b4ed_row35_col1\" class=\"data row35 col1\" >['▁peale']</td>\n",
       "      <td id=\"T_2b4ed_row35_col2\" class=\"data row35 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row35_col3\" class=\"data row35 col3\" >[0.99975]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row36_col0\" class=\"data row36 col0\" >['vene']</td>\n",
       "      <td id=\"T_2b4ed_row36_col1\" class=\"data row36 col1\" >['▁vene']</td>\n",
       "      <td id=\"T_2b4ed_row36_col2\" class=\"data row36 col2\" >['G']</td>\n",
       "      <td id=\"T_2b4ed_row36_col3\" class=\"data row36 col3\" >[0.9902]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row37_col0\" class=\"data row37 col0\" >['nägemust']</td>\n",
       "      <td id=\"T_2b4ed_row37_col1\" class=\"data row37 col1\" >['▁nägem', 'ust']</td>\n",
       "      <td id=\"T_2b4ed_row37_col2\" class=\"data row37 col2\" >['sg p_S']</td>\n",
       "      <td id=\"T_2b4ed_row37_col3\" class=\"data row37 col3\" >[0.99957]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row38_col0\" class=\"data row38 col0\" >['asjast']</td>\n",
       "      <td id=\"T_2b4ed_row38_col1\" class=\"data row38 col1\" >['▁asjast']</td>\n",
       "      <td id=\"T_2b4ed_row38_col2\" class=\"data row38 col2\" >['sg el_S']</td>\n",
       "      <td id=\"T_2b4ed_row38_col3\" class=\"data row38 col3\" >[0.99886]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row39_col0\" class=\"data row39 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row39_col1\" class=\"data row39 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row39_col2\" class=\"data row39 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row39_col3\" class=\"data row39 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row40_col0\" class=\"data row40 col0\" >['on']</td>\n",
       "      <td id=\"T_2b4ed_row40_col1\" class=\"data row40 col1\" >['▁on']</td>\n",
       "      <td id=\"T_2b4ed_row40_col2\" class=\"data row40 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row40_col3\" class=\"data row40 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row41_col0\" class=\"data row41 col0\" >['loomisel']</td>\n",
       "      <td id=\"T_2b4ed_row41_col1\" class=\"data row41 col1\" >['▁loomisel']</td>\n",
       "      <td id=\"T_2b4ed_row41_col2\" class=\"data row41 col2\" >['sg ad_S']</td>\n",
       "      <td id=\"T_2b4ed_row41_col3\" class=\"data row41 col3\" >[0.99939]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row42_col0\" class=\"data row42 col0\" >['tribunal']</td>\n",
       "      <td id=\"T_2b4ed_row42_col1\" class=\"data row42 col1\" >['▁trib', 'unal']</td>\n",
       "      <td id=\"T_2b4ed_row42_col2\" class=\"data row42 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row42_col3\" class=\"data row42 col3\" >[0.99958]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row43_col0\" class=\"data row43 col0\" >['(']</td>\n",
       "      <td id=\"T_2b4ed_row43_col1\" class=\"data row43 col1\" >['▁(']</td>\n",
       "      <td id=\"T_2b4ed_row43_col2\" class=\"data row43 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row43_col3\" class=\"data row43 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row44_col0\" class=\"data row44 col0\" >['Общественная']</td>\n",
       "      <td id=\"T_2b4ed_row44_col1\" class=\"data row44 col1\" >['О', 'б', 'щ', 'е', 'ст', 'в', 'ен', 'н', 'а', 'я']</td>\n",
       "      <td id=\"T_2b4ed_row44_col2\" class=\"data row44 col2\" >['sg g_H']</td>\n",
       "      <td id=\"T_2b4ed_row44_col3\" class=\"data row44 col3\" >[0.88898]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row45_col0\" class=\"data row45 col0\" >['комиссия']</td>\n",
       "      <td id=\"T_2b4ed_row45_col1\" class=\"data row45 col1\" >['к', 'о', 'м', 'и', 'с', 'с', 'и', 'я']</td>\n",
       "      <td id=\"T_2b4ed_row45_col2\" class=\"data row45 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row45_col3\" class=\"data row45 col3\" >[0.9257]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row46_col0\" class=\"data row46 col0\" >[')', ',']</td>\n",
       "      <td id=\"T_2b4ed_row46_col1\" class=\"data row46 col1\" >['),']</td>\n",
       "      <td id=\"T_2b4ed_row46_col2\" class=\"data row46 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row46_col3\" class=\"data row46 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row47_col0\" class=\"data row47 col0\" >['mis']</td>\n",
       "      <td id=\"T_2b4ed_row47_col1\" class=\"data row47 col1\" >['▁mis']</td>\n",
       "      <td id=\"T_2b4ed_row47_col2\" class=\"data row47 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row47_col3\" class=\"data row47 col3\" >[0.99986]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row48_col0\" class=\"data row48 col0\" >['hakkab']</td>\n",
       "      <td id=\"T_2b4ed_row48_col1\" class=\"data row48 col1\" >['▁hakkab']</td>\n",
       "      <td id=\"T_2b4ed_row48_col2\" class=\"data row48 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row48_col3\" class=\"data row48 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row49_col0\" class=\"data row49 col0\" >['inimeste']</td>\n",
       "      <td id=\"T_2b4ed_row49_col1\" class=\"data row49 col1\" >['▁inimeste']</td>\n",
       "      <td id=\"T_2b4ed_row49_col2\" class=\"data row49 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row49_col3\" class=\"data row49 col3\" >[0.99938]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row50_col0\" class=\"data row50 col0\" >['(']</td>\n",
       "      <td id=\"T_2b4ed_row50_col1\" class=\"data row50 col1\" >['▁(']</td>\n",
       "      <td id=\"T_2b4ed_row50_col2\" class=\"data row50 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row50_col3\" class=\"data row50 col3\" >[0.99995]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row51_col0\" class=\"data row51 col0\" >['ka']</td>\n",
       "      <td id=\"T_2b4ed_row51_col1\" class=\"data row51 col1\" >['ka']</td>\n",
       "      <td id=\"T_2b4ed_row51_col2\" class=\"data row51 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row51_col3\" class=\"data row51 col3\" >[0.99776]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row52_col0\" class=\"data row52 col0\" >['välismaalaste']</td>\n",
       "      <td id=\"T_2b4ed_row52_col1\" class=\"data row52 col1\" >['▁välismaalaste']</td>\n",
       "      <td id=\"T_2b4ed_row52_col2\" class=\"data row52 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row52_col3\" class=\"data row52 col3\" >[0.99893]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row53_col0\" class=\"data row53 col0\" >['!', ')']</td>\n",
       "      <td id=\"T_2b4ed_row53_col1\" class=\"data row53 col1\" >['!)']</td>\n",
       "      <td id=\"T_2b4ed_row53_col2\" class=\"data row53 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row53_col3\" class=\"data row53 col3\" >[0.99986]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row54_col0\" class=\"data row54 col0\" >['üle']</td>\n",
       "      <td id=\"T_2b4ed_row54_col1\" class=\"data row54 col1\" >['▁üle']</td>\n",
       "      <td id=\"T_2b4ed_row54_col2\" class=\"data row54 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row54_col3\" class=\"data row54 col3\" >[0.99715]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row55_col0\" class=\"data row55 col0\" >['otsustama']</td>\n",
       "      <td id=\"T_2b4ed_row55_col1\" class=\"data row55 col1\" >['▁otsustama']</td>\n",
       "      <td id=\"T_2b4ed_row55_col2\" class=\"data row55 col2\" >['ma_V']</td>\n",
       "      <td id=\"T_2b4ed_row55_col3\" class=\"data row55 col3\" >[0.99936]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row56_col0\" class=\"data row56 col0\" >['just']</td>\n",
       "      <td id=\"T_2b4ed_row56_col1\" class=\"data row56 col1\" >['▁just']</td>\n",
       "      <td id=\"T_2b4ed_row56_col2\" class=\"data row56 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row56_col3\" class=\"data row56 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row57_col0\" class=\"data row57 col0\" >['selle']</td>\n",
       "      <td id=\"T_2b4ed_row57_col1\" class=\"data row57 col1\" >['▁selle']</td>\n",
       "      <td id=\"T_2b4ed_row57_col2\" class=\"data row57 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row57_col3\" class=\"data row57 col3\" >[0.9997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row58_col0\" class=\"data row58 col0\" >['nägemuse']</td>\n",
       "      <td id=\"T_2b4ed_row58_col1\" class=\"data row58 col1\" >['▁nägem', 'use']</td>\n",
       "      <td id=\"T_2b4ed_row58_col2\" class=\"data row58 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row58_col3\" class=\"data row58 col3\" >[0.99973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row59_col0\" class=\"data row59 col0\" >['alusel']</td>\n",
       "      <td id=\"T_2b4ed_row59_col1\" class=\"data row59 col1\" >['▁alusel']</td>\n",
       "      <td id=\"T_2b4ed_row59_col2\" class=\"data row59 col2\" >['sg ad_S']</td>\n",
       "      <td id=\"T_2b4ed_row59_col3\" class=\"data row59 col3\" >[0.99928]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row60_col0\" class=\"data row60 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row60_col1\" class=\"data row60 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row60_col2\" class=\"data row60 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row60_col3\" class=\"data row60 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row61_col0\" class=\"data row61 col0\" >['kaitsema']</td>\n",
       "      <td id=\"T_2b4ed_row61_col1\" class=\"data row61 col1\" >['▁kaitse', 'ma']</td>\n",
       "      <td id=\"T_2b4ed_row61_col2\" class=\"data row61 col2\" >['ma_V']</td>\n",
       "      <td id=\"T_2b4ed_row61_col3\" class=\"data row61 col3\" >[0.99823]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row62_col0\" class=\"data row62 col0\" >['venemaa']</td>\n",
       "      <td id=\"T_2b4ed_row62_col1\" class=\"data row62 col1\" >['▁venemaa']</td>\n",
       "      <td id=\"T_2b4ed_row62_col2\" class=\"data row62 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row62_col3\" class=\"data row62 col3\" >[0.97306]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row63_col0\" class=\"data row63 col0\" >['seisukohti']</td>\n",
       "      <td id=\"T_2b4ed_row63_col1\" class=\"data row63 col1\" >['▁seisukohti']</td>\n",
       "      <td id=\"T_2b4ed_row63_col2\" class=\"data row63 col2\" >['pl p_S']</td>\n",
       "      <td id=\"T_2b4ed_row63_col3\" class=\"data row63 col3\" >[0.99967]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row64_col0\" class=\"data row64 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row64_col1\" class=\"data row64 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row64_col2\" class=\"data row64 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row64_col3\" class=\"data row64 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row65_col0\" class=\"data row65 col0\" >['või']</td>\n",
       "      <td id=\"T_2b4ed_row65_col1\" class=\"data row65 col1\" >['▁või']</td>\n",
       "      <td id=\"T_2b4ed_row65_col2\" class=\"data row65 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row65_col3\" class=\"data row65 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row66_col0\" class=\"data row66 col0\" >['mis']</td>\n",
       "      <td id=\"T_2b4ed_row66_col1\" class=\"data row66 col1\" >['▁mis']</td>\n",
       "      <td id=\"T_2b4ed_row66_col2\" class=\"data row66 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row66_col3\" class=\"data row66 col3\" >[0.99985]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row67_col0\" class=\"data row67 col0\" >['veel']</td>\n",
       "      <td id=\"T_2b4ed_row67_col1\" class=\"data row67 col1\" >['▁veel']</td>\n",
       "      <td id=\"T_2b4ed_row67_col2\" class=\"data row67 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row67_col3\" class=\"data row67 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row68_col0\" class=\"data row68 col0\" >['hullem']</td>\n",
       "      <td id=\"T_2b4ed_row68_col1\" class=\"data row68 col1\" >['▁hullem']</td>\n",
       "      <td id=\"T_2b4ed_row68_col2\" class=\"data row68 col2\" >['sg n_C']</td>\n",
       "      <td id=\"T_2b4ed_row68_col3\" class=\"data row68 col3\" >[0.99744]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row69_col0\" class=\"data row69 col0\" >['–']</td>\n",
       "      <td id=\"T_2b4ed_row69_col1\" class=\"data row69 col1\" >['▁–']</td>\n",
       "      <td id=\"T_2b4ed_row69_col2\" class=\"data row69 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row69_col3\" class=\"data row69 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row70_col0\" class=\"data row70 col0\" >['NSV']</td>\n",
       "      <td id=\"T_2b4ed_row70_col1\" class=\"data row70 col1\" >['▁NSV']</td>\n",
       "      <td id=\"T_2b4ed_row70_col2\" class=\"data row70 col2\" >['?_Y']</td>\n",
       "      <td id=\"T_2b4ed_row70_col3\" class=\"data row70 col3\" >[0.99383]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row71_col0\" class=\"data row71 col0\" >['Liidu']</td>\n",
       "      <td id=\"T_2b4ed_row71_col1\" class=\"data row71 col1\" >['▁Liidu']</td>\n",
       "      <td id=\"T_2b4ed_row71_col2\" class=\"data row71 col2\" >['sg g_H']</td>\n",
       "      <td id=\"T_2b4ed_row71_col3\" class=\"data row71 col3\" >[0.99649]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row72_col0\" class=\"data row72 col0\" >['seisukohti']</td>\n",
       "      <td id=\"T_2b4ed_row72_col1\" class=\"data row72 col1\" >['▁seisukohti']</td>\n",
       "      <td id=\"T_2b4ed_row72_col2\" class=\"data row72 col2\" >['pl p_S']</td>\n",
       "      <td id=\"T_2b4ed_row72_col3\" class=\"data row72 col3\" >[0.99966]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row73_col0\" class=\"data row73 col0\" >['!']</td>\n",
       "      <td id=\"T_2b4ed_row73_col1\" class=\"data row73 col1\" >['!']</td>\n",
       "      <td id=\"T_2b4ed_row73_col2\" class=\"data row73 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row73_col3\" class=\"data row73 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row74_col0\" class=\"data row74 col0\" >['Suure']</td>\n",
       "      <td id=\"T_2b4ed_row74_col1\" class=\"data row74 col1\" >['▁Suure']</td>\n",
       "      <td id=\"T_2b4ed_row74_col2\" class=\"data row74 col2\" >['sg g_A']</td>\n",
       "      <td id=\"T_2b4ed_row74_col3\" class=\"data row74 col3\" >[0.99954]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row75_col0\" class=\"data row75 col0\" >['sõja']</td>\n",
       "      <td id=\"T_2b4ed_row75_col1\" class=\"data row75 col1\" >['▁sõja']</td>\n",
       "      <td id=\"T_2b4ed_row75_col2\" class=\"data row75 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row75_col3\" class=\"data row75 col3\" >[0.99973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row76_col0\" class=\"data row76 col0\" >['kohta']</td>\n",
       "      <td id=\"T_2b4ed_row76_col1\" class=\"data row76 col1\" >['▁kohta']</td>\n",
       "      <td id=\"T_2b4ed_row76_col2\" class=\"data row76 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row76_col3\" class=\"data row76 col3\" >[0.99963]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row77_col0\" class=\"data row77 col0\" >['on']</td>\n",
       "      <td id=\"T_2b4ed_row77_col1\" class=\"data row77 col1\" >['▁on']</td>\n",
       "      <td id=\"T_2b4ed_row77_col2\" class=\"data row77 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row77_col3\" class=\"data row77 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row78_col0\" class=\"data row78 col0\" >['Venemaal']</td>\n",
       "      <td id=\"T_2b4ed_row78_col1\" class=\"data row78 col1\" >['▁Venemaal']</td>\n",
       "      <td id=\"T_2b4ed_row78_col2\" class=\"data row78 col2\" >['sg ad_H']</td>\n",
       "      <td id=\"T_2b4ed_row78_col3\" class=\"data row78 col3\" >[0.99159]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row79_col0\" class=\"data row79 col0\" >['käibel']</td>\n",
       "      <td id=\"T_2b4ed_row79_col1\" class=\"data row79 col1\" >['▁käib', 'el']</td>\n",
       "      <td id=\"T_2b4ed_row79_col2\" class=\"data row79 col2\" >['sg ad_S']</td>\n",
       "      <td id=\"T_2b4ed_row79_col3\" class=\"data row79 col3\" >[0.84049]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row80_col0\" class=\"data row80 col0\" >['väga']</td>\n",
       "      <td id=\"T_2b4ed_row80_col1\" class=\"data row80 col1\" >['▁väga']</td>\n",
       "      <td id=\"T_2b4ed_row80_col2\" class=\"data row80 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row80_col3\" class=\"data row80 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row81_col0\" class=\"data row81 col0\" >['palju']</td>\n",
       "      <td id=\"T_2b4ed_row81_col1\" class=\"data row81 col1\" >['▁palju']</td>\n",
       "      <td id=\"T_2b4ed_row81_col2\" class=\"data row81 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row81_col3\" class=\"data row81 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row82_col0\" class=\"data row82 col0\" >['valesid']</td>\n",
       "      <td id=\"T_2b4ed_row82_col1\" class=\"data row82 col1\" >['▁valesid']</td>\n",
       "      <td id=\"T_2b4ed_row82_col2\" class=\"data row82 col2\" >['pl p_S']</td>\n",
       "      <td id=\"T_2b4ed_row82_col3\" class=\"data row82 col3\" >[0.99963]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row83_col0\" class=\"data row83 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row83_col1\" class=\"data row83 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row83_col2\" class=\"data row83 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row83_col3\" class=\"data row83 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row84_col0\" class=\"data row84 col0\" >['mis']</td>\n",
       "      <td id=\"T_2b4ed_row84_col1\" class=\"data row84 col1\" >['▁mis']</td>\n",
       "      <td id=\"T_2b4ed_row84_col2\" class=\"data row84 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row84_col3\" class=\"data row84 col3\" >[0.99986]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row85_col0\" class=\"data row85 col0\" >['on']</td>\n",
       "      <td id=\"T_2b4ed_row85_col1\" class=\"data row85 col1\" >['▁on']</td>\n",
       "      <td id=\"T_2b4ed_row85_col2\" class=\"data row85 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row85_col3\" class=\"data row85 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row86_col0\" class=\"data row86 col0\" >['kivistunud']</td>\n",
       "      <td id=\"T_2b4ed_row86_col1\" class=\"data row86 col1\" >['▁kivist', 'unud']</td>\n",
       "      <td id=\"T_2b4ed_row86_col2\" class=\"data row86 col2\" >['nud_V']</td>\n",
       "      <td id=\"T_2b4ed_row86_col3\" class=\"data row86 col3\" >[0.78979]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row87_col0\" class=\"data row87 col0\" >['lausa']</td>\n",
       "      <td id=\"T_2b4ed_row87_col1\" class=\"data row87 col1\" >['▁lausa']</td>\n",
       "      <td id=\"T_2b4ed_row87_col2\" class=\"data row87 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row87_col3\" class=\"data row87 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row88_col0\" class=\"data row88 col0\" >['müütideks']</td>\n",
       "      <td id=\"T_2b4ed_row88_col1\" class=\"data row88 col1\" >['▁müüt', 'ideks']</td>\n",
       "      <td id=\"T_2b4ed_row88_col2\" class=\"data row88 col2\" >['pl tr_S']</td>\n",
       "      <td id=\"T_2b4ed_row88_col3\" class=\"data row88 col3\" >[0.99579]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row89_col0\" class=\"data row89 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row89_col1\" class=\"data row89 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row89_col2\" class=\"data row89 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row89_col3\" class=\"data row89 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row90_col0\" class=\"data row90 col0\" >['Nende']</td>\n",
       "      <td id=\"T_2b4ed_row90_col1\" class=\"data row90 col1\" >['▁Nende']</td>\n",
       "      <td id=\"T_2b4ed_row90_col2\" class=\"data row90 col2\" >['pl g_P']</td>\n",
       "      <td id=\"T_2b4ed_row90_col3\" class=\"data row90 col3\" >[0.99945]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row91_col0\" class=\"data row91 col0\" >['müütide']</td>\n",
       "      <td id=\"T_2b4ed_row91_col1\" class=\"data row91 col1\" >['▁müü', 'tide']</td>\n",
       "      <td id=\"T_2b4ed_row91_col2\" class=\"data row91 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row91_col3\" class=\"data row91 col3\" >[0.99962]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row92_col0\" class=\"data row92 col0\" >['lõhkumine']</td>\n",
       "      <td id=\"T_2b4ed_row92_col1\" class=\"data row92 col1\" >['▁lõhku', 'mine']</td>\n",
       "      <td id=\"T_2b4ed_row92_col2\" class=\"data row92 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row92_col3\" class=\"data row92 col3\" >[0.99977]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row93_col0\" class=\"data row93 col0\" >['on']</td>\n",
       "      <td id=\"T_2b4ed_row93_col1\" class=\"data row93 col1\" >['▁on']</td>\n",
       "      <td id=\"T_2b4ed_row93_col2\" class=\"data row93 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row93_col3\" class=\"data row93 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row94_col0\" class=\"data row94 col0\" >['nüüd']</td>\n",
       "      <td id=\"T_2b4ed_row94_col1\" class=\"data row94 col1\" >['▁nüüd']</td>\n",
       "      <td id=\"T_2b4ed_row94_col2\" class=\"data row94 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row94_col3\" class=\"data row94 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row95_col0\" class=\"data row95 col0\" >['siis']</td>\n",
       "      <td id=\"T_2b4ed_row95_col1\" class=\"data row95 col1\" >['▁siis']</td>\n",
       "      <td id=\"T_2b4ed_row95_col2\" class=\"data row95 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row95_col3\" class=\"data row95 col3\" >[0.99979]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row96_col0\" class=\"data row96 col0\" >['karistatav']</td>\n",
       "      <td id=\"T_2b4ed_row96_col1\" class=\"data row96 col1\" >['▁karistatav']</td>\n",
       "      <td id=\"T_2b4ed_row96_col2\" class=\"data row96 col2\" >['sg n_A']</td>\n",
       "      <td id=\"T_2b4ed_row96_col3\" class=\"data row96 col3\" >[0.99565]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row97_col0\" class=\"data row97 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row97_col1\" class=\"data row97 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row97_col2\" class=\"data row97 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row97_col3\" class=\"data row97 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row98_col0\" class=\"data row98 col0\" >['Suur']</td>\n",
       "      <td id=\"T_2b4ed_row98_col1\" class=\"data row98 col1\" >['▁Suur']</td>\n",
       "      <td id=\"T_2b4ed_row98_col2\" class=\"data row98 col2\" >['sg n_A']</td>\n",
       "      <td id=\"T_2b4ed_row98_col3\" class=\"data row98 col3\" >[0.99975]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row99_col0\" class=\"data row99 col0\" >['löök']</td>\n",
       "      <td id=\"T_2b4ed_row99_col1\" class=\"data row99 col1\" >['▁löök']</td>\n",
       "      <td id=\"T_2b4ed_row99_col2\" class=\"data row99 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row99_col3\" class=\"data row99 col3\" >[0.99972]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row100_col0\" class=\"data row100 col0\" >['sõnavabadusele']</td>\n",
       "      <td id=\"T_2b4ed_row100_col1\" class=\"data row100 col1\" >['▁sõna', 'vabad', 'usele']</td>\n",
       "      <td id=\"T_2b4ed_row100_col2\" class=\"data row100 col2\" >['sg all_S']</td>\n",
       "      <td id=\"T_2b4ed_row100_col3\" class=\"data row100 col3\" >[0.99909]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row101_col0\" class=\"data row101 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row101_col1\" class=\"data row101 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row101_col2\" class=\"data row101 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row101_col3\" class=\"data row101 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row102_col0\" class=\"data row102 col0\" >['tõhus']</td>\n",
       "      <td id=\"T_2b4ed_row102_col1\" class=\"data row102 col1\" >['▁tõhus']</td>\n",
       "      <td id=\"T_2b4ed_row102_col2\" class=\"data row102 col2\" >['sg n_A']</td>\n",
       "      <td id=\"T_2b4ed_row102_col3\" class=\"data row102 col3\" >[0.99977]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row103_col0\" class=\"data row103 col0\" >['tugi']</td>\n",
       "      <td id=\"T_2b4ed_row103_col1\" class=\"data row103 col1\" >['▁tugi']</td>\n",
       "      <td id=\"T_2b4ed_row103_col2\" class=\"data row103 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row103_col3\" class=\"data row103 col3\" >[0.99963]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row104_col0\" class=\"data row104 col0\" >['valede']</td>\n",
       "      <td id=\"T_2b4ed_row104_col1\" class=\"data row104 col1\" >['▁valede']</td>\n",
       "      <td id=\"T_2b4ed_row104_col2\" class=\"data row104 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row104_col3\" class=\"data row104 col3\" >[0.99957]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row105_col0\" class=\"data row105 col0\" >['püsimajäämisele']</td>\n",
       "      <td id=\"T_2b4ed_row105_col1\" class=\"data row105 col1\" >['▁püsima', 'jää', 'misele']</td>\n",
       "      <td id=\"T_2b4ed_row105_col2\" class=\"data row105 col2\" >['sg all_S']</td>\n",
       "      <td id=\"T_2b4ed_row105_col3\" class=\"data row105 col3\" >[0.99932]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row106_col0\" class=\"data row106 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row106_col1\" class=\"data row106 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row106_col2\" class=\"data row106 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row106_col3\" class=\"data row106 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row107_col0\" class=\"data row107 col0\" >['Aga']</td>\n",
       "      <td id=\"T_2b4ed_row107_col1\" class=\"data row107 col1\" >['▁Aga']</td>\n",
       "      <td id=\"T_2b4ed_row107_col2\" class=\"data row107 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row107_col3\" class=\"data row107 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row108_col0\" class=\"data row108 col0\" >['äkki']</td>\n",
       "      <td id=\"T_2b4ed_row108_col1\" class=\"data row108 col1\" >['▁äkki']</td>\n",
       "      <td id=\"T_2b4ed_row108_col2\" class=\"data row108 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row108_col3\" class=\"data row108 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row109_col0\" class=\"data row109 col0\" >['tuleks']</td>\n",
       "      <td id=\"T_2b4ed_row109_col1\" class=\"data row109 col1\" >['▁tuleks']</td>\n",
       "      <td id=\"T_2b4ed_row109_col2\" class=\"data row109 col2\" >['ks_V']</td>\n",
       "      <td id=\"T_2b4ed_row109_col3\" class=\"data row109 col3\" >[0.9997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row110_col0\" class=\"data row110 col0\" >['natsismi']</td>\n",
       "      <td id=\"T_2b4ed_row110_col1\" class=\"data row110 col1\" >['▁natsis', 'mi']</td>\n",
       "      <td id=\"T_2b4ed_row110_col2\" class=\"data row110 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row110_col3\" class=\"data row110 col3\" >[0.99961]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row111_col0\" class=\"data row111 col0\" >['võitmist']</td>\n",
       "      <td id=\"T_2b4ed_row111_col1\" class=\"data row111 col1\" >['▁võit', 'mist']</td>\n",
       "      <td id=\"T_2b4ed_row111_col2\" class=\"data row111 col2\" >['sg p_S']</td>\n",
       "      <td id=\"T_2b4ed_row111_col3\" class=\"data row111 col3\" >[0.99962]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row112_col0\" class=\"data row112 col0\" >['hinnata']</td>\n",
       "      <td id=\"T_2b4ed_row112_col1\" class=\"data row112 col1\" >['▁hinnata']</td>\n",
       "      <td id=\"T_2b4ed_row112_col2\" class=\"data row112 col2\" >['da_V']</td>\n",
       "      <td id=\"T_2b4ed_row112_col3\" class=\"data row112 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row113_col0\" class=\"data row113 col0\" >['ikka']</td>\n",
       "      <td id=\"T_2b4ed_row113_col1\" class=\"data row113 col1\" >['▁ikka']</td>\n",
       "      <td id=\"T_2b4ed_row113_col2\" class=\"data row113 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row113_col3\" class=\"data row113 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row114_col0\" class=\"data row114 col0\" >['kõigi']</td>\n",
       "      <td id=\"T_2b4ed_row114_col1\" class=\"data row114 col1\" >['▁kõigi']</td>\n",
       "      <td id=\"T_2b4ed_row114_col2\" class=\"data row114 col2\" >['pl g_P']</td>\n",
       "      <td id=\"T_2b4ed_row114_col3\" class=\"data row114 col3\" >[0.9994]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row115_col0\" class=\"data row115 col0\" >['võitjate']</td>\n",
       "      <td id=\"T_2b4ed_row115_col1\" class=\"data row115 col1\" >['▁võitjate']</td>\n",
       "      <td id=\"T_2b4ed_row115_col2\" class=\"data row115 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row115_col3\" class=\"data row115 col3\" >[0.99958]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row116_col0\" class=\"data row116 col0\" >['seisukohast']</td>\n",
       "      <td id=\"T_2b4ed_row116_col1\" class=\"data row116 col1\" >['▁seisukohast']</td>\n",
       "      <td id=\"T_2b4ed_row116_col2\" class=\"data row116 col2\" >['sg el_S']</td>\n",
       "      <td id=\"T_2b4ed_row116_col3\" class=\"data row116 col3\" >[0.99911]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row117_col0\" class=\"data row117 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row117_col1\" class=\"data row117 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row117_col2\" class=\"data row117 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row117_col3\" class=\"data row117 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row118_col0\" class=\"data row118 col0\" >['mitte']</td>\n",
       "      <td id=\"T_2b4ed_row118_col1\" class=\"data row118 col1\" >['▁mitte']</td>\n",
       "      <td id=\"T_2b4ed_row118_col2\" class=\"data row118 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row118_col3\" class=\"data row118 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row119_col0\" class=\"data row119 col0\" >['ainult']</td>\n",
       "      <td id=\"T_2b4ed_row119_col1\" class=\"data row119 col1\" >['▁ainult']</td>\n",
       "      <td id=\"T_2b4ed_row119_col2\" class=\"data row119 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row119_col3\" class=\"data row119 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row120_col0\" class=\"data row120 col0\" >['vene']</td>\n",
       "      <td id=\"T_2b4ed_row120_col1\" class=\"data row120 col1\" >['▁vene']</td>\n",
       "      <td id=\"T_2b4ed_row120_col2\" class=\"data row120 col2\" >['G']</td>\n",
       "      <td id=\"T_2b4ed_row120_col3\" class=\"data row120 col3\" >[0.86184]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row121_col0\" class=\"data row121 col0\" >['omast']</td>\n",
       "      <td id=\"T_2b4ed_row121_col1\" class=\"data row121 col1\" >['▁omast']</td>\n",
       "      <td id=\"T_2b4ed_row121_col2\" class=\"data row121 col2\" >['sg el_P']</td>\n",
       "      <td id=\"T_2b4ed_row121_col3\" class=\"data row121 col3\" >[0.99855]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row122_col0\" class=\"data row122 col0\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row122_col1\" class=\"data row122 col1\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row122_col2\" class=\"data row122 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row122_col3\" class=\"data row122 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row123_col0\" class=\"data row123 col0\" >['Ehk']</td>\n",
       "      <td id=\"T_2b4ed_row123_col1\" class=\"data row123 col1\" >['▁Ehk']</td>\n",
       "      <td id=\"T_2b4ed_row123_col2\" class=\"data row123 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row123_col3\" class=\"data row123 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row124_col0\" class=\"data row124 col0\" >['tuleks']</td>\n",
       "      <td id=\"T_2b4ed_row124_col1\" class=\"data row124 col1\" >['▁tuleks']</td>\n",
       "      <td id=\"T_2b4ed_row124_col2\" class=\"data row124 col2\" >['ks_V']</td>\n",
       "      <td id=\"T_2b4ed_row124_col3\" class=\"data row124 col3\" >[0.99971]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row125_col0\" class=\"data row125 col0\" >['lugeda']</td>\n",
       "      <td id=\"T_2b4ed_row125_col1\" class=\"data row125 col1\" >['▁lugeda']</td>\n",
       "      <td id=\"T_2b4ed_row125_col2\" class=\"data row125 col2\" >['da_V']</td>\n",
       "      <td id=\"T_2b4ed_row125_col3\" class=\"data row125 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row126_col0\" class=\"data row126 col0\" >['karistamist']</td>\n",
       "      <td id=\"T_2b4ed_row126_col1\" class=\"data row126 col1\" >['▁karistamist']</td>\n",
       "      <td id=\"T_2b4ed_row126_col2\" class=\"data row126 col2\" >['sg p_S']</td>\n",
       "      <td id=\"T_2b4ed_row126_col3\" class=\"data row126 col3\" >[0.99964]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row127_col0\" class=\"data row127 col0\" >['väärivaks']</td>\n",
       "      <td id=\"T_2b4ed_row127_col1\" class=\"data row127 col1\" >['▁vääri', 'vaks']</td>\n",
       "      <td id=\"T_2b4ed_row127_col2\" class=\"data row127 col2\" >['sg tr_A']</td>\n",
       "      <td id=\"T_2b4ed_row127_col3\" class=\"data row127 col3\" >[0.99898]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row128_col0\" class=\"data row128 col0\" >['ameeriklaste']</td>\n",
       "      <td id=\"T_2b4ed_row128_col1\" class=\"data row128 col1\" >['▁ameeriklaste']</td>\n",
       "      <td id=\"T_2b4ed_row128_col2\" class=\"data row128 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row128_col3\" class=\"data row128 col3\" >[0.9994]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row129_col0\" class=\"data row129 col0\" >[\"lend-lease'i\"]</td>\n",
       "      <td id=\"T_2b4ed_row129_col1\" class=\"data row129 col1\" >['▁lend', '-', 'le', 'ase', \"'\", 'i']</td>\n",
       "      <td id=\"T_2b4ed_row129_col2\" class=\"data row129 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row129_col3\" class=\"data row129 col3\" >[0.99944]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row130_col0\" class=\"data row130 col0\" >['alahindamine']</td>\n",
       "      <td id=\"T_2b4ed_row130_col1\" class=\"data row130 col1\" >['▁ala', 'hinda', 'mine']</td>\n",
       "      <td id=\"T_2b4ed_row130_col2\" class=\"data row130 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row130_col3\" class=\"data row130 col3\" >[0.99975]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row131_col0\" class=\"data row131 col0\" >['ja']</td>\n",
       "      <td id=\"T_2b4ed_row131_col1\" class=\"data row131 col1\" >['▁ja']</td>\n",
       "      <td id=\"T_2b4ed_row131_col2\" class=\"data row131 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row131_col3\" class=\"data row131 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row132_col0\" class=\"data row132 col0\" >['kohati']</td>\n",
       "      <td id=\"T_2b4ed_row132_col1\" class=\"data row132 col1\" >['▁kohati']</td>\n",
       "      <td id=\"T_2b4ed_row132_col2\" class=\"data row132 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row132_col3\" class=\"data row132 col3\" >[0.99988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row133_col0\" class=\"data row133 col0\" >['lausa']</td>\n",
       "      <td id=\"T_2b4ed_row133_col1\" class=\"data row133 col1\" >['▁lausa']</td>\n",
       "      <td id=\"T_2b4ed_row133_col2\" class=\"data row133 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row133_col3\" class=\"data row133 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row134_col0\" class=\"data row134 col0\" >['naeruvääristamine']</td>\n",
       "      <td id=\"T_2b4ed_row134_col1\" class=\"data row134 col1\" >['▁naeruvää', 'rista', 'mine']</td>\n",
       "      <td id=\"T_2b4ed_row134_col2\" class=\"data row134 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row134_col3\" class=\"data row134 col3\" >[0.99974]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row135_col0\" class=\"data row135 col0\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row135_col1\" class=\"data row135 col1\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row135_col2\" class=\"data row135 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row135_col3\" class=\"data row135 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row136_col0\" class=\"data row136 col0\" >['Või']</td>\n",
       "      <td id=\"T_2b4ed_row136_col1\" class=\"data row136 col1\" >['▁Või']</td>\n",
       "      <td id=\"T_2b4ed_row136_col2\" class=\"data row136 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row136_col3\" class=\"data row136 col3\" >[0.99985]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row137_col0\" class=\"data row137 col0\" >['vaadata']</td>\n",
       "      <td id=\"T_2b4ed_row137_col1\" class=\"data row137 col1\" >['▁vaadata']</td>\n",
       "      <td id=\"T_2b4ed_row137_col2\" class=\"data row137 col2\" >['da_V']</td>\n",
       "      <td id=\"T_2b4ed_row137_col3\" class=\"data row137 col3\" >[0.99988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row138_col0\" class=\"data row138 col0\" >['hoopis']</td>\n",
       "      <td id=\"T_2b4ed_row138_col1\" class=\"data row138 col1\" >['▁hoopis']</td>\n",
       "      <td id=\"T_2b4ed_row138_col2\" class=\"data row138 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row138_col3\" class=\"data row138 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row139_col0\" class=\"data row139 col0\" >['teise']</td>\n",
       "      <td id=\"T_2b4ed_row139_col1\" class=\"data row139 col1\" >['▁teise']</td>\n",
       "      <td id=\"T_2b4ed_row139_col2\" class=\"data row139 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row139_col3\" class=\"data row139 col3\" >[0.99969]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row140_col0\" class=\"data row140 col0\" >['pilguga']</td>\n",
       "      <td id=\"T_2b4ed_row140_col1\" class=\"data row140 col1\" >['▁pilguga']</td>\n",
       "      <td id=\"T_2b4ed_row140_col2\" class=\"data row140 col2\" >['sg kom_S']</td>\n",
       "      <td id=\"T_2b4ed_row140_col3\" class=\"data row140 col3\" >[0.9997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row141_col0\" class=\"data row141 col0\" >['Suurbritannia']</td>\n",
       "      <td id=\"T_2b4ed_row141_col1\" class=\"data row141 col1\" >['▁Suurbritannia']</td>\n",
       "      <td id=\"T_2b4ed_row141_col2\" class=\"data row141 col2\" >['sg g_H']</td>\n",
       "      <td id=\"T_2b4ed_row141_col3\" class=\"data row141 col3\" >[0.99861]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row142_col0\" class=\"data row142 col0\" >['sõja']</td>\n",
       "      <td id=\"T_2b4ed_row142_col1\" class=\"data row142 col1\" >['▁sõja']</td>\n",
       "      <td id=\"T_2b4ed_row142_col2\" class=\"data row142 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row142_col3\" class=\"data row142 col3\" >[0.99964]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row143_col0\" class=\"data row143 col0\" >['algaastate']</td>\n",
       "      <td id=\"T_2b4ed_row143_col1\" class=\"data row143 col1\" >['▁alga', 'asta', 'te']</td>\n",
       "      <td id=\"T_2b4ed_row143_col2\" class=\"data row143 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row143_col3\" class=\"data row143 col3\" >[0.99917]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row144_col0\" class=\"data row144 col0\" >['rolli']</td>\n",
       "      <td id=\"T_2b4ed_row144_col1\" class=\"data row144 col1\" >['▁rolli']</td>\n",
       "      <td id=\"T_2b4ed_row144_col2\" class=\"data row144 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row144_col3\" class=\"data row144 col3\" >[0.99964]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row145_col0\" class=\"data row145 col0\" >['alaväärstamisele']</td>\n",
       "      <td id=\"T_2b4ed_row145_col1\" class=\"data row145 col1\" >['▁alaväär', 'sta', 'misele']</td>\n",
       "      <td id=\"T_2b4ed_row145_col2\" class=\"data row145 col2\" >['sg all_S']</td>\n",
       "      <td id=\"T_2b4ed_row145_col3\" class=\"data row145 col3\" >[0.99932]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row146_col0\" class=\"data row146 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row146_col1\" class=\"data row146 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row146_col2\" class=\"data row146 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row146_col3\" class=\"data row146 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row147_col0\" class=\"data row147 col0\" >['võttis']</td>\n",
       "      <td id=\"T_2b4ed_row147_col1\" class=\"data row147 col1\" >['▁võttis']</td>\n",
       "      <td id=\"T_2b4ed_row147_col2\" class=\"data row147 col2\" >['s_V']</td>\n",
       "      <td id=\"T_2b4ed_row147_col3\" class=\"data row147 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row148_col0\" class=\"data row148 col0\" >['ju']</td>\n",
       "      <td id=\"T_2b4ed_row148_col1\" class=\"data row148 col1\" >['▁ju']</td>\n",
       "      <td id=\"T_2b4ed_row148_col2\" class=\"data row148 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row148_col3\" class=\"data row148 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row149_col0\" class=\"data row149 col0\" >['tema']</td>\n",
       "      <td id=\"T_2b4ed_row149_col1\" class=\"data row149 col1\" >['▁tema']</td>\n",
       "      <td id=\"T_2b4ed_row149_col2\" class=\"data row149 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row149_col3\" class=\"data row149 col3\" >[0.99972]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row150_col0\" class=\"data row150 col0\" >['vastu']</td>\n",
       "      <td id=\"T_2b4ed_row150_col1\" class=\"data row150 col1\" >['▁vastu']</td>\n",
       "      <td id=\"T_2b4ed_row150_col2\" class=\"data row150 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row150_col3\" class=\"data row150 col3\" >[0.99963]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row151_col0\" class=\"data row151 col0\" >['sakslaste']</td>\n",
       "      <td id=\"T_2b4ed_row151_col1\" class=\"data row151 col1\" >['▁sakslaste']</td>\n",
       "      <td id=\"T_2b4ed_row151_col2\" class=\"data row151 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row151_col3\" class=\"data row151 col3\" >[0.99945]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row152_col0\" class=\"data row152 col0\" >['põhilöögi']</td>\n",
       "      <td id=\"T_2b4ed_row152_col1\" class=\"data row152 col1\" >['▁põh', 'il', 'öögi']</td>\n",
       "      <td id=\"T_2b4ed_row152_col2\" class=\"data row152 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row152_col3\" class=\"data row152 col3\" >[0.9996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row153_col0\" class=\"data row153 col0\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row153_col1\" class=\"data row153 col1\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row153_col2\" class=\"data row153 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row153_col3\" class=\"data row153 col3\" >[0.99994]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row154_col0\" class=\"data row154 col0\" >['Vastust']</td>\n",
       "      <td id=\"T_2b4ed_row154_col1\" class=\"data row154 col1\" >['▁Vast', 'ust']</td>\n",
       "      <td id=\"T_2b4ed_row154_col2\" class=\"data row154 col2\" >['sg p_S']</td>\n",
       "      <td id=\"T_2b4ed_row154_col3\" class=\"data row154 col3\" >[0.99937]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row155_col0\" class=\"data row155 col0\" >['tuleks']</td>\n",
       "      <td id=\"T_2b4ed_row155_col1\" class=\"data row155 col1\" >['▁tuleks']</td>\n",
       "      <td id=\"T_2b4ed_row155_col2\" class=\"data row155 col2\" >['ks_V']</td>\n",
       "      <td id=\"T_2b4ed_row155_col3\" class=\"data row155 col3\" >[0.99972]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row156_col0\" class=\"data row156 col0\" >['nõuda']</td>\n",
       "      <td id=\"T_2b4ed_row156_col1\" class=\"data row156 col1\" >['▁nõuda']</td>\n",
       "      <td id=\"T_2b4ed_row156_col2\" class=\"data row156 col2\" >['da_V']</td>\n",
       "      <td id=\"T_2b4ed_row156_col3\" class=\"data row156 col3\" >[0.99988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row157_col0\" class=\"data row157 col0\" >['ka']</td>\n",
       "      <td id=\"T_2b4ed_row157_col1\" class=\"data row157 col1\" >['▁ka']</td>\n",
       "      <td id=\"T_2b4ed_row157_col2\" class=\"data row157 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row157_col3\" class=\"data row157 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row158_col0\" class=\"data row158 col0\" >['nendelt']</td>\n",
       "      <td id=\"T_2b4ed_row158_col1\" class=\"data row158 col1\" >['▁nendelt']</td>\n",
       "      <td id=\"T_2b4ed_row158_col2\" class=\"data row158 col2\" >['pl abl_P']</td>\n",
       "      <td id=\"T_2b4ed_row158_col3\" class=\"data row158 col3\" >[0.92749]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row159_col0\" class=\"data row159 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row159_col1\" class=\"data row159 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row159_col2\" class=\"data row159 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row159_col3\" class=\"data row159 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row160_col0\" class=\"data row160 col0\" >['kes']</td>\n",
       "      <td id=\"T_2b4ed_row160_col1\" class=\"data row160 col1\" >['▁kes']</td>\n",
       "      <td id=\"T_2b4ed_row160_col2\" class=\"data row160 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row160_col3\" class=\"data row160 col3\" >[0.99986]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row161_col0\" class=\"data row161 col0\" >['lasevad']</td>\n",
       "      <td id=\"T_2b4ed_row161_col1\" class=\"data row161 col1\" >['▁lasevad']</td>\n",
       "      <td id=\"T_2b4ed_row161_col2\" class=\"data row161 col2\" >['vad_V']</td>\n",
       "      <td id=\"T_2b4ed_row161_col3\" class=\"data row161 col3\" >[0.99948]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row162_col0\" class=\"data row162 col0\" >['välja']</td>\n",
       "      <td id=\"T_2b4ed_row162_col1\" class=\"data row162 col1\" >['▁välja']</td>\n",
       "      <td id=\"T_2b4ed_row162_col2\" class=\"data row162 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row162_col3\" class=\"data row162 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row163_col0\" class=\"data row163 col0\" >['paista']</td>\n",
       "      <td id=\"T_2b4ed_row163_col1\" class=\"data row163 col1\" >['▁paista']</td>\n",
       "      <td id=\"T_2b4ed_row163_col2\" class=\"data row163 col2\" >['da_V']</td>\n",
       "      <td id=\"T_2b4ed_row163_col3\" class=\"data row163 col3\" >[0.99981]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row164_col0\" class=\"data row164 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row164_col1\" class=\"data row164 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row164_col2\" class=\"data row164 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row164_col3\" class=\"data row164 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row165_col0\" class=\"data row165 col0\" >['nagu']</td>\n",
       "      <td id=\"T_2b4ed_row165_col1\" class=\"data row165 col1\" >['▁nagu']</td>\n",
       "      <td id=\"T_2b4ed_row165_col2\" class=\"data row165 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row165_col3\" class=\"data row165 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row166_col0\" class=\"data row166 col0\" >['oleks']</td>\n",
       "      <td id=\"T_2b4ed_row166_col1\" class=\"data row166 col1\" >['▁oleks']</td>\n",
       "      <td id=\"T_2b4ed_row166_col2\" class=\"data row166 col2\" >['ks_V']</td>\n",
       "      <td id=\"T_2b4ed_row166_col3\" class=\"data row166 col3\" >[0.99973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row167_col0\" class=\"data row167 col0\" >['Hitleri']</td>\n",
       "      <td id=\"T_2b4ed_row167_col1\" class=\"data row167 col1\" >['▁Hitleri']</td>\n",
       "      <td id=\"T_2b4ed_row167_col2\" class=\"data row167 col2\" >['sg g_H']</td>\n",
       "      <td id=\"T_2b4ed_row167_col3\" class=\"data row167 col3\" >[0.9988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row168_col0\" class=\"data row168 col0\" >['vastu']</td>\n",
       "      <td id=\"T_2b4ed_row168_col1\" class=\"data row168 col1\" >['▁vastu']</td>\n",
       "      <td id=\"T_2b4ed_row168_col2\" class=\"data row168 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row168_col3\" class=\"data row168 col3\" >[0.9986]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row169_col0\" class=\"data row169 col0\" >['sõdinud']</td>\n",
       "      <td id=\"T_2b4ed_row169_col1\" class=\"data row169 col1\" >['▁sõd', 'inud']</td>\n",
       "      <td id=\"T_2b4ed_row169_col2\" class=\"data row169 col2\" >['nud_V']</td>\n",
       "      <td id=\"T_2b4ed_row169_col3\" class=\"data row169 col3\" >[0.97377]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row170_col0\" class=\"data row170 col0\" >['(']</td>\n",
       "      <td id=\"T_2b4ed_row170_col1\" class=\"data row170 col1\" >['▁(']</td>\n",
       "      <td id=\"T_2b4ed_row170_col2\" class=\"data row170 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row170_col3\" class=\"data row170 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row171_col0\" class=\"data row171 col0\" >['ja']</td>\n",
       "      <td id=\"T_2b4ed_row171_col1\" class=\"data row171 col1\" >['ja']</td>\n",
       "      <td id=\"T_2b4ed_row171_col2\" class=\"data row171 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row171_col3\" class=\"data row171 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row172_col0\" class=\"data row172 col0\" >['teda']</td>\n",
       "      <td id=\"T_2b4ed_row172_col1\" class=\"data row172 col1\" >['▁teda']</td>\n",
       "      <td id=\"T_2b4ed_row172_col2\" class=\"data row172 col2\" >['sg p_P']</td>\n",
       "      <td id=\"T_2b4ed_row172_col3\" class=\"data row172 col3\" >[0.99981]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row173_col0\" class=\"data row173 col0\" >['võitnud']</td>\n",
       "      <td id=\"T_2b4ed_row173_col1\" class=\"data row173 col1\" >['▁võitnud']</td>\n",
       "      <td id=\"T_2b4ed_row173_col2\" class=\"data row173 col2\" >['nud_V']</td>\n",
       "      <td id=\"T_2b4ed_row173_col3\" class=\"data row173 col3\" >[0.99929]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row174_col0\" class=\"data row174 col0\" >[')']</td>\n",
       "      <td id=\"T_2b4ed_row174_col1\" class=\"data row174 col1\" >[')']</td>\n",
       "      <td id=\"T_2b4ed_row174_col2\" class=\"data row174 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row174_col3\" class=\"data row174 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row175_col0\" class=\"data row175 col0\" >['vaid']</td>\n",
       "      <td id=\"T_2b4ed_row175_col1\" class=\"data row175 col1\" >['▁vaid']</td>\n",
       "      <td id=\"T_2b4ed_row175_col2\" class=\"data row175 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row175_col3\" class=\"data row175 col3\" >[0.99985]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row176_col0\" class=\"data row176 col0\" >['Nõukogude']</td>\n",
       "      <td id=\"T_2b4ed_row176_col1\" class=\"data row176 col1\" >['▁Nõukogude']</td>\n",
       "      <td id=\"T_2b4ed_row176_col2\" class=\"data row176 col2\" >['pl g_H']</td>\n",
       "      <td id=\"T_2b4ed_row176_col3\" class=\"data row176 col3\" >[0.94688]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row177_col0\" class=\"data row177 col0\" >['Liit']</td>\n",
       "      <td id=\"T_2b4ed_row177_col1\" class=\"data row177 col1\" >['▁Liit']</td>\n",
       "      <td id=\"T_2b4ed_row177_col2\" class=\"data row177 col2\" >['sg n_H']</td>\n",
       "      <td id=\"T_2b4ed_row177_col3\" class=\"data row177 col3\" >[0.99914]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row178_col0\" class=\"data row178 col0\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row178_col1\" class=\"data row178 col1\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row178_col2\" class=\"data row178 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row178_col3\" class=\"data row178 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row179_col0\" class=\"data row179 col0\" >['Nii']</td>\n",
       "      <td id=\"T_2b4ed_row179_col1\" class=\"data row179 col1\" >['▁Nii']</td>\n",
       "      <td id=\"T_2b4ed_row179_col2\" class=\"data row179 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row179_col3\" class=\"data row179 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row180_col0\" class=\"data row180 col0\" >['võib']</td>\n",
       "      <td id=\"T_2b4ed_row180_col1\" class=\"data row180 col1\" >['▁võib']</td>\n",
       "      <td id=\"T_2b4ed_row180_col2\" class=\"data row180 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row180_col3\" class=\"data row180 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row181_col0\" class=\"data row181 col0\" >['mõõgatera']</td>\n",
       "      <td id=\"T_2b4ed_row181_col1\" class=\"data row181 col1\" >['▁mõõ', 'ga', 'tera']</td>\n",
       "      <td id=\"T_2b4ed_row181_col2\" class=\"data row181 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row181_col3\" class=\"data row181 col3\" >[0.9995]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row182_col0\" class=\"data row182 col0\" >['pöörduda']</td>\n",
       "      <td id=\"T_2b4ed_row182_col1\" class=\"data row182 col1\" >['▁pöörduda']</td>\n",
       "      <td id=\"T_2b4ed_row182_col2\" class=\"data row182 col2\" >['da_V']</td>\n",
       "      <td id=\"T_2b4ed_row182_col3\" class=\"data row182 col3\" >[0.99988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row183_col0\" class=\"data row183 col0\" >['hoopis']</td>\n",
       "      <td id=\"T_2b4ed_row183_col1\" class=\"data row183 col1\" >['▁hoopis']</td>\n",
       "      <td id=\"T_2b4ed_row183_col2\" class=\"data row183 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row183_col3\" class=\"data row183 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row184_col0\" class=\"data row184 col0\" >['iseenda']</td>\n",
       "      <td id=\"T_2b4ed_row184_col1\" class=\"data row184 col1\" >['▁iseenda']</td>\n",
       "      <td id=\"T_2b4ed_row184_col2\" class=\"data row184 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row184_col3\" class=\"data row184 col3\" >[0.99967]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row185_col0\" class=\"data row185 col0\" >['vastu']</td>\n",
       "      <td id=\"T_2b4ed_row185_col1\" class=\"data row185 col1\" >['▁vastu']</td>\n",
       "      <td id=\"T_2b4ed_row185_col2\" class=\"data row185 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row185_col3\" class=\"data row185 col3\" >[0.99968]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row186_col0\" class=\"data row186 col0\" >['!']</td>\n",
       "      <td id=\"T_2b4ed_row186_col1\" class=\"data row186 col1\" >['!']</td>\n",
       "      <td id=\"T_2b4ed_row186_col2\" class=\"data row186 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row186_col3\" class=\"data row186 col3\" >[0.99994]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row187_col0\" class=\"data row187 col0\" >['Tuvastamata']</td>\n",
       "      <td id=\"T_2b4ed_row187_col1\" class=\"data row187 col1\" >['▁Tu', 'vasta', 'mata']</td>\n",
       "      <td id=\"T_2b4ed_row187_col2\" class=\"data row187 col2\" >['A']</td>\n",
       "      <td id=\"T_2b4ed_row187_col3\" class=\"data row187 col3\" >[0.95848]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row188_col0\" class=\"data row188 col0\" >['Kasutaja']</td>\n",
       "      <td id=\"T_2b4ed_row188_col1\" class=\"data row188 col1\" >['▁Kasutaja']</td>\n",
       "      <td id=\"T_2b4ed_row188_col2\" class=\"data row188 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row188_col3\" class=\"data row188 col3\" >[0.99825]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row189_col0\" class=\"data row189 col0\" >['Suure']</td>\n",
       "      <td id=\"T_2b4ed_row189_col1\" class=\"data row189 col1\" >['▁Suure']</td>\n",
       "      <td id=\"T_2b4ed_row189_col2\" class=\"data row189 col2\" >['sg g_A']</td>\n",
       "      <td id=\"T_2b4ed_row189_col3\" class=\"data row189 col3\" >[0.99936]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row190_col0\" class=\"data row190 col0\" >['Isamaasõja']</td>\n",
       "      <td id=\"T_2b4ed_row190_col1\" class=\"data row190 col1\" >['▁Isa', 'maas', 'õja']</td>\n",
       "      <td id=\"T_2b4ed_row190_col2\" class=\"data row190 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row190_col3\" class=\"data row190 col3\" >[0.99969]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row191_col0\" class=\"data row191 col0\" >['tulemuste']</td>\n",
       "      <td id=\"T_2b4ed_row191_col1\" class=\"data row191 col1\" >['▁tulemuste']</td>\n",
       "      <td id=\"T_2b4ed_row191_col2\" class=\"data row191 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row191_col3\" class=\"data row191 col3\" >[0.99848]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row192_col0\" class=\"data row192 col0\" >['eitamise']</td>\n",
       "      <td id=\"T_2b4ed_row192_col1\" class=\"data row192 col1\" >['▁ei', 'tamise']</td>\n",
       "      <td id=\"T_2b4ed_row192_col2\" class=\"data row192 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row192_col3\" class=\"data row192 col3\" >[0.99963]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row193_col0\" class=\"data row193 col0\" >['või']</td>\n",
       "      <td id=\"T_2b4ed_row193_col1\" class=\"data row193 col1\" >['▁või']</td>\n",
       "      <td id=\"T_2b4ed_row193_col2\" class=\"data row193 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row193_col3\" class=\"data row193 col3\" >[0.99987]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row194_col0\" class=\"data row194 col0\" >['revideerimise']</td>\n",
       "      <td id=\"T_2b4ed_row194_col1\" class=\"data row194 col1\" >['▁revid', 'eerimise']</td>\n",
       "      <td id=\"T_2b4ed_row194_col2\" class=\"data row194 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row194_col3\" class=\"data row194 col3\" >[0.99964]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row195_col0\" class=\"data row195 col0\" >['eest']</td>\n",
       "      <td id=\"T_2b4ed_row195_col1\" class=\"data row195 col1\" >['▁eest']</td>\n",
       "      <td id=\"T_2b4ed_row195_col2\" class=\"data row195 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row195_col3\" class=\"data row195 col3\" >[0.99969]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row196_col0\" class=\"data row196 col0\" >['on']</td>\n",
       "      <td id=\"T_2b4ed_row196_col1\" class=\"data row196 col1\" >['▁on']</td>\n",
       "      <td id=\"T_2b4ed_row196_col2\" class=\"data row196 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row196_col3\" class=\"data row196 col3\" >[0.99992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row197_col0\" class=\"data row197 col0\" >['ette']</td>\n",
       "      <td id=\"T_2b4ed_row197_col1\" class=\"data row197 col1\" >['▁ette']</td>\n",
       "      <td id=\"T_2b4ed_row197_col2\" class=\"data row197 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row197_col3\" class=\"data row197 col3\" >[0.99987]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row198_col0\" class=\"data row198 col0\" >['nähtud']</td>\n",
       "      <td id=\"T_2b4ed_row198_col1\" class=\"data row198 col1\" >['▁nähtud']</td>\n",
       "      <td id=\"T_2b4ed_row198_col2\" class=\"data row198 col2\" >['tud_V']</td>\n",
       "      <td id=\"T_2b4ed_row198_col3\" class=\"data row198 col3\" >[0.99752]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row199_col0\" class=\"data row199 col0\" >['kriminaalkaristus']</td>\n",
       "      <td id=\"T_2b4ed_row199_col1\" class=\"data row199 col1\" >['▁kriminaal', 'karistus']</td>\n",
       "      <td id=\"T_2b4ed_row199_col2\" class=\"data row199 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row199_col3\" class=\"data row199 col3\" >[0.99957]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row200_col0\" class=\"data row200 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row200_col1\" class=\"data row200 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row200_col2\" class=\"data row200 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row200_col3\" class=\"data row200 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row201_col0\" class=\"data row201 col0\" >['Aga']</td>\n",
       "      <td id=\"T_2b4ed_row201_col1\" class=\"data row201 col1\" >['▁Aga']</td>\n",
       "      <td id=\"T_2b4ed_row201_col2\" class=\"data row201 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row201_col3\" class=\"data row201 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row202_col0\" class=\"data row202 col0\" >['millised']</td>\n",
       "      <td id=\"T_2b4ed_row202_col1\" class=\"data row202 col1\" >['▁millised']</td>\n",
       "      <td id=\"T_2b4ed_row202_col2\" class=\"data row202 col2\" >['pl n_P']</td>\n",
       "      <td id=\"T_2b4ed_row202_col3\" class=\"data row202 col3\" >[0.9998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row203_col0\" class=\"data row203 col0\" >['olid']</td>\n",
       "      <td id=\"T_2b4ed_row203_col1\" class=\"data row203 col1\" >['▁olid']</td>\n",
       "      <td id=\"T_2b4ed_row203_col2\" class=\"data row203 col2\" >['sid_V']</td>\n",
       "      <td id=\"T_2b4ed_row203_col3\" class=\"data row203 col3\" >[0.99982]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row204_col0\" class=\"data row204 col0\" >['selle']</td>\n",
       "      <td id=\"T_2b4ed_row204_col1\" class=\"data row204 col1\" >['▁selle']</td>\n",
       "      <td id=\"T_2b4ed_row204_col2\" class=\"data row204 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row204_col3\" class=\"data row204 col3\" >[0.99971]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row205_col0\" class=\"data row205 col0\" >['sõja']</td>\n",
       "      <td id=\"T_2b4ed_row205_col1\" class=\"data row205 col1\" >['▁sõja']</td>\n",
       "      <td id=\"T_2b4ed_row205_col2\" class=\"data row205 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row205_col3\" class=\"data row205 col3\" >[0.99971]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row206_col0\" class=\"data row206 col0\" >['tulemused']</td>\n",
       "      <td id=\"T_2b4ed_row206_col1\" class=\"data row206 col1\" >['▁tulemused']</td>\n",
       "      <td id=\"T_2b4ed_row206_col2\" class=\"data row206 col2\" >['pl n_S']</td>\n",
       "      <td id=\"T_2b4ed_row206_col3\" class=\"data row206 col3\" >[0.99957]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row207_col0\" class=\"data row207 col0\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row207_col1\" class=\"data row207 col1\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row207_col2\" class=\"data row207 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row207_col3\" class=\"data row207 col3\" >[0.99994]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row208_col0\" class=\"data row208 col0\" >['Tulemus']</td>\n",
       "      <td id=\"T_2b4ed_row208_col1\" class=\"data row208 col1\" >['▁Tulemus']</td>\n",
       "      <td id=\"T_2b4ed_row208_col2\" class=\"data row208 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row208_col3\" class=\"data row208 col3\" >[0.99976]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row209_col0\" class=\"data row209 col0\" >['oli']</td>\n",
       "      <td id=\"T_2b4ed_row209_col1\" class=\"data row209 col1\" >['▁oli']</td>\n",
       "      <td id=\"T_2b4ed_row209_col2\" class=\"data row209 col2\" >['s_V']</td>\n",
       "      <td id=\"T_2b4ed_row209_col3\" class=\"data row209 col3\" >[0.99992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row210_col0\" class=\"data row210 col0\" >['see']</td>\n",
       "      <td id=\"T_2b4ed_row210_col1\" class=\"data row210 col1\" >['▁see']</td>\n",
       "      <td id=\"T_2b4ed_row210_col2\" class=\"data row210 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row210_col3\" class=\"data row210 col3\" >[0.99985]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row211_col0\" class=\"data row211 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row211_col1\" class=\"data row211 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row211_col2\" class=\"data row211 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row211_col3\" class=\"data row211 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row212_col0\" class=\"data row212 col0\" >['et']</td>\n",
       "      <td id=\"T_2b4ed_row212_col1\" class=\"data row212 col1\" >['▁et']</td>\n",
       "      <td id=\"T_2b4ed_row212_col2\" class=\"data row212 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row212_col3\" class=\"data row212 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row213_col0\" class=\"data row213 col0\" >['Nõukogude']</td>\n",
       "      <td id=\"T_2b4ed_row213_col1\" class=\"data row213 col1\" >['▁Nõukogude']</td>\n",
       "      <td id=\"T_2b4ed_row213_col2\" class=\"data row213 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row213_col3\" class=\"data row213 col3\" >[0.78333]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row214_col0\" class=\"data row214 col0\" >['Liit']</td>\n",
       "      <td id=\"T_2b4ed_row214_col1\" class=\"data row214 col1\" >['▁Liit']</td>\n",
       "      <td id=\"T_2b4ed_row214_col2\" class=\"data row214 col2\" >['sg n_H']</td>\n",
       "      <td id=\"T_2b4ed_row214_col3\" class=\"data row214 col3\" >[0.98783]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row215_col0\" class=\"data row215 col0\" >['haaras']</td>\n",
       "      <td id=\"T_2b4ed_row215_col1\" class=\"data row215 col1\" >['▁haaras']</td>\n",
       "      <td id=\"T_2b4ed_row215_col2\" class=\"data row215 col2\" >['s_V']</td>\n",
       "      <td id=\"T_2b4ed_row215_col3\" class=\"data row215 col3\" >[0.99992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row216_col0\" class=\"data row216 col0\" >['enda']</td>\n",
       "      <td id=\"T_2b4ed_row216_col1\" class=\"data row216 col1\" >['▁enda']</td>\n",
       "      <td id=\"T_2b4ed_row216_col2\" class=\"data row216 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row216_col3\" class=\"data row216 col3\" >[0.99957]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row217_col0\" class=\"data row217 col0\" >['alla']</td>\n",
       "      <td id=\"T_2b4ed_row217_col1\" class=\"data row217 col1\" >['▁alla']</td>\n",
       "      <td id=\"T_2b4ed_row217_col2\" class=\"data row217 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row217_col3\" class=\"data row217 col3\" >[0.99967]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row218_col0\" class=\"data row218 col0\" >['pool']</td>\n",
       "      <td id=\"T_2b4ed_row218_col1\" class=\"data row218 col1\" >['▁pool']</td>\n",
       "      <td id=\"T_2b4ed_row218_col2\" class=\"data row218 col2\" >['sg n_N']</td>\n",
       "      <td id=\"T_2b4ed_row218_col3\" class=\"data row218 col3\" >[0.99916]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row219_col0\" class=\"data row219 col0\" >['Euroopat']</td>\n",
       "      <td id=\"T_2b4ed_row219_col1\" class=\"data row219 col1\" >['▁Euroopat']</td>\n",
       "      <td id=\"T_2b4ed_row219_col2\" class=\"data row219 col2\" >['sg p_H']</td>\n",
       "      <td id=\"T_2b4ed_row219_col3\" class=\"data row219 col3\" >[0.97531]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row220_col0\" class=\"data row220 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row220_col1\" class=\"data row220 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row220_col2\" class=\"data row220 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row220_col3\" class=\"data row220 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row221_col0\" class=\"data row221 col0\" >['(']</td>\n",
       "      <td id=\"T_2b4ed_row221_col1\" class=\"data row221 col1\" >['▁(']</td>\n",
       "      <td id=\"T_2b4ed_row221_col2\" class=\"data row221 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row221_col3\" class=\"data row221 col3\" >[0.99995]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row222_col0\" class=\"data row222 col0\" >['Teised']</td>\n",
       "      <td id=\"T_2b4ed_row222_col1\" class=\"data row222 col1\" >['Te', 'ised']</td>\n",
       "      <td id=\"T_2b4ed_row222_col2\" class=\"data row222 col2\" >['pl n_P']</td>\n",
       "      <td id=\"T_2b4ed_row222_col3\" class=\"data row222 col3\" >[0.99978]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row223_col0\" class=\"data row223 col0\" >['võitjariigid']</td>\n",
       "      <td id=\"T_2b4ed_row223_col1\" class=\"data row223 col1\" >['▁võitja', 'riigid']</td>\n",
       "      <td id=\"T_2b4ed_row223_col2\" class=\"data row223 col2\" >['pl n_S']</td>\n",
       "      <td id=\"T_2b4ed_row223_col3\" class=\"data row223 col3\" >[0.99964]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row224_col0\" class=\"data row224 col0\" >['oma']</td>\n",
       "      <td id=\"T_2b4ed_row224_col1\" class=\"data row224 col1\" >['▁oma']</td>\n",
       "      <td id=\"T_2b4ed_row224_col2\" class=\"data row224 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row224_col3\" class=\"data row224 col3\" >[0.99969]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row225_col0\" class=\"data row225 col0\" >['territooriumi']</td>\n",
       "      <td id=\"T_2b4ed_row225_col1\" class=\"data row225 col1\" >['▁territooriumi']</td>\n",
       "      <td id=\"T_2b4ed_row225_col2\" class=\"data row225 col2\" >['sg p_S']</td>\n",
       "      <td id=\"T_2b4ed_row225_col3\" class=\"data row225 col3\" >[0.99716]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row226_col0\" class=\"data row226 col0\" >['ei']</td>\n",
       "      <td id=\"T_2b4ed_row226_col1\" class=\"data row226 col1\" >['▁ei']</td>\n",
       "      <td id=\"T_2b4ed_row226_col2\" class=\"data row226 col2\" >['neg_V']</td>\n",
       "      <td id=\"T_2b4ed_row226_col3\" class=\"data row226 col3\" >[0.99988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row227_col0\" class=\"data row227 col0\" >['laiendanud']</td>\n",
       "      <td id=\"T_2b4ed_row227_col1\" class=\"data row227 col1\" >['▁lai', 'endanud']</td>\n",
       "      <td id=\"T_2b4ed_row227_col2\" class=\"data row227 col2\" >['nud_V']</td>\n",
       "      <td id=\"T_2b4ed_row227_col3\" class=\"data row227 col3\" >[0.99951]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row228_col0\" class=\"data row228 col0\" >['.', ')']</td>\n",
       "      <td id=\"T_2b4ed_row228_col1\" class=\"data row228 col1\" >['.)']</td>\n",
       "      <td id=\"T_2b4ed_row228_col2\" class=\"data row228 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row228_col3\" class=\"data row228 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row229_col0\" class=\"data row229 col0\" >['Nüüd']</td>\n",
       "      <td id=\"T_2b4ed_row229_col1\" class=\"data row229 col1\" >['▁Nüüd']</td>\n",
       "      <td id=\"T_2b4ed_row229_col2\" class=\"data row229 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row229_col3\" class=\"data row229 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row230_col0\" class=\"data row230 col0\" >['aga']</td>\n",
       "      <td id=\"T_2b4ed_row230_col1\" class=\"data row230 col1\" >['▁aga']</td>\n",
       "      <td id=\"T_2b4ed_row230_col2\" class=\"data row230 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row230_col3\" class=\"data row230 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row231_col0\" class=\"data row231 col0\" >['on']</td>\n",
       "      <td id=\"T_2b4ed_row231_col1\" class=\"data row231 col1\" >['▁on']</td>\n",
       "      <td id=\"T_2b4ed_row231_col2\" class=\"data row231 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row231_col3\" class=\"data row231 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row232_col0\" class=\"data row232 col0\" >['tegelik']</td>\n",
       "      <td id=\"T_2b4ed_row232_col1\" class=\"data row232 col1\" >['▁tegelik']</td>\n",
       "      <td id=\"T_2b4ed_row232_col2\" class=\"data row232 col2\" >['sg n_A']</td>\n",
       "      <td id=\"T_2b4ed_row232_col3\" class=\"data row232 col3\" >[0.99974]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row233_col0\" class=\"data row233 col0\" >['elu']</td>\n",
       "      <td id=\"T_2b4ed_row233_col1\" class=\"data row233 col1\" >['▁elu']</td>\n",
       "      <td id=\"T_2b4ed_row233_col2\" class=\"data row233 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row233_col3\" class=\"data row233 col3\" >[0.99973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row234_col0\" class=\"data row234 col0\" >['ja']</td>\n",
       "      <td id=\"T_2b4ed_row234_col1\" class=\"data row234 col1\" >['▁ja']</td>\n",
       "      <td id=\"T_2b4ed_row234_col2\" class=\"data row234 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row234_col3\" class=\"data row234 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row235_col0\" class=\"data row235 col0\" >['ajaloo']</td>\n",
       "      <td id=\"T_2b4ed_row235_col1\" class=\"data row235 col1\" >['▁ajaloo']</td>\n",
       "      <td id=\"T_2b4ed_row235_col2\" class=\"data row235 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row235_col3\" class=\"data row235 col3\" >[0.99969]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row236_col0\" class=\"data row236 col0\" >['kulg']</td>\n",
       "      <td id=\"T_2b4ed_row236_col1\" class=\"data row236 col1\" >['▁kul', 'g']</td>\n",
       "      <td id=\"T_2b4ed_row236_col2\" class=\"data row236 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row236_col3\" class=\"data row236 col3\" >[0.99979]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row237_col0\" class=\"data row237 col0\" >['ise']</td>\n",
       "      <td id=\"T_2b4ed_row237_col1\" class=\"data row237 col1\" >['▁ise']</td>\n",
       "      <td id=\"T_2b4ed_row237_col2\" class=\"data row237 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row237_col3\" class=\"data row237 col3\" >[0.9993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row238_col0\" class=\"data row238 col0\" >['revideerinud']</td>\n",
       "      <td id=\"T_2b4ed_row238_col1\" class=\"data row238 col1\" >['▁revid', 'eerinud']</td>\n",
       "      <td id=\"T_2b4ed_row238_col2\" class=\"data row238 col2\" >['nud_V']</td>\n",
       "      <td id=\"T_2b4ed_row238_col3\" class=\"data row238 col3\" >[0.99975]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row239_col0\" class=\"data row239 col0\" >['neid']</td>\n",
       "      <td id=\"T_2b4ed_row239_col1\" class=\"data row239 col1\" >['▁neid']</td>\n",
       "      <td id=\"T_2b4ed_row239_col2\" class=\"data row239 col2\" >['pl p_P']</td>\n",
       "      <td id=\"T_2b4ed_row239_col3\" class=\"data row239 col3\" >[0.99976]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row240_col0\" class=\"data row240 col0\" >['tulemusi']</td>\n",
       "      <td id=\"T_2b4ed_row240_col1\" class=\"data row240 col1\" >['▁tulemusi']</td>\n",
       "      <td id=\"T_2b4ed_row240_col2\" class=\"data row240 col2\" >['pl p_S']</td>\n",
       "      <td id=\"T_2b4ed_row240_col3\" class=\"data row240 col3\" >[0.99962]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row241_col0\" class=\"data row241 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row241_col1\" class=\"data row241 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row241_col2\" class=\"data row241 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row241_col3\" class=\"data row241 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row242_col0\" class=\"data row242 col0\" >['Stalini']</td>\n",
       "      <td id=\"T_2b4ed_row242_col1\" class=\"data row242 col1\" >['▁Stalini']</td>\n",
       "      <td id=\"T_2b4ed_row242_col2\" class=\"data row242 col2\" >['sg g_H']</td>\n",
       "      <td id=\"T_2b4ed_row242_col3\" class=\"data row242 col3\" >[0.99906]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row243_col0\" class=\"data row243 col0\" >['vallutused']</td>\n",
       "      <td id=\"T_2b4ed_row243_col1\" class=\"data row243 col1\" >['▁vall', 'utused']</td>\n",
       "      <td id=\"T_2b4ed_row243_col2\" class=\"data row243 col2\" >['pl n_S']</td>\n",
       "      <td id=\"T_2b4ed_row243_col3\" class=\"data row243 col3\" >[0.99962]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row244_col0\" class=\"data row244 col0\" >['on']</td>\n",
       "      <td id=\"T_2b4ed_row244_col1\" class=\"data row244 col1\" >['▁on']</td>\n",
       "      <td id=\"T_2b4ed_row244_col2\" class=\"data row244 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row244_col3\" class=\"data row244 col3\" >[0.99992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row245_col0\" class=\"data row245 col0\" >['venelastele']</td>\n",
       "      <td id=\"T_2b4ed_row245_col1\" class=\"data row245 col1\" >['▁venelastele']</td>\n",
       "      <td id=\"T_2b4ed_row245_col2\" class=\"data row245 col2\" >['pl all_S']</td>\n",
       "      <td id=\"T_2b4ed_row245_col3\" class=\"data row245 col3\" >[0.99907]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row246_col0\" class=\"data row246 col0\" >['suuremalt']</td>\n",
       "      <td id=\"T_2b4ed_row246_col1\" class=\"data row246 col1\" >['▁suuremalt']</td>\n",
       "      <td id=\"T_2b4ed_row246_col2\" class=\"data row246 col2\" >['sg abl_A']</td>\n",
       "      <td id=\"T_2b4ed_row246_col3\" class=\"data row246 col3\" >[0.1976]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row247_col0\" class=\"data row247 col0\" >['jaolt']</td>\n",
       "      <td id=\"T_2b4ed_row247_col1\" class=\"data row247 col1\" >['▁jaolt']</td>\n",
       "      <td id=\"T_2b4ed_row247_col2\" class=\"data row247 col2\" >['sg abl_S']</td>\n",
       "      <td id=\"T_2b4ed_row247_col3\" class=\"data row247 col3\" >[0.99423]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row248_col0\" class=\"data row248 col0\" >['kaduma']</td>\n",
       "      <td id=\"T_2b4ed_row248_col1\" class=\"data row248 col1\" >['▁kaduma']</td>\n",
       "      <td id=\"T_2b4ed_row248_col2\" class=\"data row248 col2\" >['ma_V']</td>\n",
       "      <td id=\"T_2b4ed_row248_col3\" class=\"data row248 col3\" >[0.99923]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row249_col0\" class=\"data row249 col0\" >['läinud']</td>\n",
       "      <td id=\"T_2b4ed_row249_col1\" class=\"data row249 col1\" >['▁läinud']</td>\n",
       "      <td id=\"T_2b4ed_row249_col2\" class=\"data row249 col2\" >['nud_V']</td>\n",
       "      <td id=\"T_2b4ed_row249_col3\" class=\"data row249 col3\" >[0.99973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row250_col0\" class=\"data row250 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row250_col1\" class=\"data row250 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row250_col2\" class=\"data row250 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row250_col3\" class=\"data row250 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row251_col0\" class=\"data row251 col0\" >['See']</td>\n",
       "      <td id=\"T_2b4ed_row251_col1\" class=\"data row251 col1\" >['▁See']</td>\n",
       "      <td id=\"T_2b4ed_row251_col2\" class=\"data row251 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row251_col3\" class=\"data row251 col3\" >[0.99985]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row252_col0\" class=\"data row252 col0\" >['on']</td>\n",
       "      <td id=\"T_2b4ed_row252_col1\" class=\"data row252 col1\" >['▁on']</td>\n",
       "      <td id=\"T_2b4ed_row252_col2\" class=\"data row252 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row252_col3\" class=\"data row252 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row253_col0\" class=\"data row253 col0\" >['asjade']</td>\n",
       "      <td id=\"T_2b4ed_row253_col1\" class=\"data row253 col1\" >['▁asjade']</td>\n",
       "      <td id=\"T_2b4ed_row253_col2\" class=\"data row253 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row253_col3\" class=\"data row253 col3\" >[0.99962]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row254_col0\" class=\"data row254 col0\" >['tegelik']</td>\n",
       "      <td id=\"T_2b4ed_row254_col1\" class=\"data row254 col1\" >['▁tegelik']</td>\n",
       "      <td id=\"T_2b4ed_row254_col2\" class=\"data row254 col2\" >['sg n_A']</td>\n",
       "      <td id=\"T_2b4ed_row254_col3\" class=\"data row254 col3\" >[0.99976]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row255_col0\" class=\"data row255 col0\" >['areng']</td>\n",
       "      <td id=\"T_2b4ed_row255_col1\" class=\"data row255 col1\" >['▁areng']</td>\n",
       "      <td id=\"T_2b4ed_row255_col2\" class=\"data row255 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row255_col3\" class=\"data row255 col3\" >[0.99975]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row256_col0\" class=\"data row256 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row256_col1\" class=\"data row256 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row256_col2\" class=\"data row256 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row256_col3\" class=\"data row256 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row257_col0\" class=\"data row257 col0\" >['Uue']</td>\n",
       "      <td id=\"T_2b4ed_row257_col1\" class=\"data row257 col1\" >['▁Uue']</td>\n",
       "      <td id=\"T_2b4ed_row257_col2\" class=\"data row257 col2\" >['sg g_A']</td>\n",
       "      <td id=\"T_2b4ed_row257_col3\" class=\"data row257 col3\" >[0.99958]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row258_col0\" class=\"data row258 col0\" >['seaduse']</td>\n",
       "      <td id=\"T_2b4ed_row258_col1\" class=\"data row258 col1\" >['▁seaduse']</td>\n",
       "      <td id=\"T_2b4ed_row258_col2\" class=\"data row258 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row258_col3\" class=\"data row258 col3\" >[0.99973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row259_col0\" class=\"data row259 col0\" >['paatos']</td>\n",
       "      <td id=\"T_2b4ed_row259_col1\" class=\"data row259 col1\" >['▁paat', 'os']</td>\n",
       "      <td id=\"T_2b4ed_row259_col2\" class=\"data row259 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row259_col3\" class=\"data row259 col3\" >[0.99915]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row260_col0\" class=\"data row260 col0\" >['on']</td>\n",
       "      <td id=\"T_2b4ed_row260_col1\" class=\"data row260 col1\" >['▁on']</td>\n",
       "      <td id=\"T_2b4ed_row260_col2\" class=\"data row260 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row260_col3\" class=\"data row260 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row261_col0\" class=\"data row261 col0\" >['aga']</td>\n",
       "      <td id=\"T_2b4ed_row261_col1\" class=\"data row261 col1\" >['▁aga']</td>\n",
       "      <td id=\"T_2b4ed_row261_col2\" class=\"data row261 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row261_col3\" class=\"data row261 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row262_col0\" class=\"data row262 col0\" >['suunatud']</td>\n",
       "      <td id=\"T_2b4ed_row262_col1\" class=\"data row262 col1\" >['▁suunatud']</td>\n",
       "      <td id=\"T_2b4ed_row262_col2\" class=\"data row262 col2\" >['A']</td>\n",
       "      <td id=\"T_2b4ed_row262_col3\" class=\"data row262 col3\" >[0.99824]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row263_col0\" class=\"data row263 col0\" >['selle']</td>\n",
       "      <td id=\"T_2b4ed_row263_col1\" class=\"data row263 col1\" >['▁selle']</td>\n",
       "      <td id=\"T_2b4ed_row263_col2\" class=\"data row263 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row263_col3\" class=\"data row263 col3\" >[0.9997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row264_col0\" class=\"data row264 col0\" >['arengu']</td>\n",
       "      <td id=\"T_2b4ed_row264_col1\" class=\"data row264 col1\" >['▁arengu']</td>\n",
       "      <td id=\"T_2b4ed_row264_col2\" class=\"data row264 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row264_col3\" class=\"data row264 col3\" >[0.99972]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row265_col0\" class=\"data row265 col0\" >['vastu']</td>\n",
       "      <td id=\"T_2b4ed_row265_col1\" class=\"data row265 col1\" >['▁vastu']</td>\n",
       "      <td id=\"T_2b4ed_row265_col2\" class=\"data row265 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row265_col3\" class=\"data row265 col3\" >[0.99965]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row266_col0\" class=\"data row266 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row266_col1\" class=\"data row266 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row266_col2\" class=\"data row266 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row266_col3\" class=\"data row266 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row267_col0\" class=\"data row267 col0\" >['sisuliselt']</td>\n",
       "      <td id=\"T_2b4ed_row267_col1\" class=\"data row267 col1\" >['▁sisuliselt']</td>\n",
       "      <td id=\"T_2b4ed_row267_col2\" class=\"data row267 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row267_col3\" class=\"data row267 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row268_col0\" class=\"data row268 col0\" >['eitades']</td>\n",
       "      <td id=\"T_2b4ed_row268_col1\" class=\"data row268 col1\" >['▁ei', 'tades']</td>\n",
       "      <td id=\"T_2b4ed_row268_col2\" class=\"data row268 col2\" >['des_V']</td>\n",
       "      <td id=\"T_2b4ed_row268_col3\" class=\"data row268 col3\" >[0.99931]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row269_col0\" class=\"data row269 col0\" >['nende']</td>\n",
       "      <td id=\"T_2b4ed_row269_col1\" class=\"data row269 col1\" >['▁nende']</td>\n",
       "      <td id=\"T_2b4ed_row269_col2\" class=\"data row269 col2\" >['pl g_P']</td>\n",
       "      <td id=\"T_2b4ed_row269_col3\" class=\"data row269 col3\" >[0.99941]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row270_col0\" class=\"data row270 col0\" >['alade']</td>\n",
       "      <td id=\"T_2b4ed_row270_col1\" class=\"data row270 col1\" >['▁alade']</td>\n",
       "      <td id=\"T_2b4ed_row270_col2\" class=\"data row270 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row270_col3\" class=\"data row270 col3\" >[0.99958]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row271_col0\" class=\"data row271 col0\" >['vabanemist']</td>\n",
       "      <td id=\"T_2b4ed_row271_col1\" class=\"data row271 col1\" >['▁vabanemist']</td>\n",
       "      <td id=\"T_2b4ed_row271_col2\" class=\"data row271 col2\" >['sg p_S']</td>\n",
       "      <td id=\"T_2b4ed_row271_col3\" class=\"data row271 col3\" >[0.99966]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row272_col0\" class=\"data row272 col0\" >['ja']</td>\n",
       "      <td id=\"T_2b4ed_row272_col1\" class=\"data row272 col1\" >['▁ja']</td>\n",
       "      <td id=\"T_2b4ed_row272_col2\" class=\"data row272 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row272_col3\" class=\"data row272 col3\" >[0.99992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row273_col0\" class=\"data row273 col0\" >['kihutades']</td>\n",
       "      <td id=\"T_2b4ed_row273_col1\" class=\"data row273 col1\" >['▁kih', 'utades']</td>\n",
       "      <td id=\"T_2b4ed_row273_col2\" class=\"data row273 col2\" >['des_V']</td>\n",
       "      <td id=\"T_2b4ed_row273_col3\" class=\"data row273 col3\" >[0.99934]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row274_col0\" class=\"data row274 col0\" >['uuele']</td>\n",
       "      <td id=\"T_2b4ed_row274_col1\" class=\"data row274 col1\" >['▁uuele']</td>\n",
       "      <td id=\"T_2b4ed_row274_col2\" class=\"data row274 col2\" >['sg all_A']</td>\n",
       "      <td id=\"T_2b4ed_row274_col3\" class=\"data row274 col3\" >[0.99792]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row275_col0\" class=\"data row275 col0\" >['ekspansioonile']</td>\n",
       "      <td id=\"T_2b4ed_row275_col1\" class=\"data row275 col1\" >['▁eks', 'pan', 'sioonile']</td>\n",
       "      <td id=\"T_2b4ed_row275_col2\" class=\"data row275 col2\" >['sg all_S']</td>\n",
       "      <td id=\"T_2b4ed_row275_col3\" class=\"data row275 col3\" >[0.99944]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row276_col0\" class=\"data row276 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row276_col1\" class=\"data row276 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row276_col2\" class=\"data row276 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row276_col3\" class=\"data row276 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row277_col0\" class=\"data row277 col0\" >['enda']</td>\n",
       "      <td id=\"T_2b4ed_row277_col1\" class=\"data row277 col1\" >['▁enda']</td>\n",
       "      <td id=\"T_2b4ed_row277_col2\" class=\"data row277 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row277_col3\" class=\"data row277 col3\" >[0.99967]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row278_col0\" class=\"data row278 col0\" >['alla']</td>\n",
       "      <td id=\"T_2b4ed_row278_col1\" class=\"data row278 col1\" >['▁alla']</td>\n",
       "      <td id=\"T_2b4ed_row278_col2\" class=\"data row278 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row278_col3\" class=\"data row278 col3\" >[0.99967]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row279_col0\" class=\"data row279 col0\" >['heitma']</td>\n",
       "      <td id=\"T_2b4ed_row279_col1\" class=\"data row279 col1\" >['▁heit', 'ma']</td>\n",
       "      <td id=\"T_2b4ed_row279_col2\" class=\"data row279 col2\" >['ma_V']</td>\n",
       "      <td id=\"T_2b4ed_row279_col3\" class=\"data row279 col3\" >[0.99859]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row280_col0\" class=\"data row280 col0\" >['neid']</td>\n",
       "      <td id=\"T_2b4ed_row280_col1\" class=\"data row280 col1\" >['▁neid']</td>\n",
       "      <td id=\"T_2b4ed_row280_col2\" class=\"data row280 col2\" >['pl p_P']</td>\n",
       "      <td id=\"T_2b4ed_row280_col3\" class=\"data row280 col3\" >[0.99978]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row281_col0\" class=\"data row281 col0\" >['territooriume']</td>\n",
       "      <td id=\"T_2b4ed_row281_col1\" class=\"data row281 col1\" >['▁territoori', 'ume']</td>\n",
       "      <td id=\"T_2b4ed_row281_col2\" class=\"data row281 col2\" >['pl p_S']</td>\n",
       "      <td id=\"T_2b4ed_row281_col3\" class=\"data row281 col3\" >[0.99965]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row282_col0\" class=\"data row282 col0\" >['ning']</td>\n",
       "      <td id=\"T_2b4ed_row282_col1\" class=\"data row282 col1\" >['▁ning']</td>\n",
       "      <td id=\"T_2b4ed_row282_col2\" class=\"data row282 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row282_col3\" class=\"data row282 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row283_col0\" class=\"data row283 col0\" >['saavutama']</td>\n",
       "      <td id=\"T_2b4ed_row283_col1\" class=\"data row283 col1\" >['▁saav', 'utama']</td>\n",
       "      <td id=\"T_2b4ed_row283_col2\" class=\"data row283 col2\" >['ma_V']</td>\n",
       "      <td id=\"T_2b4ed_row283_col3\" class=\"data row283 col3\" >[0.9973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row284_col0\" class=\"data row284 col0\" >['kontrolli']</td>\n",
       "      <td id=\"T_2b4ed_row284_col1\" class=\"data row284 col1\" >['▁kontrolli']</td>\n",
       "      <td id=\"T_2b4ed_row284_col2\" class=\"data row284 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row284_col3\" class=\"data row284 col3\" >[0.94327]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row285_col0\" class=\"data row285 col0\" >['seal']</td>\n",
       "      <td id=\"T_2b4ed_row285_col1\" class=\"data row285 col1\" >['▁seal']</td>\n",
       "      <td id=\"T_2b4ed_row285_col2\" class=\"data row285 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row285_col3\" class=\"data row285 col3\" >[0.99988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row286_col0\" class=\"data row286 col0\" >['elavate']</td>\n",
       "      <td id=\"T_2b4ed_row286_col1\" class=\"data row286 col1\" >['▁elavate']</td>\n",
       "      <td id=\"T_2b4ed_row286_col2\" class=\"data row286 col2\" >['pl g_A']</td>\n",
       "      <td id=\"T_2b4ed_row286_col3\" class=\"data row286 col3\" >[0.99883]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row287_col0\" class=\"data row287 col0\" >['inimeste']</td>\n",
       "      <td id=\"T_2b4ed_row287_col1\" class=\"data row287 col1\" >['▁inimeste']</td>\n",
       "      <td id=\"T_2b4ed_row287_col2\" class=\"data row287 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row287_col3\" class=\"data row287 col3\" >[0.9996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row288_col0\" class=\"data row288 col0\" >['üle']</td>\n",
       "      <td id=\"T_2b4ed_row288_col1\" class=\"data row288 col1\" >['▁üle']</td>\n",
       "      <td id=\"T_2b4ed_row288_col2\" class=\"data row288 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row288_col3\" class=\"data row288 col3\" >[0.9997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row289_col0\" class=\"data row289 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row289_col1\" class=\"data row289 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row289_col2\" class=\"data row289 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row289_col3\" class=\"data row289 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row290_col0\" class=\"data row290 col0\" >['Mina']</td>\n",
       "      <td id=\"T_2b4ed_row290_col1\" class=\"data row290 col1\" >['▁Mina']</td>\n",
       "      <td id=\"T_2b4ed_row290_col2\" class=\"data row290 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row290_col3\" class=\"data row290 col3\" >[0.99985]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row291_col0\" class=\"data row291 col0\" >['nimetaksin']</td>\n",
       "      <td id=\"T_2b4ed_row291_col1\" class=\"data row291 col1\" >['▁nime', 'taksin']</td>\n",
       "      <td id=\"T_2b4ed_row291_col2\" class=\"data row291 col2\" >['ksin_V']</td>\n",
       "      <td id=\"T_2b4ed_row291_col3\" class=\"data row291 col3\" >[0.9941]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row292_col0\" class=\"data row292 col0\" >['seda']</td>\n",
       "      <td id=\"T_2b4ed_row292_col1\" class=\"data row292 col1\" >['▁seda']</td>\n",
       "      <td id=\"T_2b4ed_row292_col2\" class=\"data row292 col2\" >['sg p_P']</td>\n",
       "      <td id=\"T_2b4ed_row292_col3\" class=\"data row292 col3\" >[0.99982]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row293_col0\" class=\"data row293 col0\" >['äärmiselt']</td>\n",
       "      <td id=\"T_2b4ed_row293_col1\" class=\"data row293 col1\" >['▁äärmiselt']</td>\n",
       "      <td id=\"T_2b4ed_row293_col2\" class=\"data row293 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row293_col3\" class=\"data row293 col3\" >[0.99987]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row294_col0\" class=\"data row294 col0\" >['kurjakuulutavaks']</td>\n",
       "      <td id=\"T_2b4ed_row294_col1\" class=\"data row294 col1\" >['▁kurja', 'kuul', 'utavaks']</td>\n",
       "      <td id=\"T_2b4ed_row294_col2\" class=\"data row294 col2\" >['sg tr_A']</td>\n",
       "      <td id=\"T_2b4ed_row294_col3\" class=\"data row294 col3\" >[0.99904]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row295_col0\" class=\"data row295 col0\" >['arenguks']</td>\n",
       "      <td id=\"T_2b4ed_row295_col1\" class=\"data row295 col1\" >['▁arenguks']</td>\n",
       "      <td id=\"T_2b4ed_row295_col2\" class=\"data row295 col2\" >['sg tr_S']</td>\n",
       "      <td id=\"T_2b4ed_row295_col3\" class=\"data row295 col3\" >[0.99906]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row296_col0\" class=\"data row296 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row296_col1\" class=\"data row296 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row296_col2\" class=\"data row296 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row296_col3\" class=\"data row296 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row297_col0\" class=\"data row297 col0\" >['Tuvastamata']</td>\n",
       "      <td id=\"T_2b4ed_row297_col1\" class=\"data row297 col1\" >['▁Tu', 'vasta', 'mata']</td>\n",
       "      <td id=\"T_2b4ed_row297_col2\" class=\"data row297 col2\" >['A']</td>\n",
       "      <td id=\"T_2b4ed_row297_col3\" class=\"data row297 col3\" >[0.95848]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row298_col0\" class=\"data row298 col0\" >['Kasutaja']</td>\n",
       "      <td id=\"T_2b4ed_row298_col1\" class=\"data row298 col1\" >['▁Kasutaja']</td>\n",
       "      <td id=\"T_2b4ed_row298_col2\" class=\"data row298 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row298_col3\" class=\"data row298 col3\" >[0.99825]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row299_col0\" class=\"data row299 col0\" >['Ega']</td>\n",
       "      <td id=\"T_2b4ed_row299_col1\" class=\"data row299 col1\" >['▁Ega']</td>\n",
       "      <td id=\"T_2b4ed_row299_col2\" class=\"data row299 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row299_col3\" class=\"data row299 col3\" >[0.95002]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row300_col0\" class=\"data row300 col0\" >['selle']</td>\n",
       "      <td id=\"T_2b4ed_row300_col1\" class=\"data row300 col1\" >['▁selle']</td>\n",
       "      <td id=\"T_2b4ed_row300_col2\" class=\"data row300 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row300_col3\" class=\"data row300 col3\" >[0.99969]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row301_col0\" class=\"data row301 col0\" >['väikese']</td>\n",
       "      <td id=\"T_2b4ed_row301_col1\" class=\"data row301 col1\" >['▁väikese']</td>\n",
       "      <td id=\"T_2b4ed_row301_col2\" class=\"data row301 col2\" >['sg g_A']</td>\n",
       "      <td id=\"T_2b4ed_row301_col3\" class=\"data row301 col3\" >[0.99962]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row302_col0\" class=\"data row302 col0\" >['asja']</td>\n",
       "      <td id=\"T_2b4ed_row302_col1\" class=\"data row302 col1\" >['▁asja']</td>\n",
       "      <td id=\"T_2b4ed_row302_col2\" class=\"data row302 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row302_col3\" class=\"data row302 col3\" >[0.99969]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row303_col0\" class=\"data row303 col0\" >['taha']</td>\n",
       "      <td id=\"T_2b4ed_row303_col1\" class=\"data row303 col1\" >['▁taha']</td>\n",
       "      <td id=\"T_2b4ed_row303_col2\" class=\"data row303 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row303_col3\" class=\"data row303 col3\" >[0.9996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row304_col0\" class=\"data row304 col0\" >['tohi']</td>\n",
       "      <td id=\"T_2b4ed_row304_col1\" class=\"data row304 col1\" >['▁tohi']</td>\n",
       "      <td id=\"T_2b4ed_row304_col2\" class=\"data row304 col2\" >['o_V']</td>\n",
       "      <td id=\"T_2b4ed_row304_col3\" class=\"data row304 col3\" >[0.99913]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row305_col0\" class=\"data row305 col0\" >['ometi']</td>\n",
       "      <td id=\"T_2b4ed_row305_col1\" class=\"data row305 col1\" >['▁ometi']</td>\n",
       "      <td id=\"T_2b4ed_row305_col2\" class=\"data row305 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row305_col3\" class=\"data row305 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row306_col0\" class=\"data row306 col0\" >['sõit']</td>\n",
       "      <td id=\"T_2b4ed_row306_col1\" class=\"data row306 col1\" >['▁sõit']</td>\n",
       "      <td id=\"T_2b4ed_row306_col2\" class=\"data row306 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row306_col3\" class=\"data row306 col3\" >[0.9998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row307_col0\" class=\"data row307 col0\" >['seisma']</td>\n",
       "      <td id=\"T_2b4ed_row307_col1\" class=\"data row307 col1\" >['▁seisma']</td>\n",
       "      <td id=\"T_2b4ed_row307_col2\" class=\"data row307 col2\" >['ma_V']</td>\n",
       "      <td id=\"T_2b4ed_row307_col3\" class=\"data row307 col3\" >[0.99935]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row308_col0\" class=\"data row308 col0\" >['jääda']</td>\n",
       "      <td id=\"T_2b4ed_row308_col1\" class=\"data row308 col1\" >['▁jääda']</td>\n",
       "      <td id=\"T_2b4ed_row308_col2\" class=\"data row308 col2\" >['da_V']</td>\n",
       "      <td id=\"T_2b4ed_row308_col3\" class=\"data row308 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row309_col0\" class=\"data row309 col0\" >['!']</td>\n",
       "      <td id=\"T_2b4ed_row309_col1\" class=\"data row309 col1\" >['!']</td>\n",
       "      <td id=\"T_2b4ed_row309_col2\" class=\"data row309 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row309_col3\" class=\"data row309 col3\" >[0.99995]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row310_col0\" class=\"data row310 col0\" >['Siis']</td>\n",
       "      <td id=\"T_2b4ed_row310_col1\" class=\"data row310 col1\" >['▁Siis']</td>\n",
       "      <td id=\"T_2b4ed_row310_col2\" class=\"data row310 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row310_col3\" class=\"data row310 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row311_col0\" class=\"data row311 col0\" >['saaksid']</td>\n",
       "      <td id=\"T_2b4ed_row311_col1\" class=\"data row311 col1\" >['▁saaksid']</td>\n",
       "      <td id=\"T_2b4ed_row311_col2\" class=\"data row311 col2\" >['ksid_V']</td>\n",
       "      <td id=\"T_2b4ed_row311_col3\" class=\"data row311 col3\" >[0.99875]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row312_col0\" class=\"data row312 col0\" >['kremli']</td>\n",
       "      <td id=\"T_2b4ed_row312_col1\" class=\"data row312 col1\" >['▁kremli']</td>\n",
       "      <td id=\"T_2b4ed_row312_col2\" class=\"data row312 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row312_col3\" class=\"data row312 col3\" >[0.96061]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row313_col0\" class=\"data row313 col0\" >['klähvitsad']</td>\n",
       "      <td id=\"T_2b4ed_row313_col1\" class=\"data row313 col1\" >['▁kl', 'ä', 'hv', 'itsad']</td>\n",
       "      <td id=\"T_2b4ed_row313_col2\" class=\"data row313 col2\" >['pl n_S']</td>\n",
       "      <td id=\"T_2b4ed_row313_col3\" class=\"data row313 col3\" >[0.99965]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row314_col0\" class=\"data row314 col0\" >['oma']</td>\n",
       "      <td id=\"T_2b4ed_row314_col1\" class=\"data row314 col1\" >['▁oma']</td>\n",
       "      <td id=\"T_2b4ed_row314_col2\" class=\"data row314 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row314_col3\" class=\"data row314 col3\" >[0.9996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row315_col0\" class=\"data row315 col0\" >['tahtmise']</td>\n",
       "      <td id=\"T_2b4ed_row315_col1\" class=\"data row315 col1\" >['▁tahtmise']</td>\n",
       "      <td id=\"T_2b4ed_row315_col2\" class=\"data row315 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row315_col3\" class=\"data row315 col3\" >[0.99962]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row316_col0\" class=\"data row316 col0\" >['küll']</td>\n",
       "      <td id=\"T_2b4ed_row316_col1\" class=\"data row316 col1\" >['▁küll']</td>\n",
       "      <td id=\"T_2b4ed_row316_col2\" class=\"data row316 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row316_col3\" class=\"data row316 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row317_col0\" class=\"data row317 col0\" >['liiga']</td>\n",
       "      <td id=\"T_2b4ed_row317_col1\" class=\"data row317 col1\" >['▁liiga']</td>\n",
       "      <td id=\"T_2b4ed_row317_col2\" class=\"data row317 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row317_col3\" class=\"data row317 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row318_col0\" class=\"data row318 col0\" >['kergelt']</td>\n",
       "      <td id=\"T_2b4ed_row318_col1\" class=\"data row318 col1\" >['▁kergelt']</td>\n",
       "      <td id=\"T_2b4ed_row318_col2\" class=\"data row318 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row318_col3\" class=\"data row318 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row319_col0\" class=\"data row319 col0\" >['kätte']</td>\n",
       "      <td id=\"T_2b4ed_row319_col1\" class=\"data row319 col1\" >['▁kätte']</td>\n",
       "      <td id=\"T_2b4ed_row319_col2\" class=\"data row319 col2\" >['adt_S']</td>\n",
       "      <td id=\"T_2b4ed_row319_col3\" class=\"data row319 col3\" >[0.99885]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row320_col0\" class=\"data row320 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row320_col1\" class=\"data row320 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row320_col2\" class=\"data row320 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row320_col3\" class=\"data row320 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row321_col0\" class=\"data row321 col0\" >['kui']</td>\n",
       "      <td id=\"T_2b4ed_row321_col1\" class=\"data row321 col1\" >['▁kui']</td>\n",
       "      <td id=\"T_2b4ed_row321_col2\" class=\"data row321 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row321_col3\" class=\"data row321 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row322_col0\" class=\"data row322 col0\" >['ma']</td>\n",
       "      <td id=\"T_2b4ed_row322_col1\" class=\"data row322 col1\" >['▁ma']</td>\n",
       "      <td id=\"T_2b4ed_row322_col2\" class=\"data row322 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row322_col3\" class=\"data row322 col3\" >[0.99983]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row323_col0\" class=\"data row323 col0\" >['neile']</td>\n",
       "      <td id=\"T_2b4ed_row323_col1\" class=\"data row323 col1\" >['▁neile']</td>\n",
       "      <td id=\"T_2b4ed_row323_col2\" class=\"data row323 col2\" >['pl all_P']</td>\n",
       "      <td id=\"T_2b4ed_row323_col3\" class=\"data row323 col3\" >[0.99909]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row324_col0\" class=\"data row324 col0\" >['tähelepanu']</td>\n",
       "      <td id=\"T_2b4ed_row324_col1\" class=\"data row324 col1\" >['▁tähelepanu']</td>\n",
       "      <td id=\"T_2b4ed_row324_col2\" class=\"data row324 col2\" >['sg p_S']</td>\n",
       "      <td id=\"T_2b4ed_row324_col3\" class=\"data row324 col3\" >[0.99949]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row325_col0\" class=\"data row325 col0\" >['pööraksin']</td>\n",
       "      <td id=\"T_2b4ed_row325_col1\" class=\"data row325 col1\" >['▁pööra', 'ksin']</td>\n",
       "      <td id=\"T_2b4ed_row325_col2\" class=\"data row325 col2\" >['ksin_V']</td>\n",
       "      <td id=\"T_2b4ed_row325_col3\" class=\"data row325 col3\" >[0.99462]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row326_col0\" class=\"data row326 col0\" >['ja']</td>\n",
       "      <td id=\"T_2b4ed_row326_col1\" class=\"data row326 col1\" >['▁ja']</td>\n",
       "      <td id=\"T_2b4ed_row326_col2\" class=\"data row326 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row326_col3\" class=\"data row326 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row327_col0\" class=\"data row327 col0\" >['püssi']</td>\n",
       "      <td id=\"T_2b4ed_row327_col1\" class=\"data row327 col1\" >['▁püssi']</td>\n",
       "      <td id=\"T_2b4ed_row327_col2\" class=\"data row327 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row327_col3\" class=\"data row327 col3\" >[0.99963]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row328_col0\" class=\"data row328 col0\" >['põõsasse']</td>\n",
       "      <td id=\"T_2b4ed_row328_col1\" class=\"data row328 col1\" >['▁põõ', 'sasse']</td>\n",
       "      <td id=\"T_2b4ed_row328_col2\" class=\"data row328 col2\" >['sg ill_S']</td>\n",
       "      <td id=\"T_2b4ed_row328_col3\" class=\"data row328 col3\" >[0.99664]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row329_col0\" class=\"data row329 col0\" >['viskaksin']</td>\n",
       "      <td id=\"T_2b4ed_row329_col1\" class=\"data row329 col1\" >['▁vis', 'kaks', 'in']</td>\n",
       "      <td id=\"T_2b4ed_row329_col2\" class=\"data row329 col2\" >['ksin_V']</td>\n",
       "      <td id=\"T_2b4ed_row329_col3\" class=\"data row329 col3\" >[0.994]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row330_col0\" class=\"data row330 col0\" >['!']</td>\n",
       "      <td id=\"T_2b4ed_row330_col1\" class=\"data row330 col1\" >['!']</td>\n",
       "      <td id=\"T_2b4ed_row330_col2\" class=\"data row330 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row330_col3\" class=\"data row330 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row331_col0\" class=\"data row331 col0\" >['Mõistus']</td>\n",
       "      <td id=\"T_2b4ed_row331_col1\" class=\"data row331 col1\" >['▁Mõ', 'istus']</td>\n",
       "      <td id=\"T_2b4ed_row331_col2\" class=\"data row331 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row331_col3\" class=\"data row331 col3\" >[0.9997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row332_col0\" class=\"data row332 col0\" >['tuleb']</td>\n",
       "      <td id=\"T_2b4ed_row332_col1\" class=\"data row332 col1\" >['▁tuleb']</td>\n",
       "      <td id=\"T_2b4ed_row332_col2\" class=\"data row332 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row332_col3\" class=\"data row332 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row333_col0\" class=\"data row333 col0\" >['ikka']</td>\n",
       "      <td id=\"T_2b4ed_row333_col1\" class=\"data row333 col1\" >['▁ikka']</td>\n",
       "      <td id=\"T_2b4ed_row333_col2\" class=\"data row333 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row333_col3\" class=\"data row333 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row334_col0\" class=\"data row334 col0\" >['selge']</td>\n",
       "      <td id=\"T_2b4ed_row334_col1\" class=\"data row334 col1\" >['▁selge']</td>\n",
       "      <td id=\"T_2b4ed_row334_col2\" class=\"data row334 col2\" >['sg n_A']</td>\n",
       "      <td id=\"T_2b4ed_row334_col3\" class=\"data row334 col3\" >[0.99977]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row335_col0\" class=\"data row335 col0\" >['hoida']</td>\n",
       "      <td id=\"T_2b4ed_row335_col1\" class=\"data row335 col1\" >['▁hoida']</td>\n",
       "      <td id=\"T_2b4ed_row335_col2\" class=\"data row335 col2\" >['da_V']</td>\n",
       "      <td id=\"T_2b4ed_row335_col3\" class=\"data row335 col3\" >[0.99985]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row336_col0\" class=\"data row336 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row336_col1\" class=\"data row336 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row336_col2\" class=\"data row336 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row336_col3\" class=\"data row336 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row337_col0\" class=\"data row337 col0\" >['muidu']</td>\n",
       "      <td id=\"T_2b4ed_row337_col1\" class=\"data row337 col1\" >['▁muidu']</td>\n",
       "      <td id=\"T_2b4ed_row337_col2\" class=\"data row337 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row337_col3\" class=\"data row337 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row338_col0\" class=\"data row338 col0\" >['võin']</td>\n",
       "      <td id=\"T_2b4ed_row338_col1\" class=\"data row338 col1\" >['▁võin']</td>\n",
       "      <td id=\"T_2b4ed_row338_col2\" class=\"data row338 col2\" >['n_V']</td>\n",
       "      <td id=\"T_2b4ed_row338_col3\" class=\"data row338 col3\" >[0.99957]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row339_col0\" class=\"data row339 col0\" >['hakata']</td>\n",
       "      <td id=\"T_2b4ed_row339_col1\" class=\"data row339 col1\" >['▁hakata']</td>\n",
       "      <td id=\"T_2b4ed_row339_col2\" class=\"data row339 col2\" >['da_V']</td>\n",
       "      <td id=\"T_2b4ed_row339_col3\" class=\"data row339 col3\" >[0.99987]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row340_col0\" class=\"data row340 col0\" >['juba']</td>\n",
       "      <td id=\"T_2b4ed_row340_col1\" class=\"data row340 col1\" >['▁juba']</td>\n",
       "      <td id=\"T_2b4ed_row340_col2\" class=\"data row340 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row340_col3\" class=\"data row340 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row341_col0\" class=\"data row341 col0\" >['selleski']</td>\n",
       "      <td id=\"T_2b4ed_row341_col1\" class=\"data row341 col1\" >['▁selles', 'ki']</td>\n",
       "      <td id=\"T_2b4ed_row341_col2\" class=\"data row341 col2\" >['sg in_P']</td>\n",
       "      <td id=\"T_2b4ed_row341_col3\" class=\"data row341 col3\" >[0.9992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row342_col0\" class=\"data row342 col0\" >['kahtlema']</td>\n",
       "      <td id=\"T_2b4ed_row342_col1\" class=\"data row342 col1\" >['▁kahtlema']</td>\n",
       "      <td id=\"T_2b4ed_row342_col2\" class=\"data row342 col2\" >['ma_V']</td>\n",
       "      <td id=\"T_2b4ed_row342_col3\" class=\"data row342 col3\" >[0.99926]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row343_col0\" class=\"data row343 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row343_col1\" class=\"data row343 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row343_col2\" class=\"data row343 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row343_col3\" class=\"data row343 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row344_col0\" class=\"data row344 col0\" >['kas']</td>\n",
       "      <td id=\"T_2b4ed_row344_col1\" class=\"data row344 col1\" >['▁kas']</td>\n",
       "      <td id=\"T_2b4ed_row344_col2\" class=\"data row344 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row344_col3\" class=\"data row344 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row345_col0\" class=\"data row345 col0\" >['eelmine']</td>\n",
       "      <td id=\"T_2b4ed_row345_col1\" class=\"data row345 col1\" >['▁eelmine']</td>\n",
       "      <td id=\"T_2b4ed_row345_col2\" class=\"data row345 col2\" >['sg n_A']</td>\n",
       "      <td id=\"T_2b4ed_row345_col3\" class=\"data row345 col3\" >[0.99972]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row346_col0\" class=\"data row346 col0\" >['kommentaar']</td>\n",
       "      <td id=\"T_2b4ed_row346_col1\" class=\"data row346 col1\" >['▁kommentaar']</td>\n",
       "      <td id=\"T_2b4ed_row346_col2\" class=\"data row346 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row346_col3\" class=\"data row346 col3\" >[0.99979]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row347_col0\" class=\"data row347 col0\" >['polnud']</td>\n",
       "      <td id=\"T_2b4ed_row347_col1\" class=\"data row347 col1\" >['▁polnud']</td>\n",
       "      <td id=\"T_2b4ed_row347_col2\" class=\"data row347 col2\" >['neg nud_V']</td>\n",
       "      <td id=\"T_2b4ed_row347_col3\" class=\"data row347 col3\" >[0.99971]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row348_col0\" class=\"data row348 col0\" >['mitte']</td>\n",
       "      <td id=\"T_2b4ed_row348_col1\" class=\"data row348 col1\" >['▁mitte']</td>\n",
       "      <td id=\"T_2b4ed_row348_col2\" class=\"data row348 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row348_col3\" class=\"data row348 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row349_col0\" class=\"data row349 col0\" >['mu']</td>\n",
       "      <td id=\"T_2b4ed_row349_col1\" class=\"data row349 col1\" >['▁mu']</td>\n",
       "      <td id=\"T_2b4ed_row349_col2\" class=\"data row349 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row349_col3\" class=\"data row349 col3\" >[0.99968]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row350_col0\" class=\"data row350 col0\" >['enda']</td>\n",
       "      <td id=\"T_2b4ed_row350_col1\" class=\"data row350 col1\" >['▁enda']</td>\n",
       "      <td id=\"T_2b4ed_row350_col2\" class=\"data row350 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row350_col3\" class=\"data row350 col3\" >[0.99966]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row351_col0\" class=\"data row351 col0\" >['kirjutatud']</td>\n",
       "      <td id=\"T_2b4ed_row351_col1\" class=\"data row351 col1\" >['▁kirjutatud']</td>\n",
       "      <td id=\"T_2b4ed_row351_col2\" class=\"data row351 col2\" >['tud_V']</td>\n",
       "      <td id=\"T_2b4ed_row351_col3\" class=\"data row351 col3\" >[0.98832]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row352_col0\" class=\"data row352 col0\" >['...']</td>\n",
       "      <td id=\"T_2b4ed_row352_col1\" class=\"data row352 col1\" >['...']</td>\n",
       "      <td id=\"T_2b4ed_row352_col2\" class=\"data row352 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row352_col3\" class=\"data row352 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row353_col0\" class=\"data row353 col0\" >['Tuvastamata']</td>\n",
       "      <td id=\"T_2b4ed_row353_col1\" class=\"data row353 col1\" >['▁Tu', 'vasta', 'mata']</td>\n",
       "      <td id=\"T_2b4ed_row353_col2\" class=\"data row353 col2\" >['A']</td>\n",
       "      <td id=\"T_2b4ed_row353_col3\" class=\"data row353 col3\" >[0.95848]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row354_col0\" class=\"data row354 col0\" >['Kasutaja']</td>\n",
       "      <td id=\"T_2b4ed_row354_col1\" class=\"data row354 col1\" >['▁Kasutaja']</td>\n",
       "      <td id=\"T_2b4ed_row354_col2\" class=\"data row354 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row354_col3\" class=\"data row354 col3\" >[0.99825]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row355_col0\" class=\"data row355 col0\" >['Ennustaja']</td>\n",
       "      <td id=\"T_2b4ed_row355_col1\" class=\"data row355 col1\" >['▁Enn', 'ustaja']</td>\n",
       "      <td id=\"T_2b4ed_row355_col2\" class=\"data row355 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row355_col3\" class=\"data row355 col3\" >[0.99937]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row356_col0\" class=\"data row356 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row356_col1\" class=\"data row356 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row356_col2\" class=\"data row356 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row356_col3\" class=\"data row356 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row357_col0\" class=\"data row357 col0\" >['ma']</td>\n",
       "      <td id=\"T_2b4ed_row357_col1\" class=\"data row357 col1\" >['▁ma']</td>\n",
       "      <td id=\"T_2b4ed_row357_col2\" class=\"data row357 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row357_col3\" class=\"data row357 col3\" >[0.99986]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row358_col0\" class=\"data row358 col0\" >['pean']</td>\n",
       "      <td id=\"T_2b4ed_row358_col1\" class=\"data row358 col1\" >['▁pean']</td>\n",
       "      <td id=\"T_2b4ed_row358_col2\" class=\"data row358 col2\" >['n_V']</td>\n",
       "      <td id=\"T_2b4ed_row358_col3\" class=\"data row358 col3\" >[0.99952]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row359_col0\" class=\"data row359 col0\" >['tarvilikuks']</td>\n",
       "      <td id=\"T_2b4ed_row359_col1\" class=\"data row359 col1\" >['▁tarv', 'il', 'ikuks']</td>\n",
       "      <td id=\"T_2b4ed_row359_col2\" class=\"data row359 col2\" >['sg tr_A']</td>\n",
       "      <td id=\"T_2b4ed_row359_col3\" class=\"data row359 col3\" >[0.99891]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row360_col0\" class=\"data row360 col0\" >['teid']</td>\n",
       "      <td id=\"T_2b4ed_row360_col1\" class=\"data row360 col1\" >['▁teid']</td>\n",
       "      <td id=\"T_2b4ed_row360_col2\" class=\"data row360 col2\" >['pl p_P']</td>\n",
       "      <td id=\"T_2b4ed_row360_col3\" class=\"data row360 col3\" >[0.99611]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row361_col0\" class=\"data row361 col0\" >['informeerida']</td>\n",
       "      <td id=\"T_2b4ed_row361_col1\" class=\"data row361 col1\" >['▁informeer', 'ida']</td>\n",
       "      <td id=\"T_2b4ed_row361_col2\" class=\"data row361 col2\" >['da_V']</td>\n",
       "      <td id=\"T_2b4ed_row361_col3\" class=\"data row361 col3\" >[0.99985]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row362_col0\" class=\"data row362 col0\" >['selles']</td>\n",
       "      <td id=\"T_2b4ed_row362_col1\" class=\"data row362 col1\" >['▁selles']</td>\n",
       "      <td id=\"T_2b4ed_row362_col2\" class=\"data row362 col2\" >['sg in_P']</td>\n",
       "      <td id=\"T_2b4ed_row362_col3\" class=\"data row362 col3\" >[0.99934]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row363_col0\" class=\"data row363 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row363_col1\" class=\"data row363 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row363_col2\" class=\"data row363 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row363_col3\" class=\"data row363 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row364_col0\" class=\"data row364 col0\" >['et']</td>\n",
       "      <td id=\"T_2b4ed_row364_col1\" class=\"data row364 col1\" >['▁et']</td>\n",
       "      <td id=\"T_2b4ed_row364_col2\" class=\"data row364 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row364_col3\" class=\"data row364 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row365_col0\" class=\"data row365 col0\" >['sellest']</td>\n",
       "      <td id=\"T_2b4ed_row365_col1\" class=\"data row365 col1\" >['▁sellest']</td>\n",
       "      <td id=\"T_2b4ed_row365_col2\" class=\"data row365 col2\" >['sg el_P']</td>\n",
       "      <td id=\"T_2b4ed_row365_col3\" class=\"data row365 col3\" >[0.99917]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row366_col0\" class=\"data row366 col0\" >['\"']</td>\n",
       "      <td id=\"T_2b4ed_row366_col1\" class=\"data row366 col1\" >['▁\"']</td>\n",
       "      <td id=\"T_2b4ed_row366_col2\" class=\"data row366 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row366_col3\" class=\"data row366 col3\" >[0.99995]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row367_col0\" class=\"data row367 col0\" >['lugejatrombsmaarikapiitertupolev']</td>\n",
       "      <td id=\"T_2b4ed_row367_col1\" class=\"data row367 col1\" >['lu', 'geja', 't', 'ro', 'mb', 's', 'maar', 'ika', 'pii', 'ter', 'tu', 'pole', 'v']</td>\n",
       "      <td id=\"T_2b4ed_row367_col2\" class=\"data row367 col2\" >['sg n_A']</td>\n",
       "      <td id=\"T_2b4ed_row367_col3\" class=\"data row367 col3\" >[0.9845]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row368_col0\" class=\"data row368 col0\" >['brittnolooginoff']</td>\n",
       "      <td id=\"T_2b4ed_row368_col1\" class=\"data row368 col1\" >['▁britt', 'n', 'oloo', 'gin', 'off']</td>\n",
       "      <td id=\"T_2b4ed_row368_col2\" class=\"data row368 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row368_col3\" class=\"data row368 col3\" >[0.99957]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row369_col0\" class=\"data row369 col0\" >['\"']</td>\n",
       "      <td id=\"T_2b4ed_row369_col1\" class=\"data row369 col1\" >['\"']</td>\n",
       "      <td id=\"T_2b4ed_row369_col2\" class=\"data row369 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row369_col3\" class=\"data row369 col3\" >[0.99994]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row370_col0\" class=\"data row370 col0\" >['esinen']</td>\n",
       "      <td id=\"T_2b4ed_row370_col1\" class=\"data row370 col1\" >['▁esin', 'en']</td>\n",
       "      <td id=\"T_2b4ed_row370_col2\" class=\"data row370 col2\" >['n_V']</td>\n",
       "      <td id=\"T_2b4ed_row370_col3\" class=\"data row370 col3\" >[0.99928]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row371_col0\" class=\"data row371 col0\" >['ma']</td>\n",
       "      <td id=\"T_2b4ed_row371_col1\" class=\"data row371 col1\" >['▁ma']</td>\n",
       "      <td id=\"T_2b4ed_row371_col2\" class=\"data row371 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row371_col3\" class=\"data row371 col3\" >[0.99986]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row372_col0\" class=\"data row372 col0\" >['ainult']</td>\n",
       "      <td id=\"T_2b4ed_row372_col1\" class=\"data row372 col1\" >['▁ainult']</td>\n",
       "      <td id=\"T_2b4ed_row372_col2\" class=\"data row372 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row372_col3\" class=\"data row372 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row373_col0\" class=\"data row373 col0\" >['ühe']</td>\n",
       "      <td id=\"T_2b4ed_row373_col1\" class=\"data row373 col1\" >['▁ühe']</td>\n",
       "      <td id=\"T_2b4ed_row373_col2\" class=\"data row373 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row373_col3\" class=\"data row373 col3\" >[0.99935]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row374_col0\" class=\"data row374 col0\" >['korra']</td>\n",
       "      <td id=\"T_2b4ed_row374_col1\" class=\"data row374 col1\" >['▁korra']</td>\n",
       "      <td id=\"T_2b4ed_row374_col2\" class=\"data row374 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row374_col3\" class=\"data row374 col3\" >[0.99968]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row375_col0\" class=\"data row375 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row375_col1\" class=\"data row375 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row375_col2\" class=\"data row375 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row375_col3\" class=\"data row375 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row376_col0\" class=\"data row376 col0\" >['s.t.']</td>\n",
       "      <td id=\"T_2b4ed_row376_col1\" class=\"data row376 col1\" >['▁s', '.', 't', '.']</td>\n",
       "      <td id=\"T_2b4ed_row376_col2\" class=\"data row376 col2\" >['?_Y']</td>\n",
       "      <td id=\"T_2b4ed_row376_col3\" class=\"data row376 col3\" >[0.99731]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row377_col0\" class=\"data row377 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row377_col1\" class=\"data row377 col1\" >['▁.']</td>\n",
       "      <td id=\"T_2b4ed_row377_col2\" class=\"data row377 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row377_col3\" class=\"data row377 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row378_col0\" class=\"data row378 col0\" >['Maarika-na']</td>\n",
       "      <td id=\"T_2b4ed_row378_col1\" class=\"data row378 col1\" >['▁Maar', 'ika', '-', 'na']</td>\n",
       "      <td id=\"T_2b4ed_row378_col2\" class=\"data row378 col2\" >['sg es_S']</td>\n",
       "      <td id=\"T_2b4ed_row378_col3\" class=\"data row378 col3\" >[0.24713]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row379_col0\" class=\"data row379 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row379_col1\" class=\"data row379 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row379_col2\" class=\"data row379 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row379_col3\" class=\"data row379 col3\" >[0.99995]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row380_col0\" class=\"data row380 col0\" >['Teised']</td>\n",
       "      <td id=\"T_2b4ed_row380_col1\" class=\"data row380 col1\" >['▁Teised']</td>\n",
       "      <td id=\"T_2b4ed_row380_col2\" class=\"data row380 col2\" >['pl n_P']</td>\n",
       "      <td id=\"T_2b4ed_row380_col3\" class=\"data row380 col3\" >[0.99979]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row381_col0\" class=\"data row381 col0\" >['pole']</td>\n",
       "      <td id=\"T_2b4ed_row381_col1\" class=\"data row381 col1\" >['▁pole']</td>\n",
       "      <td id=\"T_2b4ed_row381_col2\" class=\"data row381 col2\" >['neg o_V']</td>\n",
       "      <td id=\"T_2b4ed_row381_col3\" class=\"data row381 col3\" >[0.99957]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row382_col0\" class=\"data row382 col0\" >['kunagi']</td>\n",
       "      <td id=\"T_2b4ed_row382_col1\" class=\"data row382 col1\" >['▁kunagi']</td>\n",
       "      <td id=\"T_2b4ed_row382_col2\" class=\"data row382 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row382_col3\" class=\"data row382 col3\" >[0.99987]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row383_col0\" class=\"data row383 col0\" >['minu']</td>\n",
       "      <td id=\"T_2b4ed_row383_col1\" class=\"data row383 col1\" >['▁minu']</td>\n",
       "      <td id=\"T_2b4ed_row383_col2\" class=\"data row383 col2\" >['sg g_P']</td>\n",
       "      <td id=\"T_2b4ed_row383_col3\" class=\"data row383 col3\" >[0.99966]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row384_col0\" class=\"data row384 col0\" >['aliased']</td>\n",
       "      <td id=\"T_2b4ed_row384_col1\" class=\"data row384 col1\" >['▁al', 'ia', 'sed']</td>\n",
       "      <td id=\"T_2b4ed_row384_col2\" class=\"data row384 col2\" >['pl n_S']</td>\n",
       "      <td id=\"T_2b4ed_row384_col3\" class=\"data row384 col3\" >[0.99949]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row385_col0\" class=\"data row385 col0\" >['olnud']</td>\n",
       "      <td id=\"T_2b4ed_row385_col1\" class=\"data row385 col1\" >['▁olnud']</td>\n",
       "      <td id=\"T_2b4ed_row385_col2\" class=\"data row385 col2\" >['nud_V']</td>\n",
       "      <td id=\"T_2b4ed_row385_col3\" class=\"data row385 col3\" >[0.99978]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row386_col0\" class=\"data row386 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row386_col1\" class=\"data row386 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row386_col2\" class=\"data row386 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row386_col3\" class=\"data row386 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row387_col0\" class=\"data row387 col0\" >['Rahu']</td>\n",
       "      <td id=\"T_2b4ed_row387_col1\" class=\"data row387 col1\" >['▁Rahu']</td>\n",
       "      <td id=\"T_2b4ed_row387_col2\" class=\"data row387 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row387_col3\" class=\"data row387 col3\" >[0.98781]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row388_col0\" class=\"data row388 col0\" >['sulle']</td>\n",
       "      <td id=\"T_2b4ed_row388_col1\" class=\"data row388 col1\" >['▁sulle']</td>\n",
       "      <td id=\"T_2b4ed_row388_col2\" class=\"data row388 col2\" >['sg all_P']</td>\n",
       "      <td id=\"T_2b4ed_row388_col3\" class=\"data row388 col3\" >[0.99979]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row389_col0\" class=\"data row389 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row389_col1\" class=\"data row389 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row389_col2\" class=\"data row389 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row389_col3\" class=\"data row389 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row390_col0\" class=\"data row390 col0\" >['ära']</td>\n",
       "      <td id=\"T_2b4ed_row390_col1\" class=\"data row390 col1\" >['▁ära']</td>\n",
       "      <td id=\"T_2b4ed_row390_col2\" class=\"data row390 col2\" >['neg o_V']</td>\n",
       "      <td id=\"T_2b4ed_row390_col3\" class=\"data row390 col3\" >[0.99889]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row391_col0\" class=\"data row391 col0\" >['üle']</td>\n",
       "      <td id=\"T_2b4ed_row391_col1\" class=\"data row391 col1\" >['▁üle']</td>\n",
       "      <td id=\"T_2b4ed_row391_col2\" class=\"data row391 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row391_col3\" class=\"data row391 col3\" >[0.99976]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row392_col0\" class=\"data row392 col0\" >['pinguta']</td>\n",
       "      <td id=\"T_2b4ed_row392_col1\" class=\"data row392 col1\" >['▁pingu', 'ta']</td>\n",
       "      <td id=\"T_2b4ed_row392_col2\" class=\"data row392 col2\" >['o_V']</td>\n",
       "      <td id=\"T_2b4ed_row392_col3\" class=\"data row392 col3\" >[0.99821]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row393_col0\" class=\"data row393 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row393_col1\" class=\"data row393 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row393_col2\" class=\"data row393 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row393_col3\" class=\"data row393 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row394_col0\" class=\"data row394 col0\" >['Tuvastamata']</td>\n",
       "      <td id=\"T_2b4ed_row394_col1\" class=\"data row394 col1\" >['▁Tu', 'vasta', 'mata']</td>\n",
       "      <td id=\"T_2b4ed_row394_col2\" class=\"data row394 col2\" >['A']</td>\n",
       "      <td id=\"T_2b4ed_row394_col3\" class=\"data row394 col3\" >[0.95848]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row395_col0\" class=\"data row395 col0\" >['Kasutaja']</td>\n",
       "      <td id=\"T_2b4ed_row395_col1\" class=\"data row395 col1\" >['▁Kasutaja']</td>\n",
       "      <td id=\"T_2b4ed_row395_col2\" class=\"data row395 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row395_col3\" class=\"data row395 col3\" >[0.99825]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row396_col0\" class=\"data row396 col0\" >['Sageli']</td>\n",
       "      <td id=\"T_2b4ed_row396_col1\" class=\"data row396 col1\" >['▁Sageli']</td>\n",
       "      <td id=\"T_2b4ed_row396_col2\" class=\"data row396 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row396_col3\" class=\"data row396 col3\" >[0.99988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row397_col0\" class=\"data row397 col0\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row397_col1\" class=\"data row397 col1\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row397_col2\" class=\"data row397 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row397_col3\" class=\"data row397 col3\" >[0.99992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row398_col0\" class=\"data row398 col0\" >['Tead']</td>\n",
       "      <td id=\"T_2b4ed_row398_col1\" class=\"data row398 col1\" >['▁Tead']</td>\n",
       "      <td id=\"T_2b4ed_row398_col2\" class=\"data row398 col2\" >['d_V']</td>\n",
       "      <td id=\"T_2b4ed_row398_col3\" class=\"data row398 col3\" >[0.99932]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row399_col0\" class=\"data row399 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row399_col1\" class=\"data row399 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row399_col2\" class=\"data row399 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row399_col3\" class=\"data row399 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row400_col0\" class=\"data row400 col0\" >['see']</td>\n",
       "      <td id=\"T_2b4ed_row400_col1\" class=\"data row400 col1\" >['▁see']</td>\n",
       "      <td id=\"T_2b4ed_row400_col2\" class=\"data row400 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row400_col3\" class=\"data row400 col3\" >[0.99987]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row401_col0\" class=\"data row401 col0\" >['sõltub']</td>\n",
       "      <td id=\"T_2b4ed_row401_col1\" class=\"data row401 col1\" >['▁sõltub']</td>\n",
       "      <td id=\"T_2b4ed_row401_col2\" class=\"data row401 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row401_col3\" class=\"data row401 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row402_col0\" class=\"data row402 col0\" >['kui']</td>\n",
       "      <td id=\"T_2b4ed_row402_col1\" class=\"data row402 col1\" >['▁kui']</td>\n",
       "      <td id=\"T_2b4ed_row402_col2\" class=\"data row402 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row402_col3\" class=\"data row402 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row403_col0\" class=\"data row403 col0\" >['tihti']</td>\n",
       "      <td id=\"T_2b4ed_row403_col1\" class=\"data row403 col1\" >['▁tihti']</td>\n",
       "      <td id=\"T_2b4ed_row403_col2\" class=\"data row403 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row403_col3\" class=\"data row403 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row404_col0\" class=\"data row404 col0\" >['nimetatud']</td>\n",
       "      <td id=\"T_2b4ed_row404_col1\" class=\"data row404 col1\" >['▁nimetatud']</td>\n",
       "      <td id=\"T_2b4ed_row404_col2\" class=\"data row404 col2\" >['A']</td>\n",
       "      <td id=\"T_2b4ed_row404_col3\" class=\"data row404 col3\" >[0.99921]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row405_col0\" class=\"data row405 col0\" >['seltskond']</td>\n",
       "      <td id=\"T_2b4ed_row405_col1\" class=\"data row405 col1\" >['▁seltskond']</td>\n",
       "      <td id=\"T_2b4ed_row405_col2\" class=\"data row405 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row405_col3\" class=\"data row405 col3\" >[0.9998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row406_col0\" class=\"data row406 col0\" >['nimetatud']</td>\n",
       "      <td id=\"T_2b4ed_row406_col1\" class=\"data row406 col1\" >['▁nimetatud']</td>\n",
       "      <td id=\"T_2b4ed_row406_col2\" class=\"data row406 col2\" >['A']</td>\n",
       "      <td id=\"T_2b4ed_row406_col3\" class=\"data row406 col3\" >[0.99921]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row407_col0\" class=\"data row407 col0\" >['ollust']</td>\n",
       "      <td id=\"T_2b4ed_row407_col1\" class=\"data row407 col1\" >['▁ol', 'lust']</td>\n",
       "      <td id=\"T_2b4ed_row407_col2\" class=\"data row407 col2\" >['sg p_S']</td>\n",
       "      <td id=\"T_2b4ed_row407_col3\" class=\"data row407 col3\" >[0.99863]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row408_col0\" class=\"data row408 col0\" >['siia']</td>\n",
       "      <td id=\"T_2b4ed_row408_col1\" class=\"data row408 col1\" >['▁siia']</td>\n",
       "      <td id=\"T_2b4ed_row408_col2\" class=\"data row408 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row408_col3\" class=\"data row408 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row409_col0\" class=\"data row409 col0\" >['laiali']</td>\n",
       "      <td id=\"T_2b4ed_row409_col1\" class=\"data row409 col1\" >['▁laiali']</td>\n",
       "      <td id=\"T_2b4ed_row409_col2\" class=\"data row409 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row409_col3\" class=\"data row409 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row410_col0\" class=\"data row410 col0\" >['laotab']</td>\n",
       "      <td id=\"T_2b4ed_row410_col1\" class=\"data row410 col1\" >['▁lao', 'tab']</td>\n",
       "      <td id=\"T_2b4ed_row410_col2\" class=\"data row410 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row410_col3\" class=\"data row410 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row411_col0\" class=\"data row411 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row411_col1\" class=\"data row411 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row411_col2\" class=\"data row411 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row411_col3\" class=\"data row411 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row412_col0\" class=\"data row412 col0\" >['Ja']</td>\n",
       "      <td id=\"T_2b4ed_row412_col1\" class=\"data row412 col1\" >['▁Ja']</td>\n",
       "      <td id=\"T_2b4ed_row412_col2\" class=\"data row412 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row412_col3\" class=\"data row412 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row413_col0\" class=\"data row413 col0\" >['veel']</td>\n",
       "      <td id=\"T_2b4ed_row413_col1\" class=\"data row413 col1\" >['▁veel']</td>\n",
       "      <td id=\"T_2b4ed_row413_col2\" class=\"data row413 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row413_col3\" class=\"data row413 col3\" >[0.99984]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row414_col0\" class=\"data row414 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row414_col1\" class=\"data row414 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row414_col2\" class=\"data row414 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row414_col3\" class=\"data row414 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row415_col0\" class=\"data row415 col0\" >['vabanda']</td>\n",
       "      <td id=\"T_2b4ed_row415_col1\" class=\"data row415 col1\" >['▁vabanda']</td>\n",
       "      <td id=\"T_2b4ed_row415_col2\" class=\"data row415 col2\" >['o_V']</td>\n",
       "      <td id=\"T_2b4ed_row415_col3\" class=\"data row415 col3\" >[0.99886]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row416_col0\" class=\"data row416 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row416_col1\" class=\"data row416 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row416_col2\" class=\"data row416 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row416_col3\" class=\"data row416 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row417_col0\" class=\"data row417 col0\" >['aga']</td>\n",
       "      <td id=\"T_2b4ed_row417_col1\" class=\"data row417 col1\" >['▁aga']</td>\n",
       "      <td id=\"T_2b4ed_row417_col2\" class=\"data row417 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row417_col3\" class=\"data row417 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row418_col0\" class=\"data row418 col0\" >['sina']</td>\n",
       "      <td id=\"T_2b4ed_row418_col1\" class=\"data row418 col1\" >['▁sina']</td>\n",
       "      <td id=\"T_2b4ed_row418_col2\" class=\"data row418 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row418_col3\" class=\"data row418 col3\" >[0.99986]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row419_col0\" class=\"data row419 col0\" >['küll']</td>\n",
       "      <td id=\"T_2b4ed_row419_col1\" class=\"data row419 col1\" >['▁küll']</td>\n",
       "      <td id=\"T_2b4ed_row419_col2\" class=\"data row419 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row419_col3\" class=\"data row419 col3\" >[0.99985]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row420_col0\" class=\"data row420 col0\" >['ei']</td>\n",
       "      <td id=\"T_2b4ed_row420_col1\" class=\"data row420 col1\" >['▁ei']</td>\n",
       "      <td id=\"T_2b4ed_row420_col2\" class=\"data row420 col2\" >['neg_V']</td>\n",
       "      <td id=\"T_2b4ed_row420_col3\" class=\"data row420 col3\" >[0.99988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row421_col0\" class=\"data row421 col0\" >['ole']</td>\n",
       "      <td id=\"T_2b4ed_row421_col1\" class=\"data row421 col1\" >['▁ole']</td>\n",
       "      <td id=\"T_2b4ed_row421_col2\" class=\"data row421 col2\" >['o_V']</td>\n",
       "      <td id=\"T_2b4ed_row421_col3\" class=\"data row421 col3\" >[0.99964]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row422_col0\" class=\"data row422 col0\" >['armas']</td>\n",
       "      <td id=\"T_2b4ed_row422_col1\" class=\"data row422 col1\" >['▁armas']</td>\n",
       "      <td id=\"T_2b4ed_row422_col2\" class=\"data row422 col2\" >['sg n_A']</td>\n",
       "      <td id=\"T_2b4ed_row422_col3\" class=\"data row422 col3\" >[0.99977]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row423_col0\" class=\"data row423 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row423_col1\" class=\"data row423 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row423_col2\" class=\"data row423 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row423_col3\" class=\"data row423 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row424_col0\" class=\"data row424 col0\" >['kellele']</td>\n",
       "      <td id=\"T_2b4ed_row424_col1\" class=\"data row424 col1\" >['▁kellele']</td>\n",
       "      <td id=\"T_2b4ed_row424_col2\" class=\"data row424 col2\" >['sg all_P']</td>\n",
       "      <td id=\"T_2b4ed_row424_col3\" class=\"data row424 col3\" >[0.99964]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row425_col0\" class=\"data row425 col0\" >['(']</td>\n",
       "      <td id=\"T_2b4ed_row425_col1\" class=\"data row425 col1\" >['▁(']</td>\n",
       "      <td id=\"T_2b4ed_row425_col2\" class=\"data row425 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row425_col3\" class=\"data row425 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row426_col0\" class=\"data row426 col0\" >['peale']</td>\n",
       "      <td id=\"T_2b4ed_row426_col1\" class=\"data row426 col1\" >['peale']</td>\n",
       "      <td id=\"T_2b4ed_row426_col2\" class=\"data row426 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row426_col3\" class=\"data row426 col3\" >[0.99927]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row427_col0\" class=\"data row427 col0\" >['savisaarlaste']</td>\n",
       "      <td id=\"T_2b4ed_row427_col1\" class=\"data row427 col1\" >['▁savisaar', 'laste']</td>\n",
       "      <td id=\"T_2b4ed_row427_col2\" class=\"data row427 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row427_col3\" class=\"data row427 col3\" >[0.99955]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row428_col0\" class=\"data row428 col0\" >[')']</td>\n",
       "      <td id=\"T_2b4ed_row428_col1\" class=\"data row428 col1\" >[')']</td>\n",
       "      <td id=\"T_2b4ed_row428_col2\" class=\"data row428 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row428_col3\" class=\"data row428 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row429_col0\" class=\"data row429 col0\" >['saab']</td>\n",
       "      <td id=\"T_2b4ed_row429_col1\" class=\"data row429 col1\" >['▁saab']</td>\n",
       "      <td id=\"T_2b4ed_row429_col2\" class=\"data row429 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row429_col3\" class=\"data row429 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row430_col0\" class=\"data row430 col0\" >['armas']</td>\n",
       "      <td id=\"T_2b4ed_row430_col1\" class=\"data row430 col1\" >['▁armas']</td>\n",
       "      <td id=\"T_2b4ed_row430_col2\" class=\"data row430 col2\" >['sg n_A']</td>\n",
       "      <td id=\"T_2b4ed_row430_col3\" class=\"data row430 col3\" >[0.99978]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row431_col0\" class=\"data row431 col0\" >['olla']</td>\n",
       "      <td id=\"T_2b4ed_row431_col1\" class=\"data row431 col1\" >['▁olla']</td>\n",
       "      <td id=\"T_2b4ed_row431_col2\" class=\"data row431 col2\" >['da_V']</td>\n",
       "      <td id=\"T_2b4ed_row431_col3\" class=\"data row431 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row432_col0\" class=\"data row432 col0\" >['sihuke']</td>\n",
       "      <td id=\"T_2b4ed_row432_col1\" class=\"data row432 col1\" >['▁sihuke']</td>\n",
       "      <td id=\"T_2b4ed_row432_col2\" class=\"data row432 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row432_col3\" class=\"data row432 col3\" >[0.99985]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row433_col0\" class=\"data row433 col0\" >['punane']</td>\n",
       "      <td id=\"T_2b4ed_row433_col1\" class=\"data row433 col1\" >['▁punane']</td>\n",
       "      <td id=\"T_2b4ed_row433_col2\" class=\"data row433 col2\" >['sg n_A']</td>\n",
       "      <td id=\"T_2b4ed_row433_col3\" class=\"data row433 col3\" >[0.99974]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row434_col0\" class=\"data row434 col0\" >['eva']</td>\n",
       "      <td id=\"T_2b4ed_row434_col1\" class=\"data row434 col1\" >['▁e', 'va']</td>\n",
       "      <td id=\"T_2b4ed_row434_col2\" class=\"data row434 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row434_col3\" class=\"data row434 col3\" >[0.97083]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row435_col0\" class=\"data row435 col0\" >[\"'\"]</td>\n",
       "      <td id=\"T_2b4ed_row435_col1\" class=\"data row435 col1\" >[\"▁'\"]</td>\n",
       "      <td id=\"T_2b4ed_row435_col2\" class=\"data row435 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row435_col3\" class=\"data row435 col3\" >[0.99977]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row436_col0\" class=\"data row436 col0\" >['kohuke']</td>\n",
       "      <td id=\"T_2b4ed_row436_col1\" class=\"data row436 col1\" >['koh', 'uke']</td>\n",
       "      <td id=\"T_2b4ed_row436_col2\" class=\"data row436 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row436_col3\" class=\"data row436 col3\" >[0.9987]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row437_col0\" class=\"data row437 col0\" >[\"'\"]</td>\n",
       "      <td id=\"T_2b4ed_row437_col1\" class=\"data row437 col1\" >[\"'\"]</td>\n",
       "      <td id=\"T_2b4ed_row437_col2\" class=\"data row437 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row437_col3\" class=\"data row437 col3\" >[0.99978]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row438_col0\" class=\"data row438 col0\" >['britt']</td>\n",
       "      <td id=\"T_2b4ed_row438_col1\" class=\"data row438 col1\" >['▁britt']</td>\n",
       "      <td id=\"T_2b4ed_row438_col2\" class=\"data row438 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row438_col3\" class=\"data row438 col3\" >[0.99965]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row439_col0\" class=\"data row439 col0\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row439_col1\" class=\"data row439 col1\" >['?']</td>\n",
       "      <td id=\"T_2b4ed_row439_col2\" class=\"data row439 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row439_col3\" class=\"data row439 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row440_col0\" class=\"data row440 col0\" >['Tuvastamata']</td>\n",
       "      <td id=\"T_2b4ed_row440_col1\" class=\"data row440 col1\" >['▁Tu', 'vasta', 'mata']</td>\n",
       "      <td id=\"T_2b4ed_row440_col2\" class=\"data row440 col2\" >['A']</td>\n",
       "      <td id=\"T_2b4ed_row440_col3\" class=\"data row440 col3\" >[0.95848]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row441_col0\" class=\"data row441 col0\" >['Kasutaja']</td>\n",
       "      <td id=\"T_2b4ed_row441_col1\" class=\"data row441 col1\" >['▁Kasutaja']</td>\n",
       "      <td id=\"T_2b4ed_row441_col2\" class=\"data row441 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row441_col3\" class=\"data row441 col3\" >[0.99825]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row442_col0\" class=\"data row442 col0\" >['27.04.2009']</td>\n",
       "      <td id=\"T_2b4ed_row442_col1\" class=\"data row442 col1\" >['▁27', '.04.2009']</td>\n",
       "      <td id=\"T_2b4ed_row442_col2\" class=\"data row442 col2\" >['?_N']</td>\n",
       "      <td id=\"T_2b4ed_row442_col3\" class=\"data row442 col3\" >[0.99494]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row443_col0\" class=\"data row443 col0\" >['20:53']</td>\n",
       "      <td id=\"T_2b4ed_row443_col1\" class=\"data row443 col1\" >['▁20', ':5', '3']</td>\n",
       "      <td id=\"T_2b4ed_row443_col2\" class=\"data row443 col2\" >['?_N']</td>\n",
       "      <td id=\"T_2b4ed_row443_col3\" class=\"data row443 col3\" >[0.99896]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row444_col0\" class=\"data row444 col0\" >['Märkasin']</td>\n",
       "      <td id=\"T_2b4ed_row444_col1\" class=\"data row444 col1\" >['▁Mär', 'kasin']</td>\n",
       "      <td id=\"T_2b4ed_row444_col2\" class=\"data row444 col2\" >['sin_V']</td>\n",
       "      <td id=\"T_2b4ed_row444_col3\" class=\"data row444 col3\" >[0.99867]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row445_col0\" class=\"data row445 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row445_col1\" class=\"data row445 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row445_col2\" class=\"data row445 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row445_col3\" class=\"data row445 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row446_col0\" class=\"data row446 col0\" >['et']</td>\n",
       "      <td id=\"T_2b4ed_row446_col1\" class=\"data row446 col1\" >['▁et']</td>\n",
       "      <td id=\"T_2b4ed_row446_col2\" class=\"data row446 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row446_col3\" class=\"data row446 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row447_col0\" class=\"data row447 col0\" >['mulle']</td>\n",
       "      <td id=\"T_2b4ed_row447_col1\" class=\"data row447 col1\" >['▁mulle']</td>\n",
       "      <td id=\"T_2b4ed_row447_col2\" class=\"data row447 col2\" >['sg all_P']</td>\n",
       "      <td id=\"T_2b4ed_row447_col3\" class=\"data row447 col3\" >[0.99976]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row448_col0\" class=\"data row448 col0\" >['on']</td>\n",
       "      <td id=\"T_2b4ed_row448_col1\" class=\"data row448 col1\" >['▁on']</td>\n",
       "      <td id=\"T_2b4ed_row448_col2\" class=\"data row448 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row448_col3\" class=\"data row448 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row449_col0\" class=\"data row449 col0\" >['siin']</td>\n",
       "      <td id=\"T_2b4ed_row449_col1\" class=\"data row449 col1\" >['▁siin']</td>\n",
       "      <td id=\"T_2b4ed_row449_col2\" class=\"data row449 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row449_col3\" class=\"data row449 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row450_col0\" class=\"data row450 col0\" >['ametivend']</td>\n",
       "      <td id=\"T_2b4ed_row450_col1\" class=\"data row450 col1\" >['▁amet', 'iv', 'end']</td>\n",
       "      <td id=\"T_2b4ed_row450_col2\" class=\"data row450 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row450_col3\" class=\"data row450 col3\" >[0.99942]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row451_col0\" class=\"data row451 col0\" >['tekkinud']</td>\n",
       "      <td id=\"T_2b4ed_row451_col1\" class=\"data row451 col1\" >['▁tekkinud']</td>\n",
       "      <td id=\"T_2b4ed_row451_col2\" class=\"data row451 col2\" >['nud_V']</td>\n",
       "      <td id=\"T_2b4ed_row451_col3\" class=\"data row451 col3\" >[0.99977]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row452_col0\" class=\"data row452 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row452_col1\" class=\"data row452 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row452_col2\" class=\"data row452 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row452_col3\" class=\"data row452 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row453_col0\" class=\"data row453 col0\" >['Ilmutates']</td>\n",
       "      <td id=\"T_2b4ed_row453_col1\" class=\"data row453 col1\" >['▁Ilm', 'uta', 'tes']</td>\n",
       "      <td id=\"T_2b4ed_row453_col2\" class=\"data row453 col2\" >['des_V']</td>\n",
       "      <td id=\"T_2b4ed_row453_col3\" class=\"data row453 col3\" >[0.99864]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row454_col0\" class=\"data row454 col0\" >['loomulikku']</td>\n",
       "      <td id=\"T_2b4ed_row454_col1\" class=\"data row454 col1\" >['▁loomulikku']</td>\n",
       "      <td id=\"T_2b4ed_row454_col2\" class=\"data row454 col2\" >['sg p_A']</td>\n",
       "      <td id=\"T_2b4ed_row454_col3\" class=\"data row454 col3\" >[0.99944]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row455_col0\" class=\"data row455 col0\" >['huvi']</td>\n",
       "      <td id=\"T_2b4ed_row455_col1\" class=\"data row455 col1\" >['▁huvi']</td>\n",
       "      <td id=\"T_2b4ed_row455_col2\" class=\"data row455 col2\" >['sg p_S']</td>\n",
       "      <td id=\"T_2b4ed_row455_col3\" class=\"data row455 col3\" >[0.99959]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row456_col0\" class=\"data row456 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row456_col1\" class=\"data row456 col1\" >['▁,']</td>\n",
       "      <td id=\"T_2b4ed_row456_col2\" class=\"data row456 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row456_col3\" class=\"data row456 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row457_col0\" class=\"data row457 col0\" >['tahtsin']</td>\n",
       "      <td id=\"T_2b4ed_row457_col1\" class=\"data row457 col1\" >['▁tahtsin']</td>\n",
       "      <td id=\"T_2b4ed_row457_col2\" class=\"data row457 col2\" >['sin_V']</td>\n",
       "      <td id=\"T_2b4ed_row457_col3\" class=\"data row457 col3\" >[0.99872]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row458_col0\" class=\"data row458 col0\" >['küsida']</td>\n",
       "      <td id=\"T_2b4ed_row458_col1\" class=\"data row458 col1\" >['▁küsida']</td>\n",
       "      <td id=\"T_2b4ed_row458_col2\" class=\"data row458 col2\" >['da_V']</td>\n",
       "      <td id=\"T_2b4ed_row458_col3\" class=\"data row458 col3\" >[0.99986]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row459_col0\" class=\"data row459 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row459_col1\" class=\"data row459 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row459_col2\" class=\"data row459 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row459_col3\" class=\"data row459 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row460_col0\" class=\"data row460 col0\" >['mis']</td>\n",
       "      <td id=\"T_2b4ed_row460_col1\" class=\"data row460 col1\" >['▁mis']</td>\n",
       "      <td id=\"T_2b4ed_row460_col2\" class=\"data row460 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row460_col3\" class=\"data row460 col3\" >[0.99984]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row461_col0\" class=\"data row461 col0\" >['abivahendeid']</td>\n",
       "      <td id=\"T_2b4ed_row461_col1\" class=\"data row461 col1\" >['▁abi', 'vahendeid']</td>\n",
       "      <td id=\"T_2b4ed_row461_col2\" class=\"data row461 col2\" >['pl p_S']</td>\n",
       "      <td id=\"T_2b4ed_row461_col3\" class=\"data row461 col3\" >[0.99955]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row462_col0\" class=\"data row462 col0\" >['kolleeg']</td>\n",
       "      <td id=\"T_2b4ed_row462_col1\" class=\"data row462 col1\" >['▁kolleeg']</td>\n",
       "      <td id=\"T_2b4ed_row462_col2\" class=\"data row462 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row462_col3\" class=\"data row462 col3\" >[0.9998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row463_col0\" class=\"data row463 col0\" >['kasutab']</td>\n",
       "      <td id=\"T_2b4ed_row463_col1\" class=\"data row463 col1\" >['▁kasutab']</td>\n",
       "      <td id=\"T_2b4ed_row463_col2\" class=\"data row463 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row463_col3\" class=\"data row463 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row464_col0\" class=\"data row464 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row464_col1\" class=\"data row464 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row464_col2\" class=\"data row464 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row464_col3\" class=\"data row464 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row465_col0\" class=\"data row465 col0\" >['kas']</td>\n",
       "      <td id=\"T_2b4ed_row465_col1\" class=\"data row465 col1\" >['▁kas']</td>\n",
       "      <td id=\"T_2b4ed_row465_col2\" class=\"data row465 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row465_col3\" class=\"data row465 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row466_col0\" class=\"data row466 col0\" >['ennustab']</td>\n",
       "      <td id=\"T_2b4ed_row466_col1\" class=\"data row466 col1\" >['▁ennustab']</td>\n",
       "      <td id=\"T_2b4ed_row466_col2\" class=\"data row466 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row466_col3\" class=\"data row466 col3\" >[0.99992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row467_col0\" class=\"data row467 col0\" >['vasikapõrna']</td>\n",
       "      <td id=\"T_2b4ed_row467_col1\" class=\"data row467 col1\" >['▁vas', 'ika', 'põr', 'na']</td>\n",
       "      <td id=\"T_2b4ed_row467_col2\" class=\"data row467 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row467_col3\" class=\"data row467 col3\" >[0.99968]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row468_col0\" class=\"data row468 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row468_col1\" class=\"data row468 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row468_col2\" class=\"data row468 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row468_col3\" class=\"data row468 col3\" >[0.99995]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row469_col0\" class=\"data row469 col0\" >['tuvisisikonna']</td>\n",
       "      <td id=\"T_2b4ed_row469_col1\" class=\"data row469 col1\" >['▁tu', 'vis', 'is', 'iko', 'nna']</td>\n",
       "      <td id=\"T_2b4ed_row469_col2\" class=\"data row469 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row469_col3\" class=\"data row469 col3\" >[0.99964]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row470_col0\" class=\"data row470 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row470_col1\" class=\"data row470 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row470_col2\" class=\"data row470 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row470_col3\" class=\"data row470 col3\" >[0.99995]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row471_col0\" class=\"data row471 col0\" >['seasüdame']</td>\n",
       "      <td id=\"T_2b4ed_row471_col1\" class=\"data row471 col1\" >['▁seas', 'ü', 'dame']</td>\n",
       "      <td id=\"T_2b4ed_row471_col2\" class=\"data row471 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row471_col3\" class=\"data row471 col3\" >[0.99964]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row472_col0\" class=\"data row472 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row472_col1\" class=\"data row472 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row472_col2\" class=\"data row472 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row472_col3\" class=\"data row472 col3\" >[0.99995]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row473_col0\" class=\"data row473 col0\" >['kohvipaksu']</td>\n",
       "      <td id=\"T_2b4ed_row473_col1\" class=\"data row473 col1\" >['▁kohvi', 'pa', 'ksu']</td>\n",
       "      <td id=\"T_2b4ed_row473_col2\" class=\"data row473 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row473_col3\" class=\"data row473 col3\" >[0.99964]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row474_col0\" class=\"data row474 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row474_col1\" class=\"data row474 col1\" >['▁,']</td>\n",
       "      <td id=\"T_2b4ed_row474_col2\" class=\"data row474 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row474_col3\" class=\"data row474 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row475_col0\" class=\"data row475 col0\" >['või']</td>\n",
       "      <td id=\"T_2b4ed_row475_col1\" class=\"data row475 col1\" >['▁või']</td>\n",
       "      <td id=\"T_2b4ed_row475_col2\" class=\"data row475 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row475_col3\" class=\"data row475 col3\" >[0.99984]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row476_col0\" class=\"data row476 col0\" >['kõhutuule']</td>\n",
       "      <td id=\"T_2b4ed_row476_col1\" class=\"data row476 col1\" >['▁kõh', 'ut', 'uule']</td>\n",
       "      <td id=\"T_2b4ed_row476_col2\" class=\"data row476 col2\" >['sg g_S']</td>\n",
       "      <td id=\"T_2b4ed_row476_col3\" class=\"data row476 col3\" >[0.99962]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row477_col0\" class=\"data row477 col0\" >['järgi']</td>\n",
       "      <td id=\"T_2b4ed_row477_col1\" class=\"data row477 col1\" >['▁järgi']</td>\n",
       "      <td id=\"T_2b4ed_row477_col2\" class=\"data row477 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row477_col3\" class=\"data row477 col3\" >[0.99968]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row478_col0\" class=\"data row478 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row478_col1\" class=\"data row478 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row478_col2\" class=\"data row478 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row478_col3\" class=\"data row478 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row479_col0\" class=\"data row479 col0\" >['Lugedes']</td>\n",
       "      <td id=\"T_2b4ed_row479_col1\" class=\"data row479 col1\" >['▁Lugedes']</td>\n",
       "      <td id=\"T_2b4ed_row479_col2\" class=\"data row479 col2\" >['des_V']</td>\n",
       "      <td id=\"T_2b4ed_row479_col3\" class=\"data row479 col3\" >[0.99942]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row480_col0\" class=\"data row480 col0\" >['teie']</td>\n",
       "      <td id=\"T_2b4ed_row480_col1\" class=\"data row480 col1\" >['▁teie']</td>\n",
       "      <td id=\"T_2b4ed_row480_col2\" class=\"data row480 col2\" >['pl g_P']</td>\n",
       "      <td id=\"T_2b4ed_row480_col3\" class=\"data row480 col3\" >[0.99928]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row481_col0\" class=\"data row481 col0\" >['kommentaare']</td>\n",
       "      <td id=\"T_2b4ed_row481_col1\" class=\"data row481 col1\" >['▁kommentaare']</td>\n",
       "      <td id=\"T_2b4ed_row481_col2\" class=\"data row481 col2\" >['pl p_S']</td>\n",
       "      <td id=\"T_2b4ed_row481_col3\" class=\"data row481 col3\" >[0.99958]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row482_col0\" class=\"data row482 col0\" >['tundub']</td>\n",
       "      <td id=\"T_2b4ed_row482_col1\" class=\"data row482 col1\" >['▁tundub']</td>\n",
       "      <td id=\"T_2b4ed_row482_col2\" class=\"data row482 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row482_col3\" class=\"data row482 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row483_col0\" class=\"data row483 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row483_col1\" class=\"data row483 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row483_col2\" class=\"data row483 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row483_col3\" class=\"data row483 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row484_col0\" class=\"data row484 col0\" >['et']</td>\n",
       "      <td id=\"T_2b4ed_row484_col1\" class=\"data row484 col1\" >['▁et']</td>\n",
       "      <td id=\"T_2b4ed_row484_col2\" class=\"data row484 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row484_col3\" class=\"data row484 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row485_col0\" class=\"data row485 col0\" >['kasutate']</td>\n",
       "      <td id=\"T_2b4ed_row485_col1\" class=\"data row485 col1\" >['▁kasuta', 'te']</td>\n",
       "      <td id=\"T_2b4ed_row485_col2\" class=\"data row485 col2\" >['te_V']</td>\n",
       "      <td id=\"T_2b4ed_row485_col3\" class=\"data row485 col3\" >[0.99862]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row486_col0\" class=\"data row486 col0\" >['viimast']</td>\n",
       "      <td id=\"T_2b4ed_row486_col1\" class=\"data row486 col1\" >['▁viimast']</td>\n",
       "      <td id=\"T_2b4ed_row486_col2\" class=\"data row486 col2\" >['sg p_A']</td>\n",
       "      <td id=\"T_2b4ed_row486_col3\" class=\"data row486 col3\" >[0.99898]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row487_col0\" class=\"data row487 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row487_col1\" class=\"data row487 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row487_col2\" class=\"data row487 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row487_col3\" class=\"data row487 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row488_col0\" class=\"data row488 col0\" >['Hoiatan']</td>\n",
       "      <td id=\"T_2b4ed_row488_col1\" class=\"data row488 col1\" >['▁Hoia', 'tan']</td>\n",
       "      <td id=\"T_2b4ed_row488_col2\" class=\"data row488 col2\" >['n_V']</td>\n",
       "      <td id=\"T_2b4ed_row488_col3\" class=\"data row488 col3\" >[0.99952]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row489_col0\" class=\"data row489 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row489_col1\" class=\"data row489 col1\" >['▁,']</td>\n",
       "      <td id=\"T_2b4ed_row489_col2\" class=\"data row489 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row489_col3\" class=\"data row489 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row490_col0\" class=\"data row490 col0\" >['see']</td>\n",
       "      <td id=\"T_2b4ed_row490_col1\" class=\"data row490 col1\" >['▁see']</td>\n",
       "      <td id=\"T_2b4ed_row490_col2\" class=\"data row490 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row490_col3\" class=\"data row490 col3\" >[0.99987]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row491_col0\" class=\"data row491 col0\" >['ei']</td>\n",
       "      <td id=\"T_2b4ed_row491_col1\" class=\"data row491 col1\" >['▁ei']</td>\n",
       "      <td id=\"T_2b4ed_row491_col2\" class=\"data row491 col2\" >['neg_V']</td>\n",
       "      <td id=\"T_2b4ed_row491_col3\" class=\"data row491 col3\" >[0.99987]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row492_col0\" class=\"data row492 col0\" >['pruugi']</td>\n",
       "      <td id=\"T_2b4ed_row492_col1\" class=\"data row492 col1\" >['▁pruugi']</td>\n",
       "      <td id=\"T_2b4ed_row492_col2\" class=\"data row492 col2\" >['o_V']</td>\n",
       "      <td id=\"T_2b4ed_row492_col3\" class=\"data row492 col3\" >[0.99963]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row493_col0\" class=\"data row493 col0\" >['alati']</td>\n",
       "      <td id=\"T_2b4ed_row493_col1\" class=\"data row493 col1\" >['▁alati']</td>\n",
       "      <td id=\"T_2b4ed_row493_col2\" class=\"data row493 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row493_col3\" class=\"data row493 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row494_col0\" class=\"data row494 col0\" >['anda']</td>\n",
       "      <td id=\"T_2b4ed_row494_col1\" class=\"data row494 col1\" >['▁anda']</td>\n",
       "      <td id=\"T_2b4ed_row494_col2\" class=\"data row494 col2\" >['da_V']</td>\n",
       "      <td id=\"T_2b4ed_row494_col3\" class=\"data row494 col3\" >[0.99987]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row495_col0\" class=\"data row495 col0\" >['õiget']</td>\n",
       "      <td id=\"T_2b4ed_row495_col1\" class=\"data row495 col1\" >['▁õiget']</td>\n",
       "      <td id=\"T_2b4ed_row495_col2\" class=\"data row495 col2\" >['sg p_A']</td>\n",
       "      <td id=\"T_2b4ed_row495_col3\" class=\"data row495 col3\" >[0.99946]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row496_col0\" class=\"data row496 col0\" >['resultaati']</td>\n",
       "      <td id=\"T_2b4ed_row496_col1\" class=\"data row496 col1\" >['▁resul', 'ta', 'ati']</td>\n",
       "      <td id=\"T_2b4ed_row496_col2\" class=\"data row496 col2\" >['sg p_S']</td>\n",
       "      <td id=\"T_2b4ed_row496_col3\" class=\"data row496 col3\" >[0.99968]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row497_col0\" class=\"data row497 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row497_col1\" class=\"data row497 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row497_col2\" class=\"data row497 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row497_col3\" class=\"data row497 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row498_col0\" class=\"data row498 col0\" >['eriti']</td>\n",
       "      <td id=\"T_2b4ed_row498_col1\" class=\"data row498 col1\" >['▁eriti']</td>\n",
       "      <td id=\"T_2b4ed_row498_col2\" class=\"data row498 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row498_col3\" class=\"data row498 col3\" >[0.9999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row499_col0\" class=\"data row499 col0\" >['kui']</td>\n",
       "      <td id=\"T_2b4ed_row499_col1\" class=\"data row499 col1\" >['▁kui']</td>\n",
       "      <td id=\"T_2b4ed_row499_col2\" class=\"data row499 col2\" >['J']</td>\n",
       "      <td id=\"T_2b4ed_row499_col3\" class=\"data row499 col3\" >[0.99991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row500_col0\" class=\"data row500 col0\" >['päevases']</td>\n",
       "      <td id=\"T_2b4ed_row500_col1\" class=\"data row500 col1\" >['▁päeva', 'ses']</td>\n",
       "      <td id=\"T_2b4ed_row500_col2\" class=\"data row500 col2\" >['sg in_A']</td>\n",
       "      <td id=\"T_2b4ed_row500_col3\" class=\"data row500 col3\" >[0.99881]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row501_col0\" class=\"data row501 col0\" >['menüüs']</td>\n",
       "      <td id=\"T_2b4ed_row501_col1\" class=\"data row501 col1\" >['▁menüüs']</td>\n",
       "      <td id=\"T_2b4ed_row501_col2\" class=\"data row501 col2\" >['sg in_S']</td>\n",
       "      <td id=\"T_2b4ed_row501_col3\" class=\"data row501 col3\" >[0.9996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row502_col0\" class=\"data row502 col0\" >['on']</td>\n",
       "      <td id=\"T_2b4ed_row502_col1\" class=\"data row502 col1\" >['▁on']</td>\n",
       "      <td id=\"T_2b4ed_row502_col2\" class=\"data row502 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row502_col3\" class=\"data row502 col3\" >[0.99992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row503_col0\" class=\"data row503 col0\" >['hernesupp']</td>\n",
       "      <td id=\"T_2b4ed_row503_col1\" class=\"data row503 col1\" >['▁her', 'nes', 'u', 'pp']</td>\n",
       "      <td id=\"T_2b4ed_row503_col2\" class=\"data row503 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row503_col3\" class=\"data row503 col3\" >[0.99969]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row504_col0\" class=\"data row504 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row504_col1\" class=\"data row504 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row504_col2\" class=\"data row504 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row504_col3\" class=\"data row504 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row505_col0\" class=\"data row505 col0\" >['Loomulikult']</td>\n",
       "      <td id=\"T_2b4ed_row505_col1\" class=\"data row505 col1\" >['▁Loomulikult']</td>\n",
       "      <td id=\"T_2b4ed_row505_col2\" class=\"data row505 col2\" >['D']</td>\n",
       "      <td id=\"T_2b4ed_row505_col3\" class=\"data row505 col3\" >[0.99989]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row506_col0\" class=\"data row506 col0\" >['jääb']</td>\n",
       "      <td id=\"T_2b4ed_row506_col1\" class=\"data row506 col1\" >['▁jääb']</td>\n",
       "      <td id=\"T_2b4ed_row506_col2\" class=\"data row506 col2\" >['b_V']</td>\n",
       "      <td id=\"T_2b4ed_row506_col3\" class=\"data row506 col3\" >[0.99993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row507_col0\" class=\"data row507 col0\" >['see']</td>\n",
       "      <td id=\"T_2b4ed_row507_col1\" class=\"data row507 col1\" >['▁see']</td>\n",
       "      <td id=\"T_2b4ed_row507_col2\" class=\"data row507 col2\" >['sg n_P']</td>\n",
       "      <td id=\"T_2b4ed_row507_col3\" class=\"data row507 col3\" >[0.99986]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row508_col0\" class=\"data row508 col0\" >['sõbralik']</td>\n",
       "      <td id=\"T_2b4ed_row508_col1\" class=\"data row508 col1\" >['▁sõbralik']</td>\n",
       "      <td id=\"T_2b4ed_row508_col2\" class=\"data row508 col2\" >['sg n_A']</td>\n",
       "      <td id=\"T_2b4ed_row508_col3\" class=\"data row508 col3\" >[0.99977]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row509_col0\" class=\"data row509 col0\" >['soovitus']</td>\n",
       "      <td id=\"T_2b4ed_row509_col1\" class=\"data row509 col1\" >['▁soovitus']</td>\n",
       "      <td id=\"T_2b4ed_row509_col2\" class=\"data row509 col2\" >['sg n_S']</td>\n",
       "      <td id=\"T_2b4ed_row509_col3\" class=\"data row509 col3\" >[0.99969]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row510_col0\" class=\"data row510 col0\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row510_col1\" class=\"data row510 col1\" >[',']</td>\n",
       "      <td id=\"T_2b4ed_row510_col2\" class=\"data row510 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row510_col3\" class=\"data row510 col3\" >[0.99997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row511_col0\" class=\"data row511 col0\" >['kolleegide']</td>\n",
       "      <td id=\"T_2b4ed_row511_col1\" class=\"data row511 col1\" >['▁kolleegide']</td>\n",
       "      <td id=\"T_2b4ed_row511_col2\" class=\"data row511 col2\" >['pl g_S']</td>\n",
       "      <td id=\"T_2b4ed_row511_col3\" class=\"data row511 col3\" >[0.99963]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row512_col0\" class=\"data row512 col0\" >['vahele']</td>\n",
       "      <td id=\"T_2b4ed_row512_col1\" class=\"data row512 col1\" >['▁vahele']</td>\n",
       "      <td id=\"T_2b4ed_row512_col2\" class=\"data row512 col2\" >['K']</td>\n",
       "      <td id=\"T_2b4ed_row512_col3\" class=\"data row512 col3\" >[0.99958]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2b4ed_row513_col0\" class=\"data row513 col0\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row513_col1\" class=\"data row513 col1\" >['.']</td>\n",
       "      <td id=\"T_2b4ed_row513_col2\" class=\"data row513 col2\" >['Z']</td>\n",
       "      <td id=\"T_2b4ed_row513_col3\" class=\"data row513 col3\" >[0.99996]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "Layer(name='bert_morph_tagging', attributes=('bert_tokens', 'morph_labels', 'probabilities'), spans=SL[EnvelopingSpan(['Tuvastamata'], [{'bert_tokens': ['▁Tu', 'vasta', 'mata'], 'morph_labels': ['A'], 'probabilities': [0.95848]}]),\n",
       "EnvelopingSpan(['Kasutaja'], [{'bert_tokens': ['▁Kasutaja'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99825]}]),\n",
       "EnvelopingSpan(['27.04.2009'], [{'bert_tokens': ['▁27', '.04.2009'], 'morph_labels': ['?_N'], 'probabilities': [0.99323]}]),\n",
       "EnvelopingSpan(['14:42'], [{'bert_tokens': ['▁14', ':', '42'], 'morph_labels': ['?_N'], 'probabilities': [0.9991]}]),\n",
       "EnvelopingSpan(['Venemaa'], [{'bert_tokens': ['▁Venemaa'], 'morph_labels': ['sg n_H'], 'probabilities': [0.99944]}]),\n",
       "EnvelopingSpan(['käitub'], [{'bert_tokens': ['▁käitub'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['nii'], [{'bert_tokens': ['▁nii'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['nagu'], [{'bert_tokens': ['▁nagu'], 'morph_labels': ['J'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['oleks'], [{'bert_tokens': ['▁oleks'], 'morph_labels': ['ks_V'], 'probabilities': [0.9997]}]),\n",
       "EnvelopingSpan(['võit'], [{'bert_tokens': ['▁võit'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99979]}]),\n",
       "EnvelopingSpan(['natsismi'], [{'bert_tokens': ['▁natsis', 'mi'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99961]}]),\n",
       "EnvelopingSpan(['üle'], [{'bert_tokens': ['▁üle'], 'morph_labels': ['K'], 'probabilities': [0.99936]}]),\n",
       "EnvelopingSpan(['vaid'], [{'bert_tokens': ['▁vaid'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['tema'], [{'bert_tokens': ['▁tema'], 'morph_labels': ['sg g_P'], 'probabilities': [0.99969]}]),\n",
       "EnvelopingSpan(['teene'], [{'bert_tokens': ['▁teene'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99976]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Ometi'], [{'bert_tokens': ['▁Ometi'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['olid'], [{'bert_tokens': ['▁olid'], 'morph_labels': ['sid_V'], 'probabilities': [0.99983]}]),\n",
       "EnvelopingSpan(['tal'], [{'bert_tokens': ['▁tal'], 'morph_labels': ['sg ad_P'], 'probabilities': [0.99972]}]),\n",
       "EnvelopingSpan(['ju'], [{'bert_tokens': ['▁ju'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['liitlased'], [{'bert_tokens': ['▁liitlased'], 'morph_labels': ['pl n_S'], 'probabilities': [0.99958]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['see'], [{'bert_tokens': ['▁see'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99986]}]),\n",
       "EnvelopingSpan(['võit'], [{'bert_tokens': ['▁võit'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99978]}]),\n",
       "EnvelopingSpan(['oli'], [{'bert_tokens': ['▁oli'], 'morph_labels': ['s_V'], 'probabilities': [0.99992]}]),\n",
       "EnvelopingSpan(['ikkagi'], [{'bert_tokens': ['▁ikkagi'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['rahvusvaheline'], [{'bert_tokens': ['▁rahvusvaheline'], 'morph_labels': ['sg n_A'], 'probabilities': [0.99976]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Nüüd'], [{'bert_tokens': ['▁Nüüd'], 'morph_labels': ['D'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['aga'], [{'bert_tokens': ['▁aga'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['surutakse'], [{'bert_tokens': ['▁surutakse'], 'morph_labels': ['takse_V'], 'probabilities': [0.9992]}]),\n",
       "EnvelopingSpan(['lausa'], [{'bert_tokens': ['▁lausa'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['kriminaalkoodeksi'], [{'bert_tokens': ['▁kriminaal', 'koodeksi'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99974]}]),\n",
       "EnvelopingSpan(['abil'], [{'bert_tokens': ['▁abil'], 'morph_labels': ['K'], 'probabilities': [0.99967]}]),\n",
       "EnvelopingSpan(['peale'], [{'bert_tokens': ['▁peale'], 'morph_labels': ['D'], 'probabilities': [0.99975]}]),\n",
       "EnvelopingSpan(['vene'], [{'bert_tokens': ['▁vene'], 'morph_labels': ['G'], 'probabilities': [0.9902]}]),\n",
       "EnvelopingSpan(['nägemust'], [{'bert_tokens': ['▁nägem', 'ust'], 'morph_labels': ['sg p_S'], 'probabilities': [0.99957]}]),\n",
       "EnvelopingSpan(['asjast'], [{'bert_tokens': ['▁asjast'], 'morph_labels': ['sg el_S'], 'probabilities': [0.99886]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['on'], [{'bert_tokens': ['▁on'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['loomisel'], [{'bert_tokens': ['▁loomisel'], 'morph_labels': ['sg ad_S'], 'probabilities': [0.99939]}]),\n",
       "EnvelopingSpan(['tribunal'], [{'bert_tokens': ['▁trib', 'unal'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99958]}]),\n",
       "EnvelopingSpan(['('], [{'bert_tokens': ['▁('], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Общественная'], [{'bert_tokens': ['О', 'б', 'щ', 'е', 'ст', 'в', 'ен', 'н', 'а', 'я'], 'morph_labels': ['sg g_H'], 'probabilities': [0.88898]}]),\n",
       "EnvelopingSpan(['комиссия'], [{'bert_tokens': ['к', 'о', 'м', 'и', 'с', 'с', 'и', 'я'], 'morph_labels': ['sg n_S'], 'probabilities': [0.9257]}]),\n",
       "EnvelopingSpan([')', ','], [{'bert_tokens': ['),'], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['mis'], [{'bert_tokens': ['▁mis'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99986]}]),\n",
       "EnvelopingSpan(['hakkab'], [{'bert_tokens': ['▁hakkab'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['inimeste'], [{'bert_tokens': ['▁inimeste'], 'morph_labels': ['pl g_S'], 'probabilities': [0.99938]}]),\n",
       "EnvelopingSpan(['('], [{'bert_tokens': ['▁('], 'morph_labels': ['Z'], 'probabilities': [0.99995]}]),\n",
       "EnvelopingSpan(['ka'], [{'bert_tokens': ['ka'], 'morph_labels': ['D'], 'probabilities': [0.99776]}]),\n",
       "EnvelopingSpan(['välismaalaste'], [{'bert_tokens': ['▁välismaalaste'], 'morph_labels': ['pl g_S'], 'probabilities': [0.99893]}]),\n",
       "EnvelopingSpan(['!', ')'], [{'bert_tokens': ['!)'], 'morph_labels': ['Z'], 'probabilities': [0.99986]}]),\n",
       "EnvelopingSpan(['üle'], [{'bert_tokens': ['▁üle'], 'morph_labels': ['K'], 'probabilities': [0.99715]}]),\n",
       "EnvelopingSpan(['otsustama'], [{'bert_tokens': ['▁otsustama'], 'morph_labels': ['ma_V'], 'probabilities': [0.99936]}]),\n",
       "EnvelopingSpan(['just'], [{'bert_tokens': ['▁just'], 'morph_labels': ['D'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['selle'], [{'bert_tokens': ['▁selle'], 'morph_labels': ['sg g_P'], 'probabilities': [0.9997]}]),\n",
       "EnvelopingSpan(['nägemuse'], [{'bert_tokens': ['▁nägem', 'use'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99973]}]),\n",
       "EnvelopingSpan(['alusel'], [{'bert_tokens': ['▁alusel'], 'morph_labels': ['sg ad_S'], 'probabilities': [0.99928]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['kaitsema'], [{'bert_tokens': ['▁kaitse', 'ma'], 'morph_labels': ['ma_V'], 'probabilities': [0.99823]}]),\n",
       "EnvelopingSpan(['venemaa'], [{'bert_tokens': ['▁venemaa'], 'morph_labels': ['sg g_S'], 'probabilities': [0.97306]}]),\n",
       "EnvelopingSpan(['seisukohti'], [{'bert_tokens': ['▁seisukohti'], 'morph_labels': ['pl p_S'], 'probabilities': [0.99967]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['või'], [{'bert_tokens': ['▁või'], 'morph_labels': ['J'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['mis'], [{'bert_tokens': ['▁mis'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99985]}]),\n",
       "EnvelopingSpan(['veel'], [{'bert_tokens': ['▁veel'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['hullem'], [{'bert_tokens': ['▁hullem'], 'morph_labels': ['sg n_C'], 'probabilities': [0.99744]}]),\n",
       "EnvelopingSpan(['–'], [{'bert_tokens': ['▁–'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['NSV'], [{'bert_tokens': ['▁NSV'], 'morph_labels': ['?_Y'], 'probabilities': [0.99383]}]),\n",
       "EnvelopingSpan(['Liidu'], [{'bert_tokens': ['▁Liidu'], 'morph_labels': ['sg g_H'], 'probabilities': [0.99649]}]),\n",
       "EnvelopingSpan(['seisukohti'], [{'bert_tokens': ['▁seisukohti'], 'morph_labels': ['pl p_S'], 'probabilities': [0.99966]}]),\n",
       "EnvelopingSpan(['!'], [{'bert_tokens': ['!'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Suure'], [{'bert_tokens': ['▁Suure'], 'morph_labels': ['sg g_A'], 'probabilities': [0.99954]}]),\n",
       "EnvelopingSpan(['sõja'], [{'bert_tokens': ['▁sõja'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99973]}]),\n",
       "EnvelopingSpan(['kohta'], [{'bert_tokens': ['▁kohta'], 'morph_labels': ['K'], 'probabilities': [0.99963]}]),\n",
       "EnvelopingSpan(['on'], [{'bert_tokens': ['▁on'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['Venemaal'], [{'bert_tokens': ['▁Venemaal'], 'morph_labels': ['sg ad_H'], 'probabilities': [0.99159]}]),\n",
       "EnvelopingSpan(['käibel'], [{'bert_tokens': ['▁käib', 'el'], 'morph_labels': ['sg ad_S'], 'probabilities': [0.84049]}]),\n",
       "EnvelopingSpan(['väga'], [{'bert_tokens': ['▁väga'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['palju'], [{'bert_tokens': ['▁palju'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['valesid'], [{'bert_tokens': ['▁valesid'], 'morph_labels': ['pl p_S'], 'probabilities': [0.99963]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['mis'], [{'bert_tokens': ['▁mis'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99986]}]),\n",
       "EnvelopingSpan(['on'], [{'bert_tokens': ['▁on'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['kivistunud'], [{'bert_tokens': ['▁kivist', 'unud'], 'morph_labels': ['nud_V'], 'probabilities': [0.78979]}]),\n",
       "EnvelopingSpan(['lausa'], [{'bert_tokens': ['▁lausa'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['müütideks'], [{'bert_tokens': ['▁müüt', 'ideks'], 'morph_labels': ['pl tr_S'], 'probabilities': [0.99579]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Nende'], [{'bert_tokens': ['▁Nende'], 'morph_labels': ['pl g_P'], 'probabilities': [0.99945]}]),\n",
       "EnvelopingSpan(['müütide'], [{'bert_tokens': ['▁müü', 'tide'], 'morph_labels': ['pl g_S'], 'probabilities': [0.99962]}]),\n",
       "EnvelopingSpan(['lõhkumine'], [{'bert_tokens': ['▁lõhku', 'mine'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99977]}]),\n",
       "EnvelopingSpan(['on'], [{'bert_tokens': ['▁on'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['nüüd'], [{'bert_tokens': ['▁nüüd'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['siis'], [{'bert_tokens': ['▁siis'], 'morph_labels': ['D'], 'probabilities': [0.99979]}]),\n",
       "EnvelopingSpan(['karistatav'], [{'bert_tokens': ['▁karistatav'], 'morph_labels': ['sg n_A'], 'probabilities': [0.99565]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Suur'], [{'bert_tokens': ['▁Suur'], 'morph_labels': ['sg n_A'], 'probabilities': [0.99975]}]),\n",
       "EnvelopingSpan(['löök'], [{'bert_tokens': ['▁löök'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99972]}]),\n",
       "EnvelopingSpan(['sõnavabadusele'], [{'bert_tokens': ['▁sõna', 'vabad', 'usele'], 'morph_labels': ['sg all_S'], 'probabilities': [0.99909]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['tõhus'], [{'bert_tokens': ['▁tõhus'], 'morph_labels': ['sg n_A'], 'probabilities': [0.99977]}]),\n",
       "EnvelopingSpan(['tugi'], [{'bert_tokens': ['▁tugi'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99963]}]),\n",
       "EnvelopingSpan(['valede'], [{'bert_tokens': ['▁valede'], 'morph_labels': ['pl g_S'], 'probabilities': [0.99957]}]),\n",
       "EnvelopingSpan(['püsimajäämisele'], [{'bert_tokens': ['▁püsima', 'jää', 'misele'], 'morph_labels': ['sg all_S'], 'probabilities': [0.99932]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Aga'], [{'bert_tokens': ['▁Aga'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['äkki'], [{'bert_tokens': ['▁äkki'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['tuleks'], [{'bert_tokens': ['▁tuleks'], 'morph_labels': ['ks_V'], 'probabilities': [0.9997]}]),\n",
       "EnvelopingSpan(['natsismi'], [{'bert_tokens': ['▁natsis', 'mi'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99961]}]),\n",
       "EnvelopingSpan(['võitmist'], [{'bert_tokens': ['▁võit', 'mist'], 'morph_labels': ['sg p_S'], 'probabilities': [0.99962]}]),\n",
       "EnvelopingSpan(['hinnata'], [{'bert_tokens': ['▁hinnata'], 'morph_labels': ['da_V'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['ikka'], [{'bert_tokens': ['▁ikka'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['kõigi'], [{'bert_tokens': ['▁kõigi'], 'morph_labels': ['pl g_P'], 'probabilities': [0.9994]}]),\n",
       "EnvelopingSpan(['võitjate'], [{'bert_tokens': ['▁võitjate'], 'morph_labels': ['pl g_S'], 'probabilities': [0.99958]}]),\n",
       "EnvelopingSpan(['seisukohast'], [{'bert_tokens': ['▁seisukohast'], 'morph_labels': ['sg el_S'], 'probabilities': [0.99911]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['mitte'], [{'bert_tokens': ['▁mitte'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['ainult'], [{'bert_tokens': ['▁ainult'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['vene'], [{'bert_tokens': ['▁vene'], 'morph_labels': ['G'], 'probabilities': [0.86184]}]),\n",
       "EnvelopingSpan(['omast'], [{'bert_tokens': ['▁omast'], 'morph_labels': ['sg el_P'], 'probabilities': [0.99855]}]),\n",
       "EnvelopingSpan(['?'], [{'bert_tokens': ['?'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Ehk'], [{'bert_tokens': ['▁Ehk'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['tuleks'], [{'bert_tokens': ['▁tuleks'], 'morph_labels': ['ks_V'], 'probabilities': [0.99971]}]),\n",
       "EnvelopingSpan(['lugeda'], [{'bert_tokens': ['▁lugeda'], 'morph_labels': ['da_V'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['karistamist'], [{'bert_tokens': ['▁karistamist'], 'morph_labels': ['sg p_S'], 'probabilities': [0.99964]}]),\n",
       "EnvelopingSpan(['väärivaks'], [{'bert_tokens': ['▁vääri', 'vaks'], 'morph_labels': ['sg tr_A'], 'probabilities': [0.99898]}]),\n",
       "EnvelopingSpan(['ameeriklaste'], [{'bert_tokens': ['▁ameeriklaste'], 'morph_labels': ['pl g_S'], 'probabilities': [0.9994]}]),\n",
       "EnvelopingSpan([\"lend-lease'i\"], [{'bert_tokens': ['▁lend', '-', 'le', 'ase', \"'\", 'i'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99944]}]),\n",
       "EnvelopingSpan(['alahindamine'], [{'bert_tokens': ['▁ala', 'hinda', 'mine'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99975]}]),\n",
       "EnvelopingSpan(['ja'], [{'bert_tokens': ['▁ja'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['kohati'], [{'bert_tokens': ['▁kohati'], 'morph_labels': ['D'], 'probabilities': [0.99988]}]),\n",
       "EnvelopingSpan(['lausa'], [{'bert_tokens': ['▁lausa'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['naeruvääristamine'], [{'bert_tokens': ['▁naeruvää', 'rista', 'mine'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99974]}]),\n",
       "EnvelopingSpan(['?'], [{'bert_tokens': ['?'], 'morph_labels': ['Z'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['Või'], [{'bert_tokens': ['▁Või'], 'morph_labels': ['J'], 'probabilities': [0.99985]}]),\n",
       "EnvelopingSpan(['vaadata'], [{'bert_tokens': ['▁vaadata'], 'morph_labels': ['da_V'], 'probabilities': [0.99988]}]),\n",
       "EnvelopingSpan(['hoopis'], [{'bert_tokens': ['▁hoopis'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['teise'], [{'bert_tokens': ['▁teise'], 'morph_labels': ['sg g_P'], 'probabilities': [0.99969]}]),\n",
       "EnvelopingSpan(['pilguga'], [{'bert_tokens': ['▁pilguga'], 'morph_labels': ['sg kom_S'], 'probabilities': [0.9997]}]),\n",
       "EnvelopingSpan(['Suurbritannia'], [{'bert_tokens': ['▁Suurbritannia'], 'morph_labels': ['sg g_H'], 'probabilities': [0.99861]}]),\n",
       "EnvelopingSpan(['sõja'], [{'bert_tokens': ['▁sõja'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99964]}]),\n",
       "EnvelopingSpan(['algaastate'], [{'bert_tokens': ['▁alga', 'asta', 'te'], 'morph_labels': ['pl g_S'], 'probabilities': [0.99917]}]),\n",
       "EnvelopingSpan(['rolli'], [{'bert_tokens': ['▁rolli'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99964]}]),\n",
       "EnvelopingSpan(['alaväärstamisele'], [{'bert_tokens': ['▁alaväär', 'sta', 'misele'], 'morph_labels': ['sg all_S'], 'probabilities': [0.99932]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['võttis'], [{'bert_tokens': ['▁võttis'], 'morph_labels': ['s_V'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['ju'], [{'bert_tokens': ['▁ju'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['tema'], [{'bert_tokens': ['▁tema'], 'morph_labels': ['sg g_P'], 'probabilities': [0.99972]}]),\n",
       "EnvelopingSpan(['vastu'], [{'bert_tokens': ['▁vastu'], 'morph_labels': ['K'], 'probabilities': [0.99963]}]),\n",
       "EnvelopingSpan(['sakslaste'], [{'bert_tokens': ['▁sakslaste'], 'morph_labels': ['pl g_S'], 'probabilities': [0.99945]}]),\n",
       "EnvelopingSpan(['põhilöögi'], [{'bert_tokens': ['▁põh', 'il', 'öögi'], 'morph_labels': ['sg g_S'], 'probabilities': [0.9996]}]),\n",
       "EnvelopingSpan(['?'], [{'bert_tokens': ['?'], 'morph_labels': ['Z'], 'probabilities': [0.99994]}]),\n",
       "EnvelopingSpan(['Vastust'], [{'bert_tokens': ['▁Vast', 'ust'], 'morph_labels': ['sg p_S'], 'probabilities': [0.99937]}]),\n",
       "EnvelopingSpan(['tuleks'], [{'bert_tokens': ['▁tuleks'], 'morph_labels': ['ks_V'], 'probabilities': [0.99972]}]),\n",
       "EnvelopingSpan(['nõuda'], [{'bert_tokens': ['▁nõuda'], 'morph_labels': ['da_V'], 'probabilities': [0.99988]}]),\n",
       "EnvelopingSpan(['ka'], [{'bert_tokens': ['▁ka'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['nendelt'], [{'bert_tokens': ['▁nendelt'], 'morph_labels': ['pl abl_P'], 'probabilities': [0.92749]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['kes'], [{'bert_tokens': ['▁kes'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99986]}]),\n",
       "EnvelopingSpan(['lasevad'], [{'bert_tokens': ['▁lasevad'], 'morph_labels': ['vad_V'], 'probabilities': [0.99948]}]),\n",
       "EnvelopingSpan(['välja'], [{'bert_tokens': ['▁välja'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['paista'], [{'bert_tokens': ['▁paista'], 'morph_labels': ['da_V'], 'probabilities': [0.99981]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['nagu'], [{'bert_tokens': ['▁nagu'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['oleks'], [{'bert_tokens': ['▁oleks'], 'morph_labels': ['ks_V'], 'probabilities': [0.99973]}]),\n",
       "EnvelopingSpan(['Hitleri'], [{'bert_tokens': ['▁Hitleri'], 'morph_labels': ['sg g_H'], 'probabilities': [0.9988]}]),\n",
       "EnvelopingSpan(['vastu'], [{'bert_tokens': ['▁vastu'], 'morph_labels': ['K'], 'probabilities': [0.9986]}]),\n",
       "EnvelopingSpan(['sõdinud'], [{'bert_tokens': ['▁sõd', 'inud'], 'morph_labels': ['nud_V'], 'probabilities': [0.97377]}]),\n",
       "EnvelopingSpan(['('], [{'bert_tokens': ['▁('], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['ja'], [{'bert_tokens': ['ja'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['teda'], [{'bert_tokens': ['▁teda'], 'morph_labels': ['sg p_P'], 'probabilities': [0.99981]}]),\n",
       "EnvelopingSpan(['võitnud'], [{'bert_tokens': ['▁võitnud'], 'morph_labels': ['nud_V'], 'probabilities': [0.99929]}]),\n",
       "EnvelopingSpan([')'], [{'bert_tokens': [')'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['vaid'], [{'bert_tokens': ['▁vaid'], 'morph_labels': ['D'], 'probabilities': [0.99985]}]),\n",
       "EnvelopingSpan(['Nõukogude'], [{'bert_tokens': ['▁Nõukogude'], 'morph_labels': ['pl g_H'], 'probabilities': [0.94688]}]),\n",
       "EnvelopingSpan(['Liit'], [{'bert_tokens': ['▁Liit'], 'morph_labels': ['sg n_H'], 'probabilities': [0.99914]}]),\n",
       "EnvelopingSpan(['?'], [{'bert_tokens': ['?'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Nii'], [{'bert_tokens': ['▁Nii'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['võib'], [{'bert_tokens': ['▁võib'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['mõõgatera'], [{'bert_tokens': ['▁mõõ', 'ga', 'tera'], 'morph_labels': ['sg n_S'], 'probabilities': [0.9995]}]),\n",
       "EnvelopingSpan(['pöörduda'], [{'bert_tokens': ['▁pöörduda'], 'morph_labels': ['da_V'], 'probabilities': [0.99988]}]),\n",
       "EnvelopingSpan(['hoopis'], [{'bert_tokens': ['▁hoopis'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['iseenda'], [{'bert_tokens': ['▁iseenda'], 'morph_labels': ['sg g_P'], 'probabilities': [0.99967]}]),\n",
       "EnvelopingSpan(['vastu'], [{'bert_tokens': ['▁vastu'], 'morph_labels': ['K'], 'probabilities': [0.99968]}]),\n",
       "EnvelopingSpan(['!'], [{'bert_tokens': ['!'], 'morph_labels': ['Z'], 'probabilities': [0.99994]}]),\n",
       "EnvelopingSpan(['Tuvastamata'], [{'bert_tokens': ['▁Tu', 'vasta', 'mata'], 'morph_labels': ['A'], 'probabilities': [0.95848]}]),\n",
       "EnvelopingSpan(['Kasutaja'], [{'bert_tokens': ['▁Kasutaja'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99825]}]),\n",
       "EnvelopingSpan(['Suure'], [{'bert_tokens': ['▁Suure'], 'morph_labels': ['sg g_A'], 'probabilities': [0.99936]}]),\n",
       "EnvelopingSpan(['Isamaasõja'], [{'bert_tokens': ['▁Isa', 'maas', 'õja'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99969]}]),\n",
       "EnvelopingSpan(['tulemuste'], [{'bert_tokens': ['▁tulemuste'], 'morph_labels': ['pl g_S'], 'probabilities': [0.99848]}]),\n",
       "EnvelopingSpan(['eitamise'], [{'bert_tokens': ['▁ei', 'tamise'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99963]}]),\n",
       "EnvelopingSpan(['või'], [{'bert_tokens': ['▁või'], 'morph_labels': ['J'], 'probabilities': [0.99987]}]),\n",
       "EnvelopingSpan(['revideerimise'], [{'bert_tokens': ['▁revid', 'eerimise'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99964]}]),\n",
       "EnvelopingSpan(['eest'], [{'bert_tokens': ['▁eest'], 'morph_labels': ['K'], 'probabilities': [0.99969]}]),\n",
       "EnvelopingSpan(['on'], [{'bert_tokens': ['▁on'], 'morph_labels': ['b_V'], 'probabilities': [0.99992]}]),\n",
       "EnvelopingSpan(['ette'], [{'bert_tokens': ['▁ette'], 'morph_labels': ['D'], 'probabilities': [0.99987]}]),\n",
       "EnvelopingSpan(['nähtud'], [{'bert_tokens': ['▁nähtud'], 'morph_labels': ['tud_V'], 'probabilities': [0.99752]}]),\n",
       "EnvelopingSpan(['kriminaalkaristus'], [{'bert_tokens': ['▁kriminaal', 'karistus'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99957]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Aga'], [{'bert_tokens': ['▁Aga'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['millised'], [{'bert_tokens': ['▁millised'], 'morph_labels': ['pl n_P'], 'probabilities': [0.9998]}]),\n",
       "EnvelopingSpan(['olid'], [{'bert_tokens': ['▁olid'], 'morph_labels': ['sid_V'], 'probabilities': [0.99982]}]),\n",
       "EnvelopingSpan(['selle'], [{'bert_tokens': ['▁selle'], 'morph_labels': ['sg g_P'], 'probabilities': [0.99971]}]),\n",
       "EnvelopingSpan(['sõja'], [{'bert_tokens': ['▁sõja'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99971]}]),\n",
       "EnvelopingSpan(['tulemused'], [{'bert_tokens': ['▁tulemused'], 'morph_labels': ['pl n_S'], 'probabilities': [0.99957]}]),\n",
       "EnvelopingSpan(['?'], [{'bert_tokens': ['?'], 'morph_labels': ['Z'], 'probabilities': [0.99994]}]),\n",
       "EnvelopingSpan(['Tulemus'], [{'bert_tokens': ['▁Tulemus'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99976]}]),\n",
       "EnvelopingSpan(['oli'], [{'bert_tokens': ['▁oli'], 'morph_labels': ['s_V'], 'probabilities': [0.99992]}]),\n",
       "EnvelopingSpan(['see'], [{'bert_tokens': ['▁see'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99985]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['et'], [{'bert_tokens': ['▁et'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['Nõukogude'], [{'bert_tokens': ['▁Nõukogude'], 'morph_labels': ['pl g_S'], 'probabilities': [0.78333]}]),\n",
       "EnvelopingSpan(['Liit'], [{'bert_tokens': ['▁Liit'], 'morph_labels': ['sg n_H'], 'probabilities': [0.98783]}]),\n",
       "EnvelopingSpan(['haaras'], [{'bert_tokens': ['▁haaras'], 'morph_labels': ['s_V'], 'probabilities': [0.99992]}]),\n",
       "EnvelopingSpan(['enda'], [{'bert_tokens': ['▁enda'], 'morph_labels': ['sg g_P'], 'probabilities': [0.99957]}]),\n",
       "EnvelopingSpan(['alla'], [{'bert_tokens': ['▁alla'], 'morph_labels': ['K'], 'probabilities': [0.99967]}]),\n",
       "EnvelopingSpan(['pool'], [{'bert_tokens': ['▁pool'], 'morph_labels': ['sg n_N'], 'probabilities': [0.99916]}]),\n",
       "EnvelopingSpan(['Euroopat'], [{'bert_tokens': ['▁Euroopat'], 'morph_labels': ['sg p_H'], 'probabilities': [0.97531]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['('], [{'bert_tokens': ['▁('], 'morph_labels': ['Z'], 'probabilities': [0.99995]}]),\n",
       "EnvelopingSpan(['Teised'], [{'bert_tokens': ['Te', 'ised'], 'morph_labels': ['pl n_P'], 'probabilities': [0.99978]}]),\n",
       "EnvelopingSpan(['võitjariigid'], [{'bert_tokens': ['▁võitja', 'riigid'], 'morph_labels': ['pl n_S'], 'probabilities': [0.99964]}]),\n",
       "EnvelopingSpan(['oma'], [{'bert_tokens': ['▁oma'], 'morph_labels': ['sg g_P'], 'probabilities': [0.99969]}]),\n",
       "EnvelopingSpan(['territooriumi'], [{'bert_tokens': ['▁territooriumi'], 'morph_labels': ['sg p_S'], 'probabilities': [0.99716]}]),\n",
       "EnvelopingSpan(['ei'], [{'bert_tokens': ['▁ei'], 'morph_labels': ['neg_V'], 'probabilities': [0.99988]}]),\n",
       "EnvelopingSpan(['laiendanud'], [{'bert_tokens': ['▁lai', 'endanud'], 'morph_labels': ['nud_V'], 'probabilities': [0.99951]}]),\n",
       "EnvelopingSpan(['.', ')'], [{'bert_tokens': ['.)'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Nüüd'], [{'bert_tokens': ['▁Nüüd'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['aga'], [{'bert_tokens': ['▁aga'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['on'], [{'bert_tokens': ['▁on'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['tegelik'], [{'bert_tokens': ['▁tegelik'], 'morph_labels': ['sg n_A'], 'probabilities': [0.99974]}]),\n",
       "EnvelopingSpan(['elu'], [{'bert_tokens': ['▁elu'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99973]}]),\n",
       "EnvelopingSpan(['ja'], [{'bert_tokens': ['▁ja'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['ajaloo'], [{'bert_tokens': ['▁ajaloo'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99969]}]),\n",
       "EnvelopingSpan(['kulg'], [{'bert_tokens': ['▁kul', 'g'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99979]}]),\n",
       "EnvelopingSpan(['ise'], [{'bert_tokens': ['▁ise'], 'morph_labels': ['sg n_P'], 'probabilities': [0.9993]}]),\n",
       "EnvelopingSpan(['revideerinud'], [{'bert_tokens': ['▁revid', 'eerinud'], 'morph_labels': ['nud_V'], 'probabilities': [0.99975]}]),\n",
       "EnvelopingSpan(['neid'], [{'bert_tokens': ['▁neid'], 'morph_labels': ['pl p_P'], 'probabilities': [0.99976]}]),\n",
       "EnvelopingSpan(['tulemusi'], [{'bert_tokens': ['▁tulemusi'], 'morph_labels': ['pl p_S'], 'probabilities': [0.99962]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Stalini'], [{'bert_tokens': ['▁Stalini'], 'morph_labels': ['sg g_H'], 'probabilities': [0.99906]}]),\n",
       "EnvelopingSpan(['vallutused'], [{'bert_tokens': ['▁vall', 'utused'], 'morph_labels': ['pl n_S'], 'probabilities': [0.99962]}]),\n",
       "EnvelopingSpan(['on'], [{'bert_tokens': ['▁on'], 'morph_labels': ['b_V'], 'probabilities': [0.99992]}]),\n",
       "EnvelopingSpan(['venelastele'], [{'bert_tokens': ['▁venelastele'], 'morph_labels': ['pl all_S'], 'probabilities': [0.99907]}]),\n",
       "EnvelopingSpan(['suuremalt'], [{'bert_tokens': ['▁suuremalt'], 'morph_labels': ['sg abl_A'], 'probabilities': [0.1976]}]),\n",
       "EnvelopingSpan(['jaolt'], [{'bert_tokens': ['▁jaolt'], 'morph_labels': ['sg abl_S'], 'probabilities': [0.99423]}]),\n",
       "EnvelopingSpan(['kaduma'], [{'bert_tokens': ['▁kaduma'], 'morph_labels': ['ma_V'], 'probabilities': [0.99923]}]),\n",
       "EnvelopingSpan(['läinud'], [{'bert_tokens': ['▁läinud'], 'morph_labels': ['nud_V'], 'probabilities': [0.99973]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['See'], [{'bert_tokens': ['▁See'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99985]}]),\n",
       "EnvelopingSpan(['on'], [{'bert_tokens': ['▁on'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['asjade'], [{'bert_tokens': ['▁asjade'], 'morph_labels': ['pl g_S'], 'probabilities': [0.99962]}]),\n",
       "EnvelopingSpan(['tegelik'], [{'bert_tokens': ['▁tegelik'], 'morph_labels': ['sg n_A'], 'probabilities': [0.99976]}]),\n",
       "EnvelopingSpan(['areng'], [{'bert_tokens': ['▁areng'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99975]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Uue'], [{'bert_tokens': ['▁Uue'], 'morph_labels': ['sg g_A'], 'probabilities': [0.99958]}]),\n",
       "EnvelopingSpan(['seaduse'], [{'bert_tokens': ['▁seaduse'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99973]}]),\n",
       "EnvelopingSpan(['paatos'], [{'bert_tokens': ['▁paat', 'os'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99915]}]),\n",
       "EnvelopingSpan(['on'], [{'bert_tokens': ['▁on'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['aga'], [{'bert_tokens': ['▁aga'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['suunatud'], [{'bert_tokens': ['▁suunatud'], 'morph_labels': ['A'], 'probabilities': [0.99824]}]),\n",
       "EnvelopingSpan(['selle'], [{'bert_tokens': ['▁selle'], 'morph_labels': ['sg g_P'], 'probabilities': [0.9997]}]),\n",
       "EnvelopingSpan(['arengu'], [{'bert_tokens': ['▁arengu'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99972]}]),\n",
       "EnvelopingSpan(['vastu'], [{'bert_tokens': ['▁vastu'], 'morph_labels': ['K'], 'probabilities': [0.99965]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['sisuliselt'], [{'bert_tokens': ['▁sisuliselt'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['eitades'], [{'bert_tokens': ['▁ei', 'tades'], 'morph_labels': ['des_V'], 'probabilities': [0.99931]}]),\n",
       "EnvelopingSpan(['nende'], [{'bert_tokens': ['▁nende'], 'morph_labels': ['pl g_P'], 'probabilities': [0.99941]}]),\n",
       "EnvelopingSpan(['alade'], [{'bert_tokens': ['▁alade'], 'morph_labels': ['pl g_S'], 'probabilities': [0.99958]}]),\n",
       "EnvelopingSpan(['vabanemist'], [{'bert_tokens': ['▁vabanemist'], 'morph_labels': ['sg p_S'], 'probabilities': [0.99966]}]),\n",
       "EnvelopingSpan(['ja'], [{'bert_tokens': ['▁ja'], 'morph_labels': ['J'], 'probabilities': [0.99992]}]),\n",
       "EnvelopingSpan(['kihutades'], [{'bert_tokens': ['▁kih', 'utades'], 'morph_labels': ['des_V'], 'probabilities': [0.99934]}]),\n",
       "EnvelopingSpan(['uuele'], [{'bert_tokens': ['▁uuele'], 'morph_labels': ['sg all_A'], 'probabilities': [0.99792]}]),\n",
       "EnvelopingSpan(['ekspansioonile'], [{'bert_tokens': ['▁eks', 'pan', 'sioonile'], 'morph_labels': ['sg all_S'], 'probabilities': [0.99944]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['enda'], [{'bert_tokens': ['▁enda'], 'morph_labels': ['sg g_P'], 'probabilities': [0.99967]}]),\n",
       "EnvelopingSpan(['alla'], [{'bert_tokens': ['▁alla'], 'morph_labels': ['K'], 'probabilities': [0.99967]}]),\n",
       "EnvelopingSpan(['heitma'], [{'bert_tokens': ['▁heit', 'ma'], 'morph_labels': ['ma_V'], 'probabilities': [0.99859]}]),\n",
       "EnvelopingSpan(['neid'], [{'bert_tokens': ['▁neid'], 'morph_labels': ['pl p_P'], 'probabilities': [0.99978]}]),\n",
       "EnvelopingSpan(['territooriume'], [{'bert_tokens': ['▁territoori', 'ume'], 'morph_labels': ['pl p_S'], 'probabilities': [0.99965]}]),\n",
       "EnvelopingSpan(['ning'], [{'bert_tokens': ['▁ning'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['saavutama'], [{'bert_tokens': ['▁saav', 'utama'], 'morph_labels': ['ma_V'], 'probabilities': [0.9973]}]),\n",
       "EnvelopingSpan(['kontrolli'], [{'bert_tokens': ['▁kontrolli'], 'morph_labels': ['sg g_S'], 'probabilities': [0.94327]}]),\n",
       "EnvelopingSpan(['seal'], [{'bert_tokens': ['▁seal'], 'morph_labels': ['D'], 'probabilities': [0.99988]}]),\n",
       "EnvelopingSpan(['elavate'], [{'bert_tokens': ['▁elavate'], 'morph_labels': ['pl g_A'], 'probabilities': [0.99883]}]),\n",
       "EnvelopingSpan(['inimeste'], [{'bert_tokens': ['▁inimeste'], 'morph_labels': ['pl g_S'], 'probabilities': [0.9996]}]),\n",
       "EnvelopingSpan(['üle'], [{'bert_tokens': ['▁üle'], 'morph_labels': ['K'], 'probabilities': [0.9997]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Mina'], [{'bert_tokens': ['▁Mina'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99985]}]),\n",
       "EnvelopingSpan(['nimetaksin'], [{'bert_tokens': ['▁nime', 'taksin'], 'morph_labels': ['ksin_V'], 'probabilities': [0.9941]}]),\n",
       "EnvelopingSpan(['seda'], [{'bert_tokens': ['▁seda'], 'morph_labels': ['sg p_P'], 'probabilities': [0.99982]}]),\n",
       "EnvelopingSpan(['äärmiselt'], [{'bert_tokens': ['▁äärmiselt'], 'morph_labels': ['D'], 'probabilities': [0.99987]}]),\n",
       "EnvelopingSpan(['kurjakuulutavaks'], [{'bert_tokens': ['▁kurja', 'kuul', 'utavaks'], 'morph_labels': ['sg tr_A'], 'probabilities': [0.99904]}]),\n",
       "EnvelopingSpan(['arenguks'], [{'bert_tokens': ['▁arenguks'], 'morph_labels': ['sg tr_S'], 'probabilities': [0.99906]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Tuvastamata'], [{'bert_tokens': ['▁Tu', 'vasta', 'mata'], 'morph_labels': ['A'], 'probabilities': [0.95848]}]),\n",
       "EnvelopingSpan(['Kasutaja'], [{'bert_tokens': ['▁Kasutaja'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99825]}]),\n",
       "EnvelopingSpan(['Ega'], [{'bert_tokens': ['▁Ega'], 'morph_labels': ['D'], 'probabilities': [0.95002]}]),\n",
       "EnvelopingSpan(['selle'], [{'bert_tokens': ['▁selle'], 'morph_labels': ['sg g_P'], 'probabilities': [0.99969]}]),\n",
       "EnvelopingSpan(['väikese'], [{'bert_tokens': ['▁väikese'], 'morph_labels': ['sg g_A'], 'probabilities': [0.99962]}]),\n",
       "EnvelopingSpan(['asja'], [{'bert_tokens': ['▁asja'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99969]}]),\n",
       "EnvelopingSpan(['taha'], [{'bert_tokens': ['▁taha'], 'morph_labels': ['K'], 'probabilities': [0.9996]}]),\n",
       "EnvelopingSpan(['tohi'], [{'bert_tokens': ['▁tohi'], 'morph_labels': ['o_V'], 'probabilities': [0.99913]}]),\n",
       "EnvelopingSpan(['ometi'], [{'bert_tokens': ['▁ometi'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['sõit'], [{'bert_tokens': ['▁sõit'], 'morph_labels': ['sg n_S'], 'probabilities': [0.9998]}]),\n",
       "EnvelopingSpan(['seisma'], [{'bert_tokens': ['▁seisma'], 'morph_labels': ['ma_V'], 'probabilities': [0.99935]}]),\n",
       "EnvelopingSpan(['jääda'], [{'bert_tokens': ['▁jääda'], 'morph_labels': ['da_V'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['!'], [{'bert_tokens': ['!'], 'morph_labels': ['Z'], 'probabilities': [0.99995]}]),\n",
       "EnvelopingSpan(['Siis'], [{'bert_tokens': ['▁Siis'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['saaksid'], [{'bert_tokens': ['▁saaksid'], 'morph_labels': ['ksid_V'], 'probabilities': [0.99875]}]),\n",
       "EnvelopingSpan(['kremli'], [{'bert_tokens': ['▁kremli'], 'morph_labels': ['sg g_S'], 'probabilities': [0.96061]}]),\n",
       "EnvelopingSpan(['klähvitsad'], [{'bert_tokens': ['▁kl', 'ä', 'hv', 'itsad'], 'morph_labels': ['pl n_S'], 'probabilities': [0.99965]}]),\n",
       "EnvelopingSpan(['oma'], [{'bert_tokens': ['▁oma'], 'morph_labels': ['sg g_P'], 'probabilities': [0.9996]}]),\n",
       "EnvelopingSpan(['tahtmise'], [{'bert_tokens': ['▁tahtmise'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99962]}]),\n",
       "EnvelopingSpan(['küll'], [{'bert_tokens': ['▁küll'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['liiga'], [{'bert_tokens': ['▁liiga'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['kergelt'], [{'bert_tokens': ['▁kergelt'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['kätte'], [{'bert_tokens': ['▁kätte'], 'morph_labels': ['adt_S'], 'probabilities': [0.99885]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['kui'], [{'bert_tokens': ['▁kui'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['ma'], [{'bert_tokens': ['▁ma'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99983]}]),\n",
       "EnvelopingSpan(['neile'], [{'bert_tokens': ['▁neile'], 'morph_labels': ['pl all_P'], 'probabilities': [0.99909]}]),\n",
       "EnvelopingSpan(['tähelepanu'], [{'bert_tokens': ['▁tähelepanu'], 'morph_labels': ['sg p_S'], 'probabilities': [0.99949]}]),\n",
       "EnvelopingSpan(['pööraksin'], [{'bert_tokens': ['▁pööra', 'ksin'], 'morph_labels': ['ksin_V'], 'probabilities': [0.99462]}]),\n",
       "EnvelopingSpan(['ja'], [{'bert_tokens': ['▁ja'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['püssi'], [{'bert_tokens': ['▁püssi'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99963]}]),\n",
       "EnvelopingSpan(['põõsasse'], [{'bert_tokens': ['▁põõ', 'sasse'], 'morph_labels': ['sg ill_S'], 'probabilities': [0.99664]}]),\n",
       "EnvelopingSpan(['viskaksin'], [{'bert_tokens': ['▁vis', 'kaks', 'in'], 'morph_labels': ['ksin_V'], 'probabilities': [0.994]}]),\n",
       "EnvelopingSpan(['!'], [{'bert_tokens': ['!'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Mõistus'], [{'bert_tokens': ['▁Mõ', 'istus'], 'morph_labels': ['sg n_S'], 'probabilities': [0.9997]}]),\n",
       "EnvelopingSpan(['tuleb'], [{'bert_tokens': ['▁tuleb'], 'morph_labels': ['b_V'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['ikka'], [{'bert_tokens': ['▁ikka'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['selge'], [{'bert_tokens': ['▁selge'], 'morph_labels': ['sg n_A'], 'probabilities': [0.99977]}]),\n",
       "EnvelopingSpan(['hoida'], [{'bert_tokens': ['▁hoida'], 'morph_labels': ['da_V'], 'probabilities': [0.99985]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['muidu'], [{'bert_tokens': ['▁muidu'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['võin'], [{'bert_tokens': ['▁võin'], 'morph_labels': ['n_V'], 'probabilities': [0.99957]}]),\n",
       "EnvelopingSpan(['hakata'], [{'bert_tokens': ['▁hakata'], 'morph_labels': ['da_V'], 'probabilities': [0.99987]}]),\n",
       "EnvelopingSpan(['juba'], [{'bert_tokens': ['▁juba'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['selleski'], [{'bert_tokens': ['▁selles', 'ki'], 'morph_labels': ['sg in_P'], 'probabilities': [0.9992]}]),\n",
       "EnvelopingSpan(['kahtlema'], [{'bert_tokens': ['▁kahtlema'], 'morph_labels': ['ma_V'], 'probabilities': [0.99926]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['kas'], [{'bert_tokens': ['▁kas'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['eelmine'], [{'bert_tokens': ['▁eelmine'], 'morph_labels': ['sg n_A'], 'probabilities': [0.99972]}]),\n",
       "EnvelopingSpan(['kommentaar'], [{'bert_tokens': ['▁kommentaar'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99979]}]),\n",
       "EnvelopingSpan(['polnud'], [{'bert_tokens': ['▁polnud'], 'morph_labels': ['neg nud_V'], 'probabilities': [0.99971]}]),\n",
       "EnvelopingSpan(['mitte'], [{'bert_tokens': ['▁mitte'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['mu'], [{'bert_tokens': ['▁mu'], 'morph_labels': ['sg g_P'], 'probabilities': [0.99968]}]),\n",
       "EnvelopingSpan(['enda'], [{'bert_tokens': ['▁enda'], 'morph_labels': ['sg g_P'], 'probabilities': [0.99966]}]),\n",
       "EnvelopingSpan(['kirjutatud'], [{'bert_tokens': ['▁kirjutatud'], 'morph_labels': ['tud_V'], 'probabilities': [0.98832]}]),\n",
       "EnvelopingSpan(['...'], [{'bert_tokens': ['...'], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['Tuvastamata'], [{'bert_tokens': ['▁Tu', 'vasta', 'mata'], 'morph_labels': ['A'], 'probabilities': [0.95848]}]),\n",
       "EnvelopingSpan(['Kasutaja'], [{'bert_tokens': ['▁Kasutaja'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99825]}]),\n",
       "EnvelopingSpan(['Ennustaja'], [{'bert_tokens': ['▁Enn', 'ustaja'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99937]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['ma'], [{'bert_tokens': ['▁ma'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99986]}]),\n",
       "EnvelopingSpan(['pean'], [{'bert_tokens': ['▁pean'], 'morph_labels': ['n_V'], 'probabilities': [0.99952]}]),\n",
       "EnvelopingSpan(['tarvilikuks'], [{'bert_tokens': ['▁tarv', 'il', 'ikuks'], 'morph_labels': ['sg tr_A'], 'probabilities': [0.99891]}]),\n",
       "EnvelopingSpan(['teid'], [{'bert_tokens': ['▁teid'], 'morph_labels': ['pl p_P'], 'probabilities': [0.99611]}]),\n",
       "EnvelopingSpan(['informeerida'], [{'bert_tokens': ['▁informeer', 'ida'], 'morph_labels': ['da_V'], 'probabilities': [0.99985]}]),\n",
       "EnvelopingSpan(['selles'], [{'bert_tokens': ['▁selles'], 'morph_labels': ['sg in_P'], 'probabilities': [0.99934]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['et'], [{'bert_tokens': ['▁et'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['sellest'], [{'bert_tokens': ['▁sellest'], 'morph_labels': ['sg el_P'], 'probabilities': [0.99917]}]),\n",
       "EnvelopingSpan(['\"'], [{'bert_tokens': ['▁\"'], 'morph_labels': ['Z'], 'probabilities': [0.99995]}]),\n",
       "EnvelopingSpan(['lugejatrombsmaarikapiitertupolev'], [{'bert_tokens': ['lu', 'geja', 't', 'ro', 'mb', 's', 'maar', 'ika', 'pii', 'ter', 'tu', 'pole', 'v'], 'morph_labels': ['sg n_A'], 'probabilities': [0.9845]}]),\n",
       "EnvelopingSpan(['brittnolooginoff'], [{'bert_tokens': ['▁britt', 'n', 'oloo', 'gin', 'off'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99957]}]),\n",
       "EnvelopingSpan(['\"'], [{'bert_tokens': ['\"'], 'morph_labels': ['Z'], 'probabilities': [0.99994]}]),\n",
       "EnvelopingSpan(['esinen'], [{'bert_tokens': ['▁esin', 'en'], 'morph_labels': ['n_V'], 'probabilities': [0.99928]}]),\n",
       "EnvelopingSpan(['ma'], [{'bert_tokens': ['▁ma'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99986]}]),\n",
       "EnvelopingSpan(['ainult'], [{'bert_tokens': ['▁ainult'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['ühe'], [{'bert_tokens': ['▁ühe'], 'morph_labels': ['sg g_P'], 'probabilities': [0.99935]}]),\n",
       "EnvelopingSpan(['korra'], [{'bert_tokens': ['▁korra'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99968]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['s.t.'], [{'bert_tokens': ['▁s', '.', 't', '.'], 'morph_labels': ['?_Y'], 'probabilities': [0.99731]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['▁.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Maarika-na'], [{'bert_tokens': ['▁Maar', 'ika', '-', 'na'], 'morph_labels': ['sg es_S'], 'probabilities': [0.24713]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99995]}]),\n",
       "EnvelopingSpan(['Teised'], [{'bert_tokens': ['▁Teised'], 'morph_labels': ['pl n_P'], 'probabilities': [0.99979]}]),\n",
       "EnvelopingSpan(['pole'], [{'bert_tokens': ['▁pole'], 'morph_labels': ['neg o_V'], 'probabilities': [0.99957]}]),\n",
       "EnvelopingSpan(['kunagi'], [{'bert_tokens': ['▁kunagi'], 'morph_labels': ['D'], 'probabilities': [0.99987]}]),\n",
       "EnvelopingSpan(['minu'], [{'bert_tokens': ['▁minu'], 'morph_labels': ['sg g_P'], 'probabilities': [0.99966]}]),\n",
       "EnvelopingSpan(['aliased'], [{'bert_tokens': ['▁al', 'ia', 'sed'], 'morph_labels': ['pl n_S'], 'probabilities': [0.99949]}]),\n",
       "EnvelopingSpan(['olnud'], [{'bert_tokens': ['▁olnud'], 'morph_labels': ['nud_V'], 'probabilities': [0.99978]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Rahu'], [{'bert_tokens': ['▁Rahu'], 'morph_labels': ['sg n_S'], 'probabilities': [0.98781]}]),\n",
       "EnvelopingSpan(['sulle'], [{'bert_tokens': ['▁sulle'], 'morph_labels': ['sg all_P'], 'probabilities': [0.99979]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['ära'], [{'bert_tokens': ['▁ära'], 'morph_labels': ['neg o_V'], 'probabilities': [0.99889]}]),\n",
       "EnvelopingSpan(['üle'], [{'bert_tokens': ['▁üle'], 'morph_labels': ['D'], 'probabilities': [0.99976]}]),\n",
       "EnvelopingSpan(['pinguta'], [{'bert_tokens': ['▁pingu', 'ta'], 'morph_labels': ['o_V'], 'probabilities': [0.99821]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['Tuvastamata'], [{'bert_tokens': ['▁Tu', 'vasta', 'mata'], 'morph_labels': ['A'], 'probabilities': [0.95848]}]),\n",
       "EnvelopingSpan(['Kasutaja'], [{'bert_tokens': ['▁Kasutaja'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99825]}]),\n",
       "EnvelopingSpan(['Sageli'], [{'bert_tokens': ['▁Sageli'], 'morph_labels': ['D'], 'probabilities': [0.99988]}]),\n",
       "EnvelopingSpan(['?'], [{'bert_tokens': ['?'], 'morph_labels': ['Z'], 'probabilities': [0.99992]}]),\n",
       "EnvelopingSpan(['Tead'], [{'bert_tokens': ['▁Tead'], 'morph_labels': ['d_V'], 'probabilities': [0.99932]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['see'], [{'bert_tokens': ['▁see'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99987]}]),\n",
       "EnvelopingSpan(['sõltub'], [{'bert_tokens': ['▁sõltub'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['kui'], [{'bert_tokens': ['▁kui'], 'morph_labels': ['J'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['tihti'], [{'bert_tokens': ['▁tihti'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['nimetatud'], [{'bert_tokens': ['▁nimetatud'], 'morph_labels': ['A'], 'probabilities': [0.99921]}]),\n",
       "EnvelopingSpan(['seltskond'], [{'bert_tokens': ['▁seltskond'], 'morph_labels': ['sg n_S'], 'probabilities': [0.9998]}]),\n",
       "EnvelopingSpan(['nimetatud'], [{'bert_tokens': ['▁nimetatud'], 'morph_labels': ['A'], 'probabilities': [0.99921]}]),\n",
       "EnvelopingSpan(['ollust'], [{'bert_tokens': ['▁ol', 'lust'], 'morph_labels': ['sg p_S'], 'probabilities': [0.99863]}]),\n",
       "EnvelopingSpan(['siia'], [{'bert_tokens': ['▁siia'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['laiali'], [{'bert_tokens': ['▁laiali'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['laotab'], [{'bert_tokens': ['▁lao', 'tab'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Ja'], [{'bert_tokens': ['▁Ja'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['veel'], [{'bert_tokens': ['▁veel'], 'morph_labels': ['D'], 'probabilities': [0.99984]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['vabanda'], [{'bert_tokens': ['▁vabanda'], 'morph_labels': ['o_V'], 'probabilities': [0.99886]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['aga'], [{'bert_tokens': ['▁aga'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['sina'], [{'bert_tokens': ['▁sina'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99986]}]),\n",
       "EnvelopingSpan(['küll'], [{'bert_tokens': ['▁küll'], 'morph_labels': ['D'], 'probabilities': [0.99985]}]),\n",
       "EnvelopingSpan(['ei'], [{'bert_tokens': ['▁ei'], 'morph_labels': ['neg_V'], 'probabilities': [0.99988]}]),\n",
       "EnvelopingSpan(['ole'], [{'bert_tokens': ['▁ole'], 'morph_labels': ['o_V'], 'probabilities': [0.99964]}]),\n",
       "EnvelopingSpan(['armas'], [{'bert_tokens': ['▁armas'], 'morph_labels': ['sg n_A'], 'probabilities': [0.99977]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['kellele'], [{'bert_tokens': ['▁kellele'], 'morph_labels': ['sg all_P'], 'probabilities': [0.99964]}]),\n",
       "EnvelopingSpan(['('], [{'bert_tokens': ['▁('], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['peale'], [{'bert_tokens': ['peale'], 'morph_labels': ['K'], 'probabilities': [0.99927]}]),\n",
       "EnvelopingSpan(['savisaarlaste'], [{'bert_tokens': ['▁savisaar', 'laste'], 'morph_labels': ['pl g_S'], 'probabilities': [0.99955]}]),\n",
       "EnvelopingSpan([')'], [{'bert_tokens': [')'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['saab'], [{'bert_tokens': ['▁saab'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['armas'], [{'bert_tokens': ['▁armas'], 'morph_labels': ['sg n_A'], 'probabilities': [0.99978]}]),\n",
       "EnvelopingSpan(['olla'], [{'bert_tokens': ['▁olla'], 'morph_labels': ['da_V'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['sihuke'], [{'bert_tokens': ['▁sihuke'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99985]}]),\n",
       "EnvelopingSpan(['punane'], [{'bert_tokens': ['▁punane'], 'morph_labels': ['sg n_A'], 'probabilities': [0.99974]}]),\n",
       "EnvelopingSpan(['eva'], [{'bert_tokens': ['▁e', 'va'], 'morph_labels': ['sg g_S'], 'probabilities': [0.97083]}]),\n",
       "EnvelopingSpan([\"'\"], [{'bert_tokens': [\"▁'\"], 'morph_labels': ['Z'], 'probabilities': [0.99977]}]),\n",
       "EnvelopingSpan(['kohuke'], [{'bert_tokens': ['koh', 'uke'], 'morph_labels': ['sg n_S'], 'probabilities': [0.9987]}]),\n",
       "EnvelopingSpan([\"'\"], [{'bert_tokens': [\"'\"], 'morph_labels': ['Z'], 'probabilities': [0.99978]}]),\n",
       "EnvelopingSpan(['britt'], [{'bert_tokens': ['▁britt'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99965]}]),\n",
       "EnvelopingSpan(['?'], [{'bert_tokens': ['?'], 'morph_labels': ['Z'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['Tuvastamata'], [{'bert_tokens': ['▁Tu', 'vasta', 'mata'], 'morph_labels': ['A'], 'probabilities': [0.95848]}]),\n",
       "EnvelopingSpan(['Kasutaja'], [{'bert_tokens': ['▁Kasutaja'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99825]}]),\n",
       "EnvelopingSpan(['27.04.2009'], [{'bert_tokens': ['▁27', '.04.2009'], 'morph_labels': ['?_N'], 'probabilities': [0.99494]}]),\n",
       "EnvelopingSpan(['20:53'], [{'bert_tokens': ['▁20', ':5', '3'], 'morph_labels': ['?_N'], 'probabilities': [0.99896]}]),\n",
       "EnvelopingSpan(['Märkasin'], [{'bert_tokens': ['▁Mär', 'kasin'], 'morph_labels': ['sin_V'], 'probabilities': [0.99867]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['et'], [{'bert_tokens': ['▁et'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['mulle'], [{'bert_tokens': ['▁mulle'], 'morph_labels': ['sg all_P'], 'probabilities': [0.99976]}]),\n",
       "EnvelopingSpan(['on'], [{'bert_tokens': ['▁on'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['siin'], [{'bert_tokens': ['▁siin'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['ametivend'], [{'bert_tokens': ['▁amet', 'iv', 'end'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99942]}]),\n",
       "EnvelopingSpan(['tekkinud'], [{'bert_tokens': ['▁tekkinud'], 'morph_labels': ['nud_V'], 'probabilities': [0.99977]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Ilmutates'], [{'bert_tokens': ['▁Ilm', 'uta', 'tes'], 'morph_labels': ['des_V'], 'probabilities': [0.99864]}]),\n",
       "EnvelopingSpan(['loomulikku'], [{'bert_tokens': ['▁loomulikku'], 'morph_labels': ['sg p_A'], 'probabilities': [0.99944]}]),\n",
       "EnvelopingSpan(['huvi'], [{'bert_tokens': ['▁huvi'], 'morph_labels': ['sg p_S'], 'probabilities': [0.99959]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': ['▁,'], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['tahtsin'], [{'bert_tokens': ['▁tahtsin'], 'morph_labels': ['sin_V'], 'probabilities': [0.99872]}]),\n",
       "EnvelopingSpan(['küsida'], [{'bert_tokens': ['▁küsida'], 'morph_labels': ['da_V'], 'probabilities': [0.99986]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['mis'], [{'bert_tokens': ['▁mis'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99984]}]),\n",
       "EnvelopingSpan(['abivahendeid'], [{'bert_tokens': ['▁abi', 'vahendeid'], 'morph_labels': ['pl p_S'], 'probabilities': [0.99955]}]),\n",
       "EnvelopingSpan(['kolleeg'], [{'bert_tokens': ['▁kolleeg'], 'morph_labels': ['sg n_S'], 'probabilities': [0.9998]}]),\n",
       "EnvelopingSpan(['kasutab'], [{'bert_tokens': ['▁kasutab'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['kas'], [{'bert_tokens': ['▁kas'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['ennustab'], [{'bert_tokens': ['▁ennustab'], 'morph_labels': ['b_V'], 'probabilities': [0.99992]}]),\n",
       "EnvelopingSpan(['vasikapõrna'], [{'bert_tokens': ['▁vas', 'ika', 'põr', 'na'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99968]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99995]}]),\n",
       "EnvelopingSpan(['tuvisisikonna'], [{'bert_tokens': ['▁tu', 'vis', 'is', 'iko', 'nna'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99964]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99995]}]),\n",
       "EnvelopingSpan(['seasüdame'], [{'bert_tokens': ['▁seas', 'ü', 'dame'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99964]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99995]}]),\n",
       "EnvelopingSpan(['kohvipaksu'], [{'bert_tokens': ['▁kohvi', 'pa', 'ksu'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99964]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': ['▁,'], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['või'], [{'bert_tokens': ['▁või'], 'morph_labels': ['J'], 'probabilities': [0.99984]}]),\n",
       "EnvelopingSpan(['kõhutuule'], [{'bert_tokens': ['▁kõh', 'ut', 'uule'], 'morph_labels': ['sg g_S'], 'probabilities': [0.99962]}]),\n",
       "EnvelopingSpan(['järgi'], [{'bert_tokens': ['▁järgi'], 'morph_labels': ['K'], 'probabilities': [0.99968]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['Lugedes'], [{'bert_tokens': ['▁Lugedes'], 'morph_labels': ['des_V'], 'probabilities': [0.99942]}]),\n",
       "EnvelopingSpan(['teie'], [{'bert_tokens': ['▁teie'], 'morph_labels': ['pl g_P'], 'probabilities': [0.99928]}]),\n",
       "EnvelopingSpan(['kommentaare'], [{'bert_tokens': ['▁kommentaare'], 'morph_labels': ['pl p_S'], 'probabilities': [0.99958]}]),\n",
       "EnvelopingSpan(['tundub'], [{'bert_tokens': ['▁tundub'], 'morph_labels': ['b_V'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['et'], [{'bert_tokens': ['▁et'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['kasutate'], [{'bert_tokens': ['▁kasuta', 'te'], 'morph_labels': ['te_V'], 'probabilities': [0.99862]}]),\n",
       "EnvelopingSpan(['viimast'], [{'bert_tokens': ['▁viimast'], 'morph_labels': ['sg p_A'], 'probabilities': [0.99898]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}]),\n",
       "EnvelopingSpan(['Hoiatan'], [{'bert_tokens': ['▁Hoia', 'tan'], 'morph_labels': ['n_V'], 'probabilities': [0.99952]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': ['▁,'], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['see'], [{'bert_tokens': ['▁see'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99987]}]),\n",
       "EnvelopingSpan(['ei'], [{'bert_tokens': ['▁ei'], 'morph_labels': ['neg_V'], 'probabilities': [0.99987]}]),\n",
       "EnvelopingSpan(['pruugi'], [{'bert_tokens': ['▁pruugi'], 'morph_labels': ['o_V'], 'probabilities': [0.99963]}]),\n",
       "EnvelopingSpan(['alati'], [{'bert_tokens': ['▁alati'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['anda'], [{'bert_tokens': ['▁anda'], 'morph_labels': ['da_V'], 'probabilities': [0.99987]}]),\n",
       "EnvelopingSpan(['õiget'], [{'bert_tokens': ['▁õiget'], 'morph_labels': ['sg p_A'], 'probabilities': [0.99946]}]),\n",
       "EnvelopingSpan(['resultaati'], [{'bert_tokens': ['▁resul', 'ta', 'ati'], 'morph_labels': ['sg p_S'], 'probabilities': [0.99968]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['eriti'], [{'bert_tokens': ['▁eriti'], 'morph_labels': ['D'], 'probabilities': [0.9999]}]),\n",
       "EnvelopingSpan(['kui'], [{'bert_tokens': ['▁kui'], 'morph_labels': ['J'], 'probabilities': [0.99991]}]),\n",
       "EnvelopingSpan(['päevases'], [{'bert_tokens': ['▁päeva', 'ses'], 'morph_labels': ['sg in_A'], 'probabilities': [0.99881]}]),\n",
       "EnvelopingSpan(['menüüs'], [{'bert_tokens': ['▁menüüs'], 'morph_labels': ['sg in_S'], 'probabilities': [0.9996]}]),\n",
       "EnvelopingSpan(['on'], [{'bert_tokens': ['▁on'], 'morph_labels': ['b_V'], 'probabilities': [0.99992]}]),\n",
       "EnvelopingSpan(['hernesupp'], [{'bert_tokens': ['▁her', 'nes', 'u', 'pp'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99969]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['Loomulikult'], [{'bert_tokens': ['▁Loomulikult'], 'morph_labels': ['D'], 'probabilities': [0.99989]}]),\n",
       "EnvelopingSpan(['jääb'], [{'bert_tokens': ['▁jääb'], 'morph_labels': ['b_V'], 'probabilities': [0.99993]}]),\n",
       "EnvelopingSpan(['see'], [{'bert_tokens': ['▁see'], 'morph_labels': ['sg n_P'], 'probabilities': [0.99986]}]),\n",
       "EnvelopingSpan(['sõbralik'], [{'bert_tokens': ['▁sõbralik'], 'morph_labels': ['sg n_A'], 'probabilities': [0.99977]}]),\n",
       "EnvelopingSpan(['soovitus'], [{'bert_tokens': ['▁soovitus'], 'morph_labels': ['sg n_S'], 'probabilities': [0.99969]}]),\n",
       "EnvelopingSpan([','], [{'bert_tokens': [','], 'morph_labels': ['Z'], 'probabilities': [0.99997]}]),\n",
       "EnvelopingSpan(['kolleegide'], [{'bert_tokens': ['▁kolleegide'], 'morph_labels': ['pl g_S'], 'probabilities': [0.99963]}]),\n",
       "EnvelopingSpan(['vahele'], [{'bert_tokens': ['▁vahele'], 'morph_labels': ['K'], 'probabilities': [0.99958]}]),\n",
       "EnvelopingSpan(['.'], [{'bert_tokens': ['.'], 'morph_labels': ['Z'], 'probabilities': [0.99996]}])])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_obj.bert_morph_tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='end'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
