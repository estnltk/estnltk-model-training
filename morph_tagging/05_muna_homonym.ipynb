{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with homonym \"muna\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "   1. [**Gathering Data**](#andmete_kogumine)\n",
    "   2. [**Model training**](#margendaja_treenimine)\n",
    "   3. [**Tagger Evaluation**](#margendaja_hindamine)\n",
    "   4. [**Evaluation on UD treebank**](#margendaja_hindamine_puudepangal)\n",
    "   5. [**Results**](#tulemused)\n",
    "\n",
    "\n",
    "[end](#end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import evaluate\n",
    "import pkg_resources\n",
    "import types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import estnltk, estnltk.converters, estnltk.taggers\n",
    "import sklearn\n",
    "\n",
    "from bert_morph_tagger_notebook_functions import NotebookFunctions\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "from tqdm import tqdm\n",
    "from bert_morph_tagger import BertMorphTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estnltk==1.7.3\n",
      "evaluate==0.4.2\n",
      "matplotlib==3.9.0\n",
      "numpy==1.26.4\n",
      "pandas==2.2.2\n",
      "scikit-learn==1.5.1\n",
      "simpletransformers==0.70.1\n",
      "torch==2.5.1\n",
      "tqdm==4.66.5\n"
     ]
    }
   ],
   "source": [
    "# Get locally imported modules from current notebook - https://stackoverflow.com/questions/40428931/package-for-listing-version-of-packages-used-in-a-jupyter-notebook - Alex P. Miller\n",
    "def get_imports():\n",
    "    \n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "            \n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system/pip names. Unfortunately,\n",
    "        # there is no systematic way to get pip names from\n",
    "        # a package's imported name. You'll have to add\n",
    "        # exceptions to this list manually!\n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "            \n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "# The only way I found to get the version of the root package\n",
    "# from only the name of the package is to cross-check the names \n",
    "# of installed packages vs. imported packages\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='andmete_kogumine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "with open('./100_manually_corrected_muna_sentences.jl', 'r') as file:\n",
    "    for line in file:\n",
    "        lines.append(json.loads(line))\n",
    "\n",
    "data = pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>correct_forms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nc19_Balanced_Corpus.vert</td>\n",
       "      <td>1751</td>\n",
       "      <td>\"Salmonella levib hästi piimatoitudes, kus on kasutatud toorest muna,\" nimetab tohter ühe ohuallikana paljude lemmikmaiust tiramisut.</td>\n",
       "      <td>[{'word': 'muna', 'form': 'S_sg p', 'start': 64, 'end': 68}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nc19_Balanced_Corpus.vert</td>\n",
       "      <td>2471</td>\n",
       "      <td>(1994) Tshak-tshak Taignaks 400 g nisujahu, 160 g muna, 67 g piima, 10 g suhkrut, 22 g soola, 210 g sulavõid.</td>\n",
       "      <td>[{'word': 'muna', 'form': 'S_sg p', 'start': 50, 'end': 54}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nc19_Balanced_Corpus.vert</td>\n",
       "      <td>2917</td>\n",
       "      <td>Geko igatseb kaasat Hallikirjut kummimänguasja meenutav tokee geko valvab kaht oma abikaasa munetud muna.</td>\n",
       "      <td>[{'word': 'muna', 'form': 'S_sg p', 'start': 100, 'end': 104}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nc19_Balanced_Corpus.vert</td>\n",
       "      <td>3301</td>\n",
       "      <td>Nüüd tuleb lisada suhkur ja ükshaaval kuus muna.</td>\n",
       "      <td>[{'word': 'muna', 'form': 'S_sg p', 'start': 43, 'end': 47}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nc19_Balanced_Corpus.vert</td>\n",
       "      <td>4057</td>\n",
       "      <td>Ülestõusmispühadeks pidi iga hellake muna keetma ja selle värvima, koos viidi siis need vanadekodusse.</td>\n",
       "      <td>[{'word': 'muna', 'form': 'S_sg g', 'start': 37, 'end': 41}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      corpus  doc_id  \\\n",
       "0  nc19_Balanced_Corpus.vert    1751   \n",
       "1  nc19_Balanced_Corpus.vert    2471   \n",
       "2  nc19_Balanced_Corpus.vert    2917   \n",
       "3  nc19_Balanced_Corpus.vert    3301   \n",
       "4  nc19_Balanced_Corpus.vert    4057   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0  \"Salmonella levib hästi piimatoitudes, kus on kasutatud toorest muna,\" nimetab tohter ühe ohuallikana paljude lemmikmaiust tiramisut.   \n",
       "1                          (1994) Tshak-tshak Taignaks 400 g nisujahu, 160 g muna, 67 g piima, 10 g suhkrut, 22 g soola, 210 g sulavõid.   \n",
       "2                              Geko igatseb kaasat Hallikirjut kummimänguasja meenutav tokee geko valvab kaht oma abikaasa munetud muna.   \n",
       "3                                                                                       Nüüd tuleb lisada suhkur ja ükshaaval kuus muna.   \n",
       "4                                 Ülestõusmispühadeks pidi iga hellake muna keetma ja selle värvima, koos viidi siis need vanadekodusse.   \n",
       "\n",
       "                                                    correct_forms  \n",
       "0    [{'word': 'muna', 'form': 'S_sg p', 'start': 64, 'end': 68}]  \n",
       "1    [{'word': 'muna', 'form': 'S_sg p', 'start': 50, 'end': 54}]  \n",
       "2  [{'word': 'muna', 'form': 'S_sg p', 'start': 100, 'end': 104}]  \n",
       "3    [{'word': 'muna', 'form': 'S_sg p', 'start': 43, 'end': 47}]  \n",
       "4    [{'word': 'muna', 'form': 'S_sg g', 'start': 37, 'end': 41}]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='margendaja_treenimine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating required dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./muna_data.csv\"):\n",
    "    rows = []\n",
    "    sentence_id = 0\n",
    "\n",
    "    for d in data.iterrows():\n",
    "        text = d[1].text\n",
    "        text_obj = estnltk.Text(text)\n",
    "        morph_analysis = text_obj.tag_layer('morph_analysis')\n",
    "        i = 0\n",
    "        for sentence in morph_analysis.sentences:\n",
    "            sentence_analysis = sentence.morph_analysis\n",
    "            for text_part, form, pos in zip(sentence_analysis.text, sentence_analysis.form, sentence_analysis.partofspeech):\n",
    "                if text_part == \"muna\":\n",
    "                    pos_form = d[1]['correct_forms'][i]['form'].split('_')\n",
    "                    rows.append((sentence_id, text_part, pos_form[1], pos_form[0]))\n",
    "                    i+=1\n",
    "                    continue\n",
    "                if text_part:\n",
    "                    rows.append((sentence_id, text_part, form[0], pos[0])) # In case of multiplicity, select the first or index 0\n",
    "            sentence_id += 1\n",
    "\n",
    "    train_df = pd.DataFrame(rows, columns = ['sentence_id', 'words', 'form', 'pos'])\n",
    "    NotebookFunctions.create_labels_column(train_df, \"muna_data.csv\")\n",
    "    train_df\n",
    "\n",
    "else:\n",
    "    train_df = pd.read_csv('muna_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model<!-- Mudeli ülesehitamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = NotebookFunctions.get_unique_labels(\"./unique_labels.json\")\n",
    "model = NotebookFunctions.initialize_model(\"NER_mudel_v2\", unique_labels=unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set directories where new model will be created and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.args.output_dir = 'NER_mudel_v2_muna_1'\n",
    "model.args.cache_dir = os.path.join(model.args.output_dir, 'cache')\n",
    "model.args.best_model_dir = os.path.join(model.args.output_dir, 'best_model')\n",
    "model.args.num_train_epochs = 20\n",
    "model.args.learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4000.02it/s]\n",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:762: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n",
      "Epoch 1 of 20:   0%|          | 0/20 [00:00<?, ?it/s]e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:786: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n",
      "Epochs 1/20. Running Loss:    0.5540: 100%|██████████| 13/13 [00:01<00:00, 11.02it/s]\n",
      "Epochs 2/20. Running Loss:    0.8486: 100%|██████████| 13/13 [00:00<00:00, 16.18it/s]\n",
      "Epochs 3/20. Running Loss:    0.9083: 100%|██████████| 13/13 [00:00<00:00, 15.99it/s]\n",
      "Epochs 4/20. Running Loss:    0.3594: 100%|██████████| 13/13 [00:00<00:00, 15.04it/s]\n",
      "Epochs 5/20. Running Loss:    0.2994: 100%|██████████| 13/13 [00:00<00:00, 15.90it/s]\n",
      "Epochs 6/20. Running Loss:    0.1518: 100%|██████████| 13/13 [00:00<00:00, 16.95it/s]\n",
      "Epochs 7/20. Running Loss:    0.1544: 100%|██████████| 13/13 [00:00<00:00, 16.97it/s]\n",
      "Epochs 8/20. Running Loss:    0.2860: 100%|██████████| 13/13 [00:00<00:00, 16.77it/s]\n",
      "Epochs 9/20. Running Loss:    0.0619: 100%|██████████| 13/13 [00:00<00:00, 16.53it/s]\n",
      "Epochs 10/20. Running Loss:    0.0250: 100%|██████████| 13/13 [00:00<00:00, 16.56it/s]\n",
      "Epochs 11/20. Running Loss:    0.0766: 100%|██████████| 13/13 [00:00<00:00, 16.66it/s]\n",
      "Epochs 12/20. Running Loss:    0.1121: 100%|██████████| 13/13 [00:00<00:00, 17.02it/s]\n",
      "Epochs 13/20. Running Loss:    0.0154: 100%|██████████| 13/13 [00:00<00:00, 17.05it/s]\n",
      "Epochs 14/20. Running Loss:    0.0105: 100%|██████████| 13/13 [00:00<00:00, 16.87it/s]\n",
      "Epochs 15/20. Running Loss:    0.1435: 100%|██████████| 13/13 [00:00<00:00, 16.93it/s]\n",
      "Epochs 16/20. Running Loss:    0.0954: 100%|██████████| 13/13 [00:00<00:00, 17.15it/s]\n",
      "Epochs 17/20. Running Loss:    0.0082: 100%|██████████| 13/13 [00:00<00:00, 16.83it/s]\n",
      "Epochs 18/20. Running Loss:    0.0581: 100%|██████████| 13/13 [00:00<00:00, 16.51it/s]\n",
      "Epochs 19/20. Running Loss:    0.0063: 100%|██████████| 13/13 [00:00<00:00, 17.18it/s]\n",
      "Epochs 20/20. Running Loss:    0.0021: 100%|██████████| 13/13 [00:00<00:00, 16.67it/s]\n",
      "Epoch 20 of 20: 100%|██████████| 20/20 [00:16<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join('./' + model.args.output_dir, 'config.json')):\n",
    "    # Train model\n",
    "    print(\"Training model\")\n",
    "    model.train_model(train_df, output_dir=model.args.output_dir, only_muna=\"muna\")\n",
    "else:\n",
    "    model = NotebookFunctions.initialize_model(model.args.output_dir, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set directories where new model will be created and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.args.output_dir = 'NER_mudel_v2_muna_2'\n",
    "model.args.cache_dir = os.path.join(model.args.output_dir, 'cache')\n",
    "model.args.best_model_dir = os.path.join(model.args.output_dir, 'best_model')\n",
    "model.args.num_train_epochs = 20\n",
    "model.args.learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 3333.55it/s]\n",
      "Epochs 1/20. Running Loss:    0.0022: 100%|██████████| 13/13 [00:00<00:00, 16.08it/s]\n",
      "Epochs 2/20. Running Loss:    0.0132: 100%|██████████| 13/13 [00:00<00:00, 16.31it/s]\n",
      "Epochs 3/20. Running Loss:    0.0129: 100%|██████████| 13/13 [00:00<00:00, 15.53it/s]\n",
      "Epochs 4/20. Running Loss:    0.0024: 100%|██████████| 13/13 [00:00<00:00, 15.34it/s]\n",
      "Epochs 5/20. Running Loss:    0.0024: 100%|██████████| 13/13 [00:00<00:00, 16.05it/s]\n",
      "Epochs 6/20. Running Loss:    0.0037: 100%|██████████| 13/13 [00:00<00:00, 15.24it/s]\n",
      "Epochs 7/20. Running Loss:    0.0021: 100%|██████████| 13/13 [00:00<00:00, 16.18it/s]\n",
      "Epochs 8/20. Running Loss:    0.0956: 100%|██████████| 13/13 [00:00<00:00, 17.14it/s]\n",
      "Epochs 9/20. Running Loss:    0.0015: 100%|██████████| 13/13 [00:00<00:00, 16.60it/s]\n",
      "Epochs 10/20. Running Loss:    0.0816: 100%|██████████| 13/13 [00:00<00:00, 16.98it/s]\n",
      "Epochs 11/20. Running Loss:    0.0059: 100%|██████████| 13/13 [00:00<00:00, 16.72it/s]\n",
      "Epochs 12/20. Running Loss:    0.0012: 100%|██████████| 13/13 [00:00<00:00, 16.84it/s]\n",
      "Epochs 13/20. Running Loss:    0.0031: 100%|██████████| 13/13 [00:00<00:00, 16.63it/s]\n",
      "Epochs 14/20. Running Loss:    0.0013: 100%|██████████| 13/13 [00:00<00:00, 16.56it/s]\n",
      "Epochs 15/20. Running Loss:    0.0080: 100%|██████████| 13/13 [00:00<00:00, 16.73it/s]\n",
      "Epochs 16/20. Running Loss:    0.0027: 100%|██████████| 13/13 [00:00<00:00, 17.12it/s]\n",
      "Epochs 17/20. Running Loss:    0.0021: 100%|██████████| 13/13 [00:00<00:00, 17.23it/s]\n",
      "Epochs 18/20. Running Loss:    0.0053: 100%|██████████| 13/13 [00:00<00:00, 17.17it/s]\n",
      "Epochs 19/20. Running Loss:    0.0014: 100%|██████████| 13/13 [00:00<00:00, 17.35it/s]\n",
      "Epochs 20/20. Running Loss:    0.0020: 100%|██████████| 13/13 [00:00<00:00, 17.00it/s]\n",
      "Epoch 20 of 20: 100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\".\\\\NER_mudel_v2_muna_2\\\\config.json\"):\n",
    "    # Train model\n",
    "    print(\"Training model\")\n",
    "    model.train_model(train_df, output_dir=model.args.output_dir, only_muna=\"all\")\n",
    "else:\n",
    "    model = NotebookFunctions.initialize_model(model.args.output_dir, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='margendaja_hindamine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagger Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['./NER_mudel/', './NER_mudel_v2/', './NER_mudel_v2_muna_1/', './NER_mudel_v2_muna_2/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', module='bert_tokens_to_words_rewriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(data):\n",
    "    results = []\n",
    "    for _, d in data[['text', 'correct_forms']].iterrows():\n",
    "        correct_forms = d['correct_forms']\n",
    "        text_obj = estnltk.Text(d['text'])\n",
    "        text_obj.tag_layer('sentences')\n",
    "        morph_tagger.tag(text_obj)\n",
    "\n",
    "        span_dict = {\n",
    "            (span.start, span.end): span.partofspeech[0] + '_' + span.form[0]\n",
    "            for span in text_obj.bert_morph_tagging.spans\n",
    "        }\n",
    "\n",
    "        for correct_form in correct_forms:\n",
    "            key = (correct_form['start'], correct_form['end'])\n",
    "            if key in span_dict:\n",
    "                predicted_form = span_dict[key]\n",
    "                is_same = predicted_form == correct_form['form']\n",
    "                # print(f\"Tagger: {predicted_form}, Correct form: {correct_form['form']}, Same: {is_same}\")\n",
    "                results.append(is_same)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(results):\n",
    "\n",
    "    results_binary = np.array([int(is_correct) for is_correct in results])\n",
    "    true_values = np.ones(len(results_binary), dtype=int)\n",
    "\n",
    "    accuracy = sklearn.metrics.accuracy_score(results_binary, true_values)\n",
    "    precision = sklearn.metrics.precision_score(results_binary, true_values, zero_division=0)\n",
    "    # recall = sklearn.metrics.recall_score(results_binary, true_values, zero_division=0)\n",
    "    f1 = sklearn.metrics.f1_score(results_binary, true_values, zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    # print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    unique, counts = np.unique(results_binary, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_path: ./NER_mudel/\n",
      "Accuracy: 0.8350\n",
      "Precision: 0.8350\n",
      "F1 Score: 0.9101\n",
      "{0: 17, 1: 86}\n",
      "Model_path: ./NER_mudel_v2/\n",
      "Accuracy: 0.9320\n",
      "Precision: 0.9320\n",
      "F1 Score: 0.9648\n",
      "{0: 7, 1: 96}\n",
      "Model_path: ./NER_mudel_v2_muna_1/\n",
      "Accuracy: 0.9903\n",
      "Precision: 0.9903\n",
      "F1 Score: 0.9951\n",
      "{0: 1, 1: 102}\n",
      "Model_path: ./NER_mudel_v2_muna_2/\n",
      "Accuracy: 0.9903\n",
      "Precision: 0.9903\n",
      "F1 Score: 0.9951\n",
      "{0: 1, 1: 102}\n"
     ]
    }
   ],
   "source": [
    "for model_path in models:\n",
    "    morph_tagger = BertMorphTagger(model_path)\n",
    "    print(f\"Model_path: {model_path}\")\n",
    "    evaluate_results(get_results(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='margendaja_hindamine_puudepangal'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on UD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ud = pd.read_csv('UD_train.csv', keep_default_na=False)\n",
    "test_df_ud = pd.read_csv('UD_test.csv', keep_default_na=False)\n",
    "dev_df_ud = pd.read_csv('UD_dev.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unnecessary columns for the model<!-- Mudelile ebavajalike veergude eemaldamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ud = train_df_ud.drop(labels=['file_prefix', 'source'], axis=1)\n",
    "test_df_ud = test_df_ud.drop(labels=['file_prefix', 'source'], axis=1)\n",
    "dev_df_ud = dev_df_ud.drop(labels=['file_prefix', 'source'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:09<00:00,  3.14s/it]\n",
      "Running Evaluation:   0%|          | 0/12 [00:00<?, ?it/s]e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:1341: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n",
      "Running Evaluation: 100%|██████████| 12/12 [00:01<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss:0.3237\n",
      "Precision: \t0.9568\n",
      "Recall: \t0.9466\n",
      "F1 Score: \t0.9474\n"
     ]
    }
   ],
   "source": [
    "poseval = evaluate.load(\"evaluate-metric/poseval\", module_type=\"metric\")\n",
    "\n",
    "def custom_metrics(preds, labels):\n",
    "\n",
    "    # Evaluate using poseval\n",
    "    result = poseval.compute(predictions=preds, references=labels)\n",
    "\n",
    "    return result\n",
    "\n",
    "model = NotebookFunctions.initialize_model(\"NER_mudel_v2_muna_1\", unique_labels=unique_labels)\n",
    "\n",
    "# Evaluate the model\n",
    "result, model_outputs, preds_list = model.eval_model(test_df_ud, extra_metrics=custom_metrics)\n",
    "\n",
    "print(f\"Evaluation Loss:{result['eval_loss']:.4f}\")\n",
    "print(f\"Precision: \\t{result['extra_metrics']['weighted avg']['precision']:.4f}\")\n",
    "print(f\"Recall: \\t{result['extra_metrics']['weighted avg']['recall']:.4f}\")\n",
    "print(f\"F1 Score: \\t{result['extra_metrics']['weighted avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:09<00:00,  3.20s/it]\n",
      "Running Evaluation:   0%|          | 0/12 [00:00<?, ?it/s]e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:1341: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n",
      "Running Evaluation: 100%|██████████| 12/12 [00:01<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss:0.3896\n",
      "Precision: \t0.9559\n",
      "Recall: \t0.9455\n",
      "F1 Score: \t0.9464\n"
     ]
    }
   ],
   "source": [
    "poseval = evaluate.load(\"evaluate-metric/poseval\", module_type=\"metric\")\n",
    "\n",
    "def custom_metrics(preds, labels):\n",
    "\n",
    "    # Evaluate using poseval\n",
    "    result = poseval.compute(predictions=preds, references=labels)\n",
    "\n",
    "    return result\n",
    "\n",
    "model = NotebookFunctions.initialize_model(\"NER_mudel_v2_muna_2\", unique_labels=unique_labels)\n",
    "\n",
    "# Evaluate the model\n",
    "result, model_outputs, preds_list = model.eval_model(test_df_ud, extra_metrics=custom_metrics)\n",
    "\n",
    "print(f\"Evaluation Loss:{result['eval_loss']:.4f}\")\n",
    "print(f\"Precision: \\t{result['extra_metrics']['weighted avg']['precision']:.4f}\")\n",
    "print(f\"Recall: \\t{result['extra_metrics']['weighted avg']['recall']:.4f}\")\n",
    "print(f\"F1 Score: \\t{result['extra_metrics']['weighted avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tulemused'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting label to word \"muna\"\n",
    "\n",
    "| Model                    | Accuracy  | Precision | F1 score |\n",
    "|--------------------------|-----------|-----------|----------|\n",
    "| Vabamorf                 | 0.8350    | 0.8350    | 0.9101   |\n",
    "| Bert_morph_v2            | 0.9320    | 0.9320    | 0.9648   |\n",
    "| Bert_morph_v2_muna_1[^1] | 0.9903    | 0.9903    | 0.9951   |\n",
    "| Bert_morph_v2_muna_2[^2] | 0.9903    | 0.9903    | 0.9951   |\n",
    "\n",
    "Evaluations on UD treebank\n",
    "\n",
    "| Model                    | Precision | Recall | F1 score |\n",
    "|--------------------------|-----------|--------|----------|\n",
    "| Vabamorf                 | 0.9194    | 0.9067 | 0.9082   |\n",
    "| Bert_morph_v2            | 0.9778    | 0.9765 | 0.9769   |\n",
    "| Bert_morph_v2_muna_1[^1] | 0.9568    | 0.9466 | 0.9474   |\n",
    "| Bert_morph_v2_muna_2[^2] | 0.9559    | 0.9455 | 0.9464   |\n",
    "\n",
    "\\* Metrics are from weighted average\n",
    "\n",
    "[^1]: Bert_morph_v2 model trained on the finetuning phase, on the muna-training set completely ignoring non-\"muna\" tokens\n",
    "[^2]: Bert_morph_v2 model trained on the finetuning phase, on the muna-training set including other tokens in the loss with a smaller weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='end'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpulocal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
