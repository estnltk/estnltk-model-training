{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert-based morphological tagger <!-- Morfoloogiline ühestaja -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "[Code](#kood)\n",
    "\n",
    "   1. [**enc2017 corpus**](#enc_korpus)\n",
    "      1. [**Defined functions**](#defineeritud_funktsioonid)\n",
    "      2. [**Collecting data and applying morphological analysis**](#andmete_lugemine_kogumine_ja_margendamine)\n",
    "         * [Verification](#kontrollimine_1)\n",
    "      3. [**Statistics**](#statistika)\n",
    "      4. [**Data splitting**](#andmete_jagamine_treening_test_ja_valideerimishulkadesse)\n",
    "         * [Grouping data by text type](#andmete_grupeerimine_tekstiliigiti)\n",
    "         * [Splitting data to sets](#andmete_jagamine_hulkadesse)\n",
    "      5. [**Token classification model (for assigning morpological categories)**](#ner_mudel)\n",
    "      6. [*Token classification model (for assigning morpological categories) by text type*](#ner_mudel_tekstiliigiti)\n",
    "\n",
    "[End](#end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## TODO\n",
    "1) - [ ] Neurovõrkudel põhineva morfoloogilise ühestaja loomine ja integreerimine EstNLTK teegiga:\n",
    "    1) - [ ] mudeli treenimiseks vajaliku töövoo loomine;\n",
    "    2) - [ ] töövoo kasutamine mudeli treenimiseks;\n",
    "    3) - [ ] mudeli integreerimine EstNLTK teeki.\n",
    "\n",
    "2) - [ ] Morfoloogilise ühestaja treenimiseks vajalike andmestike moodustamine olemasolevatest andmestikest:\n",
    "    1) - [ ] morfoloogilisemärgenduse loomine kasutades olemaolevaid EstNLTK automaatmärgendamise töövoogusid;\n",
    "    2) - [ ] morfoloogilisemärgenduse loomine kasutades UD puudepanga andmeid;\n",
    "\n",
    "3) - [ ] Saadud ühestaja tulemuste võrdlemine teiste kasutatavate rakendustega:\n",
    "    1) - [ ] täpsuse ja saagise absoluutne hindamine testvalimil;\n",
    "    2) - [ ] täpsuse ja saagise relatiivne hindamine märgendamata testandmetel;\n",
    "    3) - [ ] uute informatiivsete treeningandmete välja valimine.\n",
    "\n",
    "4) - [ ] Juhendmaterjalide  loomine uue ühestaja kasutamiseks\n",
    "5) - [ ] Zip the model somehow, https://github.com/estnltk/estnltk/blob/main/estnltk_neural/estnltk_neural/taggers/ner/estbertner_tagger.py -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code <!-- Kood -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kood'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:config.py:58: PyTorch version 2.4.0 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import typing\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import pkg_resources\n",
    "import types\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import estnltk, estnltk.converters, estnltk.taggers\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "from bert_morph_tagger import BertMorphTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estnltk==1.7.3\n",
      "numpy==1.26.4\n",
      "pandas==2.2.2\n",
      "scikit-learn==1.5.1\n",
      "simpletransformers==0.70.1\n",
      "torch==2.4.0\n",
      "tqdm==4.66.5\n"
     ]
    }
   ],
   "source": [
    "# Get locally imported modules from current notebook - https://stackoverflow.com/questions/40428931/package-for-listing-version-of-packages-used-in-a-jupyter-notebook - Alex P. Miller\n",
    "def get_imports():\n",
    "    \n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "            \n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system/pip names. Unfortunately,\n",
    "        # there is no systematic way to get pip names from\n",
    "        # a package's imported name. You'll have to add\n",
    "        # exceptions to this list manually!\n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "            \n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "# The only way I found to get the version of the root package\n",
    "# from only the name of the package is to cross-check the names \n",
    "# of installed packages vs. imported packages\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enc2017 corpus\n",
    "\n",
    "Data has been gathered from the [corpus](https://github.com/estnltk/eval_experiments_lrec_2020/blob/master/scripts_and_data/enc2017_selection_plain_texts_json.zip) that was used in evaluation experiments reported in LREC 2020 paper \"EstNLTK 1.6: Remastered Estonian NLP Pipeline\"[^1].\n",
    "\n",
    "[^1]: Sven Laur, Siim Orasmaa, Dage Särg, Paul Tammo. \"EstNLTK 1.6: Remastered Estonian NLP Pipeline\". *Proceedings of The 12th Language Resources and Evaluation Conference*. European Language Resources Association: Marseille, France, May 2020, p. 7154-7162"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='enc_korpus'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defined functions <!-- Defineeritud funktsioonid -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='defineeritud_funktsioonid'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading text from files, morphologically tagging texts and save tagged texts file by file into JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_file_by_file_enc2017(\n",
    "    jsons: typing.List[str], \n",
    "    in_dir: str, \n",
    "    save_dir: str, \n",
    "    do_morph_layer: bool = True, \n",
    "    bert_morph_tagger: typing.Optional[BertMorphTagger] = None, \n",
    "    necessary_layers: typing.List[str] = ['words', 'sentences', 'morph_analysis', 'bert_morph_tagging']\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates a JSON file for each text file.\n",
    "    <ul>\n",
    "        <li>Skips JSON files that have already been created.</li>\n",
    "        <li>Converts JSON file into EstNLTK Text object.</li>\n",
    "        <li>Adds text type metadata and morph analysis.</li>\n",
    "        <li>Adds <code>BertMorphTagger</code> layer</li>\n",
    "        <li>Removes unnecessary layers.</li>\n",
    "        <li>Converts EstNLTK Text object into JSON using <code>estnltk.converters.text_to_json.</code></li>\n",
    "    </ul>\n",
    "    Args:\n",
    "        jsons (list): List of json files from which to read in the text\n",
    "        in_dir (str): Directory where to read the json files from\n",
    "        save_dir (str): Directory where to save the new json files\n",
    "        bert_morph_tagger (optional, BertMorphTagger): Configured <code>BertMorphTagger</code> class instance, if None, will not use this tagger\n",
    "        necessary_layers (optional, list[str]): Text object layers that will not be deleted\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Beginning to morphologically tag file by file\")\n",
    "    for file_name in tqdm(jsons):\n",
    "\n",
    "        # Skipping previous JSON files\n",
    "        if os.path.exists(os.path.join(save_dir, file_name)):\n",
    "            continue\n",
    "\n",
    "        # Convert json to EstNLTK Text object\n",
    "        text = estnltk.converters.json_to_text(file=os.path.join(in_dir, file_name))\n",
    "\n",
    "        # Add text type metadata\n",
    "        text_type = text.meta.get('texttype') # Text type\n",
    "        if not text_type:\n",
    "            if file_name.startswith('wiki17'):\n",
    "                text.meta.update({'texttype': 'wikipedia'})\n",
    "            elif file_name.startswith('web13'):\n",
    "                text.meta.update({'texttype': 'blogs_and_forums'})\n",
    "            else:\n",
    "                raise RuntimeError(\"Could not assign text type\")\n",
    "\n",
    "        # Add morph layer\n",
    "        if do_morph_layer:\n",
    "            text.tag_layer('morph_analysis')\n",
    "        # Add BERT morph layer\n",
    "        if isinstance(bert_morph_tagger, BertMorphTagger):\n",
    "            if not do_morph_layer:\n",
    "                text.tag_layer('sentences')\n",
    "            text.add_layer(bert_morph_tagger.make_layer(text))\n",
    "\n",
    "        # Remove unnecessary layers\n",
    "        for layer in text.layers:\n",
    "            if layer not in necessary_layers:\n",
    "                text.pop_layer(layer, cascading=False)\n",
    "\n",
    "        if 'morph_analysis' in text.layers and 'bert_morph_tagging' in text.layers: # Assertion that the length of both layers are the same\n",
    "            assert len(text.morph_analysis) == len(text.bert_morph_tagging), \\\n",
    "            f\"\"\"Failed to assert file '{file_name}'\n",
    "            Length of layers aren't the same:\n",
    "            morph_analysis = {len(text.morph_analysis)}\n",
    "            bert_morph_tagging = {len(text.bert_morph_tagging)}\"\"\"\n",
    "        # Save to JSON\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        estnltk.converters.text_to_json(text=text, file=os.path.join(save_dir, file_name))\n",
    "\n",
    "    print(\"Morphological tagging completed successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading text from files, morphologically tagging texts and save tagged texts file by file into CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_file_by_file_enc2017(\n",
    "    jsons: typing.List[str], \n",
    "    in_dir: str, \n",
    "    csv_dir: str\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates a CSV file for each text file. \\n\n",
    "    Skips CSV files that have already been created. \\n\n",
    "    For each <code>.json</code> file, the following info is gathered:\n",
    "    <ul>\n",
    "        <li><code>sentence_id</code> -- given for each sentence</li>\n",
    "        <li><code>words</code> -- words gathered from text</li>\n",
    "        <li><code>form</code> -- word form notation</li>\n",
    "        <li><code>pos</code> -- part of speech</li>\n",
    "        <li><code>type</code> -- text type (i.e. genre)</li>\n",
    "        <li><code>source</code> -- file name where the text is taken from</li>\n",
    "    </ul>\n",
    "    <a href=\"https://github.com/Filosoft/vabamorf/blob/e6d42371006710175f7ec328c98f90b122930555/doc/tagset.md\">Tables of morphological categories</a> for more information about <code>form</code> and <code>pos</code>.\n",
    "\n",
    "    Args:\n",
    "        jsons (list): List of json files from which to read in the text\n",
    "        in_dir (str): Directory where to read the json files from\n",
    "        csv_dir (str): Directory where to save the new csv files\n",
    "    \"\"\"\n",
    "    print(\"Beginning to morphologically tag file by file\")\n",
    "    for file_name in tqdm(jsons):\n",
    "        tokens = list()\n",
    "        sentence_id = 0\n",
    "\n",
    "        # Skipping previous CSV files\n",
    "        csv_file_name = file_name[:-4]+'csv'\n",
    "        if os.path.exists(os.path.join(csv_dir, csv_file_name)):\n",
    "            # print(f\"Skipping {file_name} as {csv_file_name} already exists.\")\n",
    "            continue\n",
    "\n",
    "        # print(f\"Beginning to tag {file_name}\")\n",
    "\n",
    "        # Morph. tagging using estnltk\n",
    "        text = estnltk.converters.json_to_text(file=os.path.join(in_dir, file_name))\n",
    "        text_type = text.meta.get('texttype') # Text type\n",
    "        if file_name.startswith('wiki17'):\n",
    "            text_type = 'wikipedia'\n",
    "        elif file_name.startswith('web13'):\n",
    "            text_type = 'blogs_and_forums'\n",
    "        morph_analysis = text.tag_layer('morph_analysis')\n",
    "        for sentence in morph_analysis.sentences:\n",
    "            sentence_analysis = sentence.morph_analysis\n",
    "            for text, form, pos in zip(sentence_analysis.text, sentence_analysis.form, sentence_analysis.partofspeech):\n",
    "                if text:\n",
    "                    tokens.append((sentence_id, text, form[0], pos[0], text_type, file_name)) # In case of multiplicity, select the first or index 0\n",
    "            sentence_id += 1\n",
    "\n",
    "        # print(f\"{file_name} tagged, now saving\")\n",
    "\n",
    "        # Salvestamine\n",
    "        with open(os.path.join(csv_dir, csv_file_name), 'w') as f:\n",
    "            fieldnames = ['sentence_id', 'word', 'form', 'pos']\n",
    "            writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(fieldnames)\n",
    "            for row in tokens:\n",
    "                writer.writerow(row)\n",
    "\n",
    "        # print(f\"{file_name} saved to {csv_file_name}\\n\")\n",
    "\n",
    "    print(\"Morphological tagging completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading text from files, morphologically tagging texts and save tagged texts into one whole dataset. <!-- Failidest teksti lugemine, märgendamine ja salvestamine kokku üheks andmestikuks -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_enc2017_csv(\n",
    "    jsons: typing.List[str], \n",
    "    in_dir: str, \n",
    "    csv_filename: str\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates a new dataset from converted the Estonian UD EDT <a href=\"https://github.com/UniversalDependencies/UD_Estonian-EDT\">corpus</a>. \\n\n",
    "    For each <code>.json</code> file, the following info is gathered:\n",
    "    <ul>\n",
    "        <li><code>sentence_id</code> -- given for each sentence</li>\n",
    "        <li><code>words</code> -- words gathered from text</li>\n",
    "        <li><code>form</code> -- word form notation</li>\n",
    "        <li><code>pos</code> -- part of speech</li>\n",
    "        <li><code>file_prefix</code> -- metadata</li>\n",
    "        <li><code>source</code> -- file name where the text is taken from</li>\n",
    "    </ul>\n",
    "    <a href=\"https://github.com/Filosoft/vabamorf/blob/e6d42371006710175f7ec328c98f90b122930555/doc/tagset.md\">Tables of morphological categories</a> for more information about <code>form</code> and <code>pos</code>.\n",
    "\n",
    "    Args:\n",
    "        jsons (list): List of json files from which to read in the text\n",
    "        in_dir (str): Directory containing list of files (<code>jsons</code>)\n",
    "        csv_filename (str): CSV filename where to save the gathered text\n",
    "    \"\"\"\n",
    "    tokens = list()\n",
    "    sentence_id = 0\n",
    "    fieldnames = ['sentence_id', 'words', 'form', 'pos', 'type', 'source']\n",
    "\n",
    "    print(\"Beginning to morphologically tag file by file. This can take a while.\")\n",
    "    for file_name in tqdm(jsons):\n",
    "        # print(f\"Beginning to tag {file_name}\")\n",
    "        sentence_id = 0\n",
    "\n",
    "        # Morph. tagging\n",
    "        text = estnltk.converters.json_to_text(file=os.path.join(in_dir, file_name))\n",
    "        text_type = text.meta.get('texttype') # Text type\n",
    "        if file_name.startswith('wiki17'):\n",
    "            text_type = 'wikipedia'\n",
    "        elif file_name.startswith('web13'):\n",
    "            text_type = 'blogs_and_forums'\n",
    "        morph_analysis = text.tag_layer('morph_analysis')\n",
    "        for sentence in morph_analysis.sentences:\n",
    "            sentence_analysis = sentence.morph_analysis\n",
    "            for text, form, pos in zip(sentence_analysis.text, sentence_analysis.form, sentence_analysis.partofspeech):\n",
    "                if text:\n",
    "                    tokens.append((sentence_id, text, form[0], pos[0], text_type, file_name)) # In case of multiplicity, select the first or index 0\n",
    "            sentence_id += 1\n",
    "        # print(f\"{file_name} tagged\")\n",
    "\n",
    "    print(\"Morphological tagging completed successfully\")\n",
    "    print(\"Creating Pandas dataframe\")\n",
    "    df = pd.DataFrame(data=tokens, columns=fieldnames)\n",
    "    df.to_csv(path_or_buf=csv_filename, index=False)\n",
    "    print(f\"Tagged texts saved to {csv_filename}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning dataset by:\n",
    "    <ul>\n",
    "        <li>filling NaN values in columns <code>form</code> and <code>pos</code> with empty strings;</li>\n",
    "        <li>removing NaN words.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(\n",
    "    df: pd.DataFrame, \n",
    "    df_file_name: typing.Optional[str] = None\n",
    "    ):\n",
    "    \"\"\"Finishes dataframe by:\n",
    "    <ul>\n",
    "        <li>filling NaN values in columns <code>form</code> and <code>pos</code> with empty strings;</li>\n",
    "        <li>removing NaN words.</li>\n",
    "    </ul>\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Pandas dataframe to clean\n",
    "        df_file_name (str): CSV file name from which dataframe was created\n",
    "    \"\"\"\n",
    "    print(\"Assigning NaN values in columns form and pos with an empty string\")\n",
    "    # NaN values are assigned with an empty string\n",
    "    df['form'] = df['form'].fillna('')\n",
    "    df['pos'] = df['pos'].fillna('')\n",
    "    print(\"Removing NaN words\")\n",
    "    # Removing NaN words\n",
    "    df.dropna(subset=['words'], inplace=True)\n",
    "    if df_file_name:\n",
    "        df.to_csv(path_or_buf=df_file_name, index=False)\n",
    "        print(f\"Modified dataframe saved to {df_file_name}\")\n",
    "    else:\n",
    "        print(\"Dataframe cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a new column `labels` concatenating the values of columns `pos` (part of speech) and `form` (word form notation)<!-- Uue veeru `labels` loomine, mis konkateneerib veergude `pos`(sõnaliik) ja `form`(sõnavormi tähistus) väärtused -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New 'labels' column\n",
    "def create_labels_column(\n",
    "    df: pd.DataFrame, \n",
    "    df_file_name: typing.Optional[str] = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates a new column <code>labels</code> concatenating the values of columns <code>pos</code> (part of speech) and <code>form</code> (word form notation)\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Pandas dataframe to create a new column\n",
    "        df_file_name (str): CSV file name from which dataframe was created\n",
    "    \"\"\"\n",
    "    print(\"Creating column 'labels'\")\n",
    "    df['labels'] = df.apply(lambda row: str(row['form']) + '_' + str(row['pos']) if row['form'] and row['pos'] else str(row['form']) or str(row['pos']), axis=1)\n",
    "    print(\"Column 'labels' created\")\n",
    "    if df_file_name:\n",
    "        df.to_csv(path_or_buf=df_file_name, index=False)\n",
    "        print(f\"Modified dataframe saved to {df_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining unique tags that the model must predict<!-- Unikaalsete väärtuste saamine, mida mudel peab hakkama ennustama -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_labels():\n",
    "    \"\"\"Creates list of unique labels that the model must predict by creating all possible combinations of POS (Part Of Speech) and form.\n",
    "\n",
    "    <i>Gathering unique labels from the enc2017 database proved to be insufficient for future model evaluation,\n",
    "    because the database does not contain all possible combinations of POS and form.\n",
    "    Evaluating model with UD Est-EDT test corpus proved that this problem existed.</i>\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique labels\n",
    "    \"\"\"\n",
    "    # Separately, if one of two doesn't exist\n",
    "    pos_labels = ['A', 'C', 'D', 'G', 'H', 'I', 'J', 'K', 'N', 'O', 'P', 'S', 'U', 'V', 'X', 'Y', 'Z']\n",
    "    form_labels = ['ab', 'abl', 'ad', 'adt', 'all', 'el', 'es', 'g', 'ill', 'in', 'kom', 'n', 'p', 'pl', 'sg', 'ter', 'tr', 'b', 'd', 'da', 'des', 'ge', 'gem', 'gu', 'ks', 'ksid', 'ksime', 'ksin', 'ksite', 'ma', 'maks', 'mas', 'mast', 'mata', 'me', 'n', 'neg', 'nud', 'nuks', 'nuksid', 'nuksime', 'nuksin', 'nuksite', 'nuvat', 'o', 's', 'sid', 'sime', 'sin', 'site', 'ta', 'tagu', 'taks', 'takse', 'tama', 'tav', 'tavat', 'te', 'ti', 'tud', 'tuks', 'tuvat', 'v', 'vad', 'vat']\n",
    "\n",
    "    pos_labels_mutable = ['A', 'C', 'N', 'H', 'O', 'P', 'S', 'U', 'Y', 'X']\n",
    "    #pos_labels_immutable = ['D', 'G', 'I', 'J', 'K', 'X', 'Y', 'Z']\n",
    "    pos_label_verb = ['V']\n",
    "    form_labels_noun = ['ab', 'abl', 'ad', 'adt', 'all', 'el', 'es', 'g', 'ill', 'in', 'kom', 'n', 'p', 'ter', 'tr']\n",
    "    form_labels_noun_count = ['pl', 'sg']\n",
    "    form_labels_verb = ['b', 'd', 'da', 'des', 'ge', 'gem', 'gu', 'ks', 'ksid', 'ksime', 'ksin', 'ksite', 'ma', 'maks', 'mas', 'mast', 'mata', 'me', 'n', 'neg', 'neg ge', 'neg gem', 'neg gu', 'neg ks', 'neg me', 'neg nud', 'neg nuks', 'neg o', 'neg vat', 'neg tud', 'nud', 'nuks', 'nuksid', 'nuksime', 'nuksin', 'nuksite', 'nuvat', 'o', 's', 'sid', 'sime', 'sin', 'site', 'ta', 'tagu', 'taks', 'takse', 'tama', 'tav', 'tavat', 'te', 'ti', 'tud', 'tuks', 'tuvat', 'v', 'vad', 'vat']\n",
    "\n",
    "    noun_labels_without_pos = list(itertools.product(form_labels_noun_count, form_labels_noun))\n",
    "    noun_labels_nested = list(itertools.product(noun_labels_without_pos, pos_labels_mutable))\n",
    "    form_pos_labels = list(itertools.product(form_labels, pos_labels))\n",
    "    noun_labels = list()\n",
    "    verb_labels = list(itertools.product(form_labels_verb, pos_label_verb))\n",
    "\n",
    "    # Connect count and form in mutables\n",
    "    for form, pos in noun_labels_nested:\n",
    "        noun_labels.append((form[0] + ' ' + form[1], pos))\n",
    "\n",
    "    pos_label_only = [('', pos) for pos in pos_labels]\n",
    "    form_label_only = [(form, '') for form in form_labels]\n",
    "    unknown_form_labels = [('?', pos) for pos in pos_labels] # form '?' comes from enc2017 corpus after tagging\n",
    "\n",
    "    unique_labels = pos_label_only + form_label_only + noun_labels + verb_labels + unknown_form_labels + form_pos_labels + ['?'] # '?' for labels unknown to Vabamorf\n",
    "\n",
    "    unique_labels_df = pd.DataFrame(unique_labels, columns=['form', 'pos'])\n",
    "    create_labels_column(unique_labels_df)\n",
    "    unique_labels_df.drop(labels=['form', 'pos'], axis=1)\n",
    "    print(\"List of unique labels created\")\n",
    "    return unique_labels_df['labels'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that collects texts for each text type *more or less* in proportion to the number of words given as `n` <!-- Funktsioon, millega kogutakse iga tekstiliigi kohta tekste enam-vähem proportsionaalselt sõnade arvu suhtes -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collect texts file by file\n",
    "def gather_rows_for_text_type(\n",
    "    df: pd.DataFrame, \n",
    "    n: int, \n",
    "    random_state: typing.Optional[int] = None\n",
    "    ):\n",
    "    \"\"\"Gathers about `n` (>= n) rows for each text type\\n\n",
    "    Ensures that all text types have about the same number of words.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the text data.\n",
    "        n (int): Number of words to gather.\n",
    "        random_state (optional, int): Seed for the shuffle function (acts like <code>random_state</code>).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Gathered rows for each text type.\n",
    "    \"\"\"\n",
    "\n",
    "    def gather_rows_for_type(\n",
    "        group: pd.Group, \n",
    "        n: int, \n",
    "        random_state: typing.Optional[int] = None\n",
    "        ):\n",
    "        \"\"\"Gathers about `n` (>= n) rows for the text type\\n\n",
    "\n",
    "        Args:\n",
    "            group (): Pandas dataframe group\n",
    "            n (int): Number of words to gather\n",
    "            random_state (int): Seed for the shuffle function (acts like <code>random_state</code>)\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Gathered rows for text type\n",
    "        \"\"\"\n",
    "        gathered_rows = pd.DataFrame()\n",
    "        sources = group['source'].unique()\n",
    "\n",
    "        if random_state:\n",
    "            np.random.seed(random_state)\n",
    "            np.random.shuffle(sources)\n",
    "\n",
    "        for source in sources:\n",
    "            source_rows = group[group['source'] == source]\n",
    "            gathered_rows = pd.concat([gathered_rows, source_rows])\n",
    "            if len(gathered_rows) >= n:\n",
    "                break\n",
    "\n",
    "        return gathered_rows\n",
    "\n",
    "    grouped = df.groupby('type')\n",
    "    data = pd.concat([gather_rows_for_type(group, n, random_state) for _, group in grouped])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to predict tags to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, sentences):\n",
    "    \"\"\"Predicts tags to a sentence or number of sentences\n",
    "\n",
    "    Args:\n",
    "        model: Model to use to predict tags.\n",
    "        sentences (List[str] or str): List of sentences strings or a sentence string\n",
    "\n",
    "    Raises:\n",
    "        TypeError: Variable <code>sentences</code> is not List[str] or str\n",
    "\n",
    "    Returns:\n",
    "        (predictions, raw_outputs): Predictions and model's raw output\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(sentences, str):\n",
    "        text = estnltk.Text(sentences)\n",
    "        text.tag_layer(\"morph_analysis\")\n",
    "        text.morph_analysis;\n",
    "        sentences = [s.text for s in text.sentences]\n",
    "        predictions, raw_outputs = model.predict(sentences, split_on_space=False)\n",
    "\n",
    "    elif isinstance(sentences, list) and isinstance(sentences[0], list):\n",
    "        predictions, raw_outputs = model.predict(sentences, split_on_space=False)\n",
    "\n",
    "    else:\n",
    "        raise TypeError(f\"Input is in wrong format. Possible formats are str or list of lists. Your input is {type(sentences)}\")\n",
    "\n",
    "    return predictions, raw_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for predicting top n tags to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_tags(raw_outputs, tag_list, n=3, with_confidence=True):\n",
    "    \"\"\"Extract the top <code>n</code> tags and their probabilities for each word based on the raw output logits.\n",
    "\n",
    "    Args:\n",
    "        raw_outputs (list): Raw prediction logits from the model.\n",
    "        tag_list (list): List of all possible tags.\n",
    "        n (int): Number of top tags to extract.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists, where each sublist contains tuples of the top `n` tags and their probabilities for a word.\n",
    "    \"\"\"\n",
    "    top_n_tags = []\n",
    "\n",
    "    for sentence_logits in raw_outputs:\n",
    "        for word_logits in sentence_logits:\n",
    "\n",
    "            # Get the logits for the word\n",
    "            logits = list(word_logits.values())[0][0]\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "\n",
    "            # Get the indices of the top `n` probabilities\n",
    "            top_n_indices = np.argsort(probabilities)[-n:][::-1]\n",
    "\n",
    "            # Map these indices to the actual tags and their probabilities\n",
    "            if with_confidence:\n",
    "                top_n_tags.append(\n",
    "                    [(tag_list[i], probabilities[i]) for i in top_n_indices]\n",
    "                )\n",
    "            else:\n",
    "                top_n_tags.append(\n",
    "                    [(tag_list[i]) for i in top_n_indices]\n",
    "                )\n",
    "\n",
    "    return top_n_tags\n",
    "\n",
    "def predict_top_n_tags(model, to_predict, with_confidence=True):\n",
    "    \"\"\"Predicts top <code>n</code> tags and their probabilities for each word.\n",
    "\n",
    "    Args:\n",
    "        to_predict (list): A python list of text (str) to be sent to the model for prediction.\n",
    "        Must be a a list of lists, with the inner list being a list of strings consisting of the split sequences. \n",
    "        The outer list is the list of sequences to predict on.\n",
    "        with_confidence (bool): Whether to output confidence values for each tag.\n",
    "    \"\"\"\n",
    "    _, raw_outputs = model.predict(to_predict, split_on_space=False)\n",
    "\n",
    "    tag_list = model.config.id2label  # Assuming the model has a mapping from ids to labels\n",
    "    top_n_tags_per_word = get_top_n_tags(raw_outputs, tag_list, n=3, with_confidence=with_confidence)\n",
    "\n",
    "    for sentence in to_predict:\n",
    "        for word, tags in zip(sentence, top_n_tags_per_word):\n",
    "            print(f\"Word: {word}\")\n",
    "            if with_confidence:\n",
    "                for tag, confidence in tags:\n",
    "                    print(f\"\\tTag: {tag} \\t Confidence: {confidence:.4f}\")\n",
    "            else:\n",
    "                for tag in tags:\n",
    "                    print(f\"\\tTag: {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to predict labels to the group of sentences. Includes sentence splitting into clauses and clauses into equal length clauses' parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_token_count(model, sentence):\n",
    "    \"\"\"Checks token count in the sentence\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        sentence (list): list of words in a sentence\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the token count exceeds model's maximum sequence length\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = model.tokenizer('  '.join(sentence), return_tensors=\"pt\")\n",
    "    return bool(len(inputs[\"input_ids\"][0]) >= model.args.max_seq_length)\n",
    "\n",
    "def get_clause_parts(model, clause):\n",
    "    \"\"\"Splits clause into equal length clause segments. <i>Clauses can get long when there is a list in a sentence</i>\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        clause (list): list of words in a clause\n",
    "\n",
    "    Returns:\n",
    "        list: List of clause segments. Each segment is a list of words.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = model.tokenizer('  '.join(clause), return_tensors=\"pt\")\n",
    "    clause_parts = np.array_split(clause, math.ceil(len(inputs[\"input_ids\"][0]) / model.args.max_seq_length))\n",
    "    return clause_parts\n",
    "\n",
    "def get_clauses(model, sentence):\n",
    "    \"\"\"Splits sentence into clauses using EstNLTK clauses layer.\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        sentence (list): list of words in a sentence\n",
    "\n",
    "    Returns:\n",
    "        list: List of clauses. Each clause is a list of words.\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_text = '  '.join(sentence)\n",
    "    text = estnltk.Text(sentence_text)\n",
    "    text.tag_layer('clauses')\n",
    "    clauses = list()\n",
    "    for clause in text.clauses:\n",
    "        if check_token_count(model, clause.text):\n",
    "            clause_parts = get_clause_parts(model, clause.text)\n",
    "            clauses.extend(clause_parts)\n",
    "        else:\n",
    "            clauses.append(clause.text)\n",
    "    return clauses\n",
    "\n",
    "def get_clauses_labels(model, sentence):\n",
    "    \"\"\"Predicts labels to clauses\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        sentence (list): list of words in a sentence\n",
    "\n",
    "    Returns:\n",
    "        list: List of tags predicted to words in clauses\n",
    "    \"\"\"\n",
    "\n",
    "    clauses = get_clauses(model, sentence)\n",
    "\n",
    "    ner_labels_parts = list()\n",
    "\n",
    "    # Predict tags\n",
    "    predictions, raw_outputs = model.predict(clauses, split_on_space=False)\n",
    "    for prediction_part in predictions:\n",
    "        # ner_words_part = [list(p.keys())[0] for p in prediction_part]\n",
    "        ner_labels_part = [list(p.values())[0] for p in prediction_part]\n",
    "\n",
    "        # ner_words_parts.append(ner_words_part)\n",
    "        ner_labels_parts.append(ner_labels_part)\n",
    "\n",
    "    # ner_words = list(itertools.chain.from_iterable(ner_words_parts))\n",
    "    ner_labels = list(itertools.chain.from_iterable(ner_labels_parts))\n",
    "\n",
    "    return ner_labels\n",
    "\n",
    "def get_sentence_labels(model, sentence):\n",
    "    \"\"\"Predicts labels to a sentence\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        sentence (list): list of words in a sentence\n",
    "\n",
    "    Returns:\n",
    "        list: List of tags predicted to words in a sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict tags\n",
    "    predictions, raw_outputs = model.predict([sentence], split_on_space=False)\n",
    "\n",
    "    # ner_words = [list(p.keys())[0] for p in predictions[0]]\n",
    "    ner_labels = [list(p.values())[0] for p in predictions[0]]\n",
    "    return ner_labels\n",
    "\n",
    "def process_groups(model, groups):\n",
    "    \"\"\"Predicts labels to a list of groups. This group contains sentences for each source text file.\n",
    "\n",
    "    Args:\n",
    "        model (): NER model that is predicted with\n",
    "        groups (): Group containing sentences for each source text file\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: When the length of predicted labels mismatch the length of the sentence length. \n",
    "        <i>This might mean that generated clauses are wrong (meaning some words are missing because of <code>'  '.join(sentence)</code>)</i>\n",
    "\n",
    "    Returns:\n",
    "        list: List of predicted labels for each sentence in each source text file\n",
    "    \"\"\"\n",
    "    chunk_results = []\n",
    "\n",
    "    for _, group in groups:\n",
    "        sentence = group.words.tolist()\n",
    "\n",
    "        if check_token_count(model, sentence):  # Sentence splitting if token count is above model's max sequence length\n",
    "            ner_labels = get_clauses_labels(model, sentence)\n",
    "        else:\n",
    "            ner_labels = get_sentence_labels(model, sentence)\n",
    "\n",
    "        if len(ner_labels) != len(sentence):\n",
    "            display(group)\n",
    "            print(len(sentence), len(ner_labels))\n",
    "            raise AssertionError(\"Predicted labels length mismatch sentence length.\")\n",
    "\n",
    "        group['ner_labels'] = ner_labels\n",
    "        chunk_results.append(group)\n",
    "\n",
    "    return chunk_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting data and applying morphological analysis <!-- Andmete lugemine kogumine ja margendamine -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='andmete_lugemine_kogumine_ja_margendamine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading text from files, morphologically tagging texts and save them file by file. <!--Failidest teksti lugemine, märgendamine ja salvestamine failhaaval -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting `.json` files from directory <!--`json` failide kogumine kaustast-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = '_plain_texts_json'\n",
    "\n",
    "jsons = os.listdir(in_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data creation and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./andmestik.csv'):\n",
    "    create_df_enc2017_csv(jsons, in_dir, 'andmestik.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning NaN values in columns form and pos with an empty string\n",
      "Removing NaN words\n",
      "Modified dataframe saved to andmestik.csv\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"andmestik.csv\"\n",
    "df = pd.read_csv(csv_file, keep_default_na=False)\n",
    "clean_df(df, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'labels' created\n"
     ]
    }
   ],
   "source": [
    "create_labels_column(df, csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verification <!-- Kontrollimine -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kontrollimine_1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring that correct info was collected from the json file<!-- Kontrollitakse, et json failist said õiged infokillud kogutud -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For JSON file\n",
    "text = estnltk.converters.json_to_text(file='.\\\\_plain_texts_json\\\\nc_255_27981.json')\n",
    "morph_analysis = text.tag_layer('morph_analysis')\n",
    "ma_json = morph_analysis.sentences[0].morph_analysis\n",
    "test_json = pd.DataFrame(data=list(zip(ma_json.text, ma_json.form, ma_json.partofspeech)), columns=(\"text\", \"form\", \"partofspeech\"))\n",
    "# display(test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CSV file\n",
    "lines = list()\n",
    "\n",
    "with open('.\\\\_plain_texts_csv\\\\nc_255_27981.csv', 'r') as f:\n",
    "    csvFile = csv.reader(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for line in csvFile:\n",
    "        if line:\n",
    "            lines.append(line)\n",
    "\n",
    "test_csv = pd.DataFrame(lines[1:], columns=lines[0])\n",
    "\n",
    "# display(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Pandas dataframe\n",
    "test_csv = df[df['source'] == 'nc_255_27981.json']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the first sentence in file `nc_255_27981`<!-- Näide esimesest lausest failis `nc_255_27981` -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>form</th>\n",
       "      <th>partofspeech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>samas</td>\n",
       "      <td>(sg in)</td>\n",
       "      <td>(P)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kestab</td>\n",
       "      <td>(b)</td>\n",
       "      <td>(V)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>kohtuvaidlus</td>\n",
       "      <td>(sg n)</td>\n",
       "      <td>(S)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>veel</td>\n",
       "      <td>()</td>\n",
       "      <td>(D)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30</td>\n",
       "      <td>(?)</td>\n",
       "      <td>(N)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FIEna</td>\n",
       "      <td>(sg es)</td>\n",
       "      <td>(Y)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ametis</td>\n",
       "      <td>(sg in)</td>\n",
       "      <td>(S)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>olnud</td>\n",
       "      <td>(nud, , sg n, pl n)</td>\n",
       "      <td>(V, A, A, A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>töötaja</td>\n",
       "      <td>(sg g)</td>\n",
       "      <td>(S)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>palga</td>\n",
       "      <td>(sg g)</td>\n",
       "      <td>(S)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kompenseerimiseks</td>\n",
       "      <td>(sg tr)</td>\n",
       "      <td>(S)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>,</td>\n",
       "      <td>()</td>\n",
       "      <td>(Z)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>teatas</td>\n",
       "      <td>(s)</td>\n",
       "      <td>(V)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ETA</td>\n",
       "      <td>(?)</td>\n",
       "      <td>(Y)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>.</td>\n",
       "      <td>()</td>\n",
       "      <td>(Z)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text                 form  partofspeech\n",
       "15              samas              (sg in)           (P)\n",
       "16             kestab                  (b)           (V)\n",
       "17       kohtuvaidlus               (sg n)           (S)\n",
       "18               veel                   ()           (D)\n",
       "19                 30                  (?)           (N)\n",
       "20              FIEna              (sg es)           (Y)\n",
       "21             ametis              (sg in)           (S)\n",
       "22              olnud  (nud, , sg n, pl n)  (V, A, A, A)\n",
       "23            töötaja               (sg g)           (S)\n",
       "24              palga               (sg g)           (S)\n",
       "25  kompenseerimiseks              (sg tr)           (S)\n",
       "26                  ,                   ()           (Z)\n",
       "27             teatas                  (s)           (V)\n",
       "28                ETA                  (?)           (Y)\n",
       "29                  .                   ()           (Z)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>form</th>\n",
       "      <th>pos</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4861762</th>\n",
       "      <td>0</td>\n",
       "      <td>samas</td>\n",
       "      <td>sg in</td>\n",
       "      <td>P</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861763</th>\n",
       "      <td>0</td>\n",
       "      <td>kestab</td>\n",
       "      <td>b</td>\n",
       "      <td>V</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861764</th>\n",
       "      <td>0</td>\n",
       "      <td>kohtuvaidlus</td>\n",
       "      <td>sg n</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861765</th>\n",
       "      <td>0</td>\n",
       "      <td>veel</td>\n",
       "      <td></td>\n",
       "      <td>D</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861766</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>?</td>\n",
       "      <td>N</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861767</th>\n",
       "      <td>0</td>\n",
       "      <td>FIEna</td>\n",
       "      <td>sg es</td>\n",
       "      <td>Y</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861768</th>\n",
       "      <td>0</td>\n",
       "      <td>ametis</td>\n",
       "      <td>sg in</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861769</th>\n",
       "      <td>0</td>\n",
       "      <td>olnud</td>\n",
       "      <td>nud</td>\n",
       "      <td>V</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861770</th>\n",
       "      <td>0</td>\n",
       "      <td>töötaja</td>\n",
       "      <td>sg g</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861771</th>\n",
       "      <td>0</td>\n",
       "      <td>palga</td>\n",
       "      <td>sg g</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861772</th>\n",
       "      <td>0</td>\n",
       "      <td>kompenseerimiseks</td>\n",
       "      <td>sg tr</td>\n",
       "      <td>S</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861773</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861774</th>\n",
       "      <td>0</td>\n",
       "      <td>teatas</td>\n",
       "      <td>s</td>\n",
       "      <td>V</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861775</th>\n",
       "      <td>0</td>\n",
       "      <td>ETA</td>\n",
       "      <td>?</td>\n",
       "      <td>Y</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861776</th>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>Z</td>\n",
       "      <td>periodicals</td>\n",
       "      <td>nc_255_27981.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id              words   form pos         type  \\\n",
       "4861762            0              samas  sg in   P  periodicals   \n",
       "4861763            0             kestab      b   V  periodicals   \n",
       "4861764            0       kohtuvaidlus   sg n   S  periodicals   \n",
       "4861765            0               veel          D  periodicals   \n",
       "4861766            0                 30      ?   N  periodicals   \n",
       "4861767            0              FIEna  sg es   Y  periodicals   \n",
       "4861768            0             ametis  sg in   S  periodicals   \n",
       "4861769            0              olnud    nud   V  periodicals   \n",
       "4861770            0            töötaja   sg g   S  periodicals   \n",
       "4861771            0              palga   sg g   S  periodicals   \n",
       "4861772            0  kompenseerimiseks  sg tr   S  periodicals   \n",
       "4861773            0                  ,          Z  periodicals   \n",
       "4861774            0             teatas      s   V  periodicals   \n",
       "4861775            0                ETA      ?   Y  periodicals   \n",
       "4861776            0                  .          Z  periodicals   \n",
       "\n",
       "                    source  \n",
       "4861762  nc_255_27981.json  \n",
       "4861763  nc_255_27981.json  \n",
       "4861764  nc_255_27981.json  \n",
       "4861765  nc_255_27981.json  \n",
       "4861766  nc_255_27981.json  \n",
       "4861767  nc_255_27981.json  \n",
       "4861768  nc_255_27981.json  \n",
       "4861769  nc_255_27981.json  \n",
       "4861770  nc_255_27981.json  \n",
       "4861771  nc_255_27981.json  \n",
       "4861772  nc_255_27981.json  \n",
       "4861773  nc_255_27981.json  \n",
       "4861774  nc_255_27981.json  \n",
       "4861775  nc_255_27981.json  \n",
       "4861776  nc_255_27981.json  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_json[-15:])\n",
    "display(test_csv[test_csv.sentence_id == 0][-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics <!-- Statistika -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='statistika'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file\n",
    "csv_file = \"andmestik.csv\"\n",
    "df = pd.read_csv(csv_file, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token count in each text type<!-- Sõnade arv igal tekstiliigil -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "type\n",
       "fiction             2170692\n",
       "science             2121583\n",
       "periodicals         2099374\n",
       "wikipedia           2007179\n",
       "blogs_and_forums    1991307\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 10390135\n"
     ]
    }
   ],
   "source": [
    "print(\"Word count\")\n",
    "display(df['type'].value_counts())\n",
    "print(f'Sum: {df['type'].count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence count in each text type<!-- Lausete arv igal tekstiliigil -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence count\n",
      "periodicals: 144979\n",
      "fiction: 165249\n",
      "science: 129810\n",
      "blogs_and_forums: 159826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia: 136377\n",
      "\n",
      "Sum: 736241\n"
     ]
    }
   ],
   "source": [
    "text_types = df['type'].unique().tolist()\n",
    "print(\"Sentence count\")\n",
    "for text_type in text_types:\n",
    "    sentence_grouped = df[df['type'] == text_type].groupby(['source', 'sentence_id'])\n",
    "    sentence_count = sentence_grouped.ngroups\n",
    "    print(f\"{text_type}: {sentence_count}\")\n",
    "sentence_grouped = df.groupby(['source', 'sentence_id'])\n",
    "sentence_count = sentence_grouped.ngroups\n",
    "print(f\"\\nSum: {sentence_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File count in each text type<!-- Failide arv tekstiliigiti -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File count\n",
      "periodicals: 5917\n",
      "fiction: 53\n",
      "science: 229\n",
      "blogs_and_forums: 3016\n",
      "wikipedia: 9270\n",
      "\n",
      "Sum: 18485\n"
     ]
    }
   ],
   "source": [
    "print(\"File count\")\n",
    "for text_type in text_types:\n",
    "    print(f\"{text_type}: {len(df['source'][df['type'] == text_type].value_counts().tolist())}\")\n",
    "print(f\"\\nSum: {len(df['source'].value_counts().tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs word count for each source (i.e. text file) present in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_each_source_word_count(data):\n",
    "    \"\"\"Outputs word count for each source (i.e. text file) present in the data\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Pandas dataframe\n",
    "    \"\"\"\n",
    "    for text_type in text_types:\n",
    "        print(f\"{text_type}, sources: {len(data['source'][data['type'] == text_type].value_counts().tolist())}\")\n",
    "        display(data['source'][data['type'] == text_type].value_counts().head(5))\n",
    "        if len(data['source'][data['type'] == text_type].value_counts().tolist()) > 5:\n",
    "            display(data['source'][data['type'] == text_type].value_counts().tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_each_source_word_count(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data splitting<!-- ### Andmete jagamine treening-, test- ja valideerimishulkadesse -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='andmete_jagamine_treening_test_ja_valideerimishulkadesse'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the data used in training was based on the [statistics](https://github.com/UniversalDependencies/UD_Estonian-EDT/blob/master/stats.xml) found in [Estonian UD EDT treebank](https://github.com/UniversalDependencies/UD_Estonian-EDT). Data is gathered in such a way that all text types have more or less proportionate numbers of words.\n",
    "\n",
    "|Text type|Word count|\n",
    "|---|---|\n",
    "|Fiction|             115847|\n",
    "|Blogs and forums|    115719|\n",
    "|Science|             115347|\n",
    "|Periodicals|         115050|\n",
    "|Wikipedia|           115014|\n",
    "|**Sum**|             576977|\n",
    "\n",
    "The tag that the model starts to predict consists of a concatenation of columns `form` and `pos` connected with underscore _ symbol (e.g. `form` s and `pos` V will create s_V). In the data those concatenations will be stored in the column `labels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grouping data by text type <!-- #### Andmete grupeerimine tekstiliigiti -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='andmete_grupeerimine_tekstiliigiti'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file\n",
    "csv_file = \"andmestik.csv\"\n",
    "df = pd.read_csv(csv_file, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#pos_labels = ['A', 'C', 'D', 'G', 'H', 'I', 'J', 'K', 'N', 'O', 'P', 'S', 'U', 'V', 'X', 'Y', 'Z']\n",
    "#form_labels = ['ab', 'abl', 'ad', 'adt', 'all', 'el', 'es', 'g', 'ill', 'in', 'kom', 'n', 'p', 'pl', 'sg', 'ter', 'tr', 'b', 'd', 'da', 'des', 'ge', 'gem', 'gu', 'ks', 'ksid', 'ksime', 'ksin', 'ksite', 'ma', 'maks', 'mas', 'mast', 'mata', 'me', 'n', 'neg', 'neg ge', 'neg gem', 'neg gu', 'neg ks', 'neg me', 'neg nud', 'neg nuks', 'neg o', 'neg vat', 'neg tud', 'nud', 'nuks', 'nuksid', 'nuksime', 'nuksin', 'nuksite', 'nuvat', 'o', 's', 'sid', 'sime', 'sin', 'site', 'ta', 'tagu', 'taks', 'takse', 'tama', 'tav', 'tavat', 'te', 'ti', 'tud', 'tuks', 'tuvat', 'v', 'vad', 'vat']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels that have '?' in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['?_Y', '?_N', '?_O', '?_H', '?_S', '?_A', '?_Z'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_mark_labels = df[df['labels'].str.contains(r\"\\?\")]\n",
    "question_mark_labels['labels'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating column 'labels'\n",
      "Column 'labels' created\n",
      "List of unique labels created\n"
     ]
    }
   ],
   "source": [
    "unique_labels = get_unique_labels()\n",
    "# display(unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving unique labels for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./unique_labels.json\"):\n",
    "    with open(\"unique_labels.json\", 'w') as f:\n",
    "        json.dump(unique_labels, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gather_rows_for_text_type(df, 115000) #115000 words per text type for NER model\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "fiction             115847\n",
       "blogs_and_forums    115719\n",
       "science             115347\n",
       "periodicals         115050\n",
       "wikipedia           115014\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 576977\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "display(data['type'].value_counts())\n",
    "print(f\"Sum: {data['words'].size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathers text types for evaluating model by text type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_types = data['type'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving model data to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(path_or_buf='model_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting data to sets<!-- #### Andmete jagamine hulkadesse -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='andmete_jagamine_hulkadesse'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file\n",
    "data = pd.read_csv(\"model_data.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping data by filename to preserve the integrity of texts<!-- Andmete grupeerimine failinimede kaupa, et säilitada tekstide terviklikkus -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby('source')\n",
    "\n",
    "groups = list(grouped.groups.keys())\n",
    "train_groups, test_groups = sk.model_selection.train_test_split(groups, test_size=0.2, random_state=42)\n",
    "\n",
    "def filter_by_group(df, groups):\n",
    "    return df[df['source'].isin(groups)]\n",
    "\n",
    "# Splitting dataframe\n",
    "train_df = filter_by_group(data, train_groups)\n",
    "test_df = filter_by_group(data, test_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unnecessary columns for the model<!-- Mudelile ebavajalike veergude eemaldamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(labels=['type', 'source'], axis=1)\n",
    "test_df = test_df.drop(labels=['type', 'source'], axis=1)\n",
    "# display(train_df)\n",
    "# display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token classification model (for assigning morpological categories)<!-- ### NER Mudel -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ner_mudel'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unique_labels.json\", 'r') as f:\n",
    "    unique_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model<!-- Mudeli ülesehitamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at EMBEDDIA/est-roberta and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger('simpletransformers.ner.ner_model')\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Suppress specific warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning) # For warning message \"FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated.\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # For warnings from seqeval.metrics like \"UserWarning: <tag> seems not to be NE tag.\"\n",
    "\n",
    "# Configurations\n",
    "model_args = NERArgs()\n",
    "model_args.train_batch_size = 8\n",
    "model_args.evaluate_during_training = False\n",
    "model_args.learning_rate = 5e-5\n",
    "model_args.num_train_epochs = 10\n",
    "model_args.use_early_stopping = True\n",
    "model_args.use_cuda = torch.cuda.is_available()  # Use GPU if available\n",
    "model_args.save_eval_checkpoints = False\n",
    "model_args.save_model_every_epoch = False # Takes a lot of storage space\n",
    "model_args.save_steps = -1\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.cache_dir = 'NER_mudel/cache'\n",
    "model_args.best_model_dir = 'NER_mudel/best_model'\n",
    "model_args.output_dir = 'NER_mudel'\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.silent = True\n",
    "\n",
    "# Initialization\n",
    "model = NERModel(\"camembert\", \"EMBEDDIA/est-roberta\", args=model_args, labels=unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model<!-- Mudeli treenimine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\".\\\\NER_mudel\\\\config.json\"):\n",
    "    # Train model\n",
    "    print(\"Training model\")\n",
    "    model.train_model(train_df)\n",
    "else:\n",
    "    model = NERModel(\"camembert\", \"NER_mudel\", args=model_args, labels=unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model<!-- Mudeli hindamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:1303: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23933184891939163, 'precision': 0.9545189765649504, 'recall': 0.9534082262901132, 'f1_score': 0.953963278101209}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, preds_list = model.eval_model(test_df)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss:0.2393\n",
      "Precision: \t0.9545\n",
      "Recall: \t0.9534\n",
      "F1 Score: \t0.9540\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluation Loss:{result['eval_loss']:.4f}\")\n",
    "print(f\"Precision: \\t{result['precision']:.4f}\")\n",
    "print(f\"Recall: \\t{result['recall']:.4f}\")\n",
    "print(f\"F1 Score: \\t{result['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Previously, you could see a lot of UserWarnings logged in the output. It did not impose a problem because the model could still predict these tags. This is now fixed by configuring the logger. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model by text type<!-- Mudeli hindamine tekstiliigiti -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text type:\tblogs_and_forums\n",
      "Token count:\t115719\n",
      "Starting evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\envs\\gpulocal\\Lib\\site-packages\\simpletransformers\\ner\\ner_model.py:1303: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Loss:0.0894\n",
      "Precision: \t0.9740\n",
      "Recall: \t0.9733\n",
      "F1 Score: \t0.9737\n",
      "\n",
      "Text type:\tfiction\n",
      "Token count:\t115847\n",
      "Starting evaluation\n",
      "\n",
      "Evaluation Loss:0.0048\n",
      "Precision: \t0.9991\n",
      "Recall: \t0.9990\n",
      "F1 Score: \t0.9990\n",
      "\n",
      "Text type:\tperiodicals\n",
      "Token count:\t115050\n",
      "Starting evaluation\n",
      "\n",
      "Evaluation Loss:0.0968\n",
      "Precision: \t0.9808\n",
      "Recall: \t0.9802\n",
      "F1 Score: \t0.9805\n",
      "\n",
      "Text type:\tscience\n",
      "Token count:\t115347\n",
      "Starting evaluation\n",
      "\n",
      "Evaluation Loss:0.1643\n",
      "Precision: \t0.9678\n",
      "Recall: \t0.9688\n",
      "F1 Score: \t0.9683\n",
      "\n",
      "Text type:\twikipedia\n",
      "Token count:\t115014\n",
      "Starting evaluation\n",
      "\n",
      "Evaluation Loss:0.2907\n",
      "Precision: \t0.9479\n",
      "Recall: \t0.9472\n",
      "F1 Score: \t0.9476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text_type in text_types:\n",
    "\n",
    "    print(f\"Text type:\\t{text_type}\")\n",
    "\n",
    "    text_type_data = data.loc[data['type'] == text_type]\n",
    "\n",
    "    print(f\"Token count:\\t{len(text_type_data)}\")\n",
    "\n",
    "    print(f\"Starting evaluation\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    result, model_outputs, preds_list = model.eval_model(text_type_data)\n",
    "    # print(result)\n",
    "    print()\n",
    "    print(f\"Evaluation Loss:{result['eval_loss']:.4f}\")\n",
    "    print(f\"Precision: \\t{result['precision']:.4f}\")\n",
    "    print(f\"Recall: \\t{result['recall']:.4f}\")\n",
    "    print(f\"F1 Score: \\t{result['f1_score']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions with the model <!-- Mudelil ennustamine -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'A. H. Tammsaare': 'sg n_H'}, {'oli': 's_V'}, {'eesti': 'G'}, {'kirjanik': 'sg n_S'}, {',': 'Z'}, {'esseist': 'sg n_S'}, {',': 'Z'}, {'kultuurifilosoof': 'sg n_S'}, {'ja': 'J'}, {'tõlkija': 'sg n_S'}, {'.': 'Z'}]]\n"
     ]
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict([['A. H. Tammsaare',\n",
    " 'oli',\n",
    " 'eesti',\n",
    " 'kirjanik',\n",
    " ',',\n",
    " 'esseist',\n",
    " ',',\n",
    " 'kultuurifilosoof',\n",
    " 'ja',\n",
    " 'tõlkija',\n",
    " '.']], split_on_space=False)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'A. H. Tammsaare': 'sg n_H'}, {'oli': 's_V'}, {'eesti': 'G'}, {'kirjanik': 'sg n_S'}, {',': 'Z'}, {'esseist': 'sg n_S'}, {',': 'Z'}, {'kultuurifilosoof': 'sg n_S'}, {'ja': 'J'}, {'tõlkija': 'sg n_S'}, {'.': 'Z'}], [{'Üksnes': 'D'}, {'autorihüvitis': 'sg n_S'}, {'oli': 's_V'}, {'12 431': '?_N'}, {'krooni': 'sg p_S'}, {'.': 'Z'}]]\n"
     ]
    }
   ],
   "source": [
    "test_text = estnltk.Text(\"A. H. Tammsaare oli eesti kirjanik, esseist, kultuurifilosoof ja tõlkija. Üksnes autorihüvitis oli 12 431 krooni.\")\n",
    "test_text.tag_layer(\"morph_analysis\")\n",
    "test_text.morph_analysis;\n",
    "sentences = [s.text for s in test_text.sentences]\n",
    "\n",
    "# Predict tags\n",
    "predictions, raw_outputs = model.predict(sentences, split_on_space=False)\n",
    "\n",
    "# Output of predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: A. H. Tammsaare\n",
      "\tTag: sg n_H \t Confidence: 0.9976\n",
      "\tTag: sg g_H \t Confidence: 0.0012\n",
      "\tTag: ?_Y \t Confidence: 0.0001\n",
      "Word: oli\n",
      "\tTag: s_V \t Confidence: 1.0000\n",
      "\tTag: sg n_S \t Confidence: 0.0000\n",
      "\tTag: A \t Confidence: 0.0000\n",
      "Word: eesti\n",
      "\tTag: G \t Confidence: 0.9956\n",
      "\tTag: sg g_S \t Confidence: 0.0004\n",
      "\tTag: sg g_H \t Confidence: 0.0002\n",
      "Word: kirjanik\n",
      "\tTag: sg n_S \t Confidence: 0.9995\n",
      "\tTag: sg in_S \t Confidence: 0.0000\n",
      "\tTag: sg g_S \t Confidence: 0.0000\n",
      "Word: ,\n",
      "\tTag: Z \t Confidence: 1.0000\n",
      "\tTag: J \t Confidence: 0.0000\n",
      "\tTag: A \t Confidence: 0.0000\n",
      "Word: esseist\n",
      "\tTag: sg n_S \t Confidence: 0.9995\n",
      "\tTag: sg n_A \t Confidence: 0.0001\n",
      "\tTag: sg in_S \t Confidence: 0.0000\n",
      "Word: ,\n",
      "\tTag: Z \t Confidence: 1.0000\n",
      "\tTag: J \t Confidence: 0.0000\n",
      "\tTag: s_V \t Confidence: 0.0000\n",
      "Word: kultuurifilosoof\n",
      "\tTag: sg n_S \t Confidence: 0.9995\n",
      "\tTag: sg n_A \t Confidence: 0.0000\n",
      "\tTag: sg in_S \t Confidence: 0.0000\n",
      "Word: ja\n",
      "\tTag: J \t Confidence: 1.0000\n",
      "\tTag: Z \t Confidence: 0.0000\n",
      "\tTag: o_V \t Confidence: 0.0000\n",
      "Word: tõlkija\n",
      "\tTag: sg n_S \t Confidence: 0.9995\n",
      "\tTag: sg g_S \t Confidence: 0.0001\n",
      "\tTag: sg n_A \t Confidence: 0.0001\n",
      "Word: .\n",
      "\tTag: Z \t Confidence: 1.0000\n",
      "\tTag: J \t Confidence: 0.0000\n",
      "\tTag: A \t Confidence: 0.0000\n"
     ]
    }
   ],
   "source": [
    "predict_top_n_tags(model, [['A. H. Tammsaare',\n",
    " 'oli',\n",
    " 'eesti',\n",
    " 'kirjanik',\n",
    " ',',\n",
    " 'esseist',\n",
    " ',',\n",
    " 'kultuurifilosoof',\n",
    " 'ja',\n",
    " 'tõlkija',\n",
    " '.']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token classification model (for assigning morpological categories) NER model by text type<!-- ### NER mudel tekstiliigiti -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ner_mudel_tekstiliigiti'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_type in text_types:\n",
    "\n",
    "    print(f\"Text type:\\t{text_type}\")\n",
    "\n",
    "    text_type_data = data.loc[data['type'] == text_type]\n",
    "\n",
    "    print(f\"Token count: {len(text_type_data)}\")\n",
    "\n",
    "    grouped = data.groupby('source')\n",
    "\n",
    "    groups = list(grouped.groups.keys())\n",
    "    train_groups, test_groups = sk.model_selection.train_test_split(groups, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Split the dataframe\n",
    "    train_df = df[df['source'].isin(train_groups)]\n",
    "    test_df = df[df['source'].isin(test_groups)]\n",
    "\n",
    "    print(f\"Train- and testset created.\\nBeginning to train\")\n",
    "\n",
    "    # Model\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    transformers_logger = logging.getLogger(\"transformers\")\n",
    "    transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "    # Configure the model\n",
    "    model_args = NERArgs()\n",
    "    model_args.train_batch_size = 8\n",
    "    model_args.evaluate_during_training = False\n",
    "    model_args.learning_rate = 5e-5\n",
    "    model_args.num_train_epochs = 10\n",
    "    model_args.use_early_stopping = True\n",
    "    model_args.use_cuda = torch.cuda.is_available()  # Use GPU if available\n",
    "    model_args.overwrite_output_dir = True\n",
    "    model_args.save_eval_checkpoints = False\n",
    "    model_args.save_model_every_epoch = False\n",
    "    model_args.save_steps = -1\n",
    "    model_args.cache_dir = text_type + '_NER_mudel/cache'\n",
    "    model_args.best_model_dir = text_type + '_NER_mudel/best_model'\n",
    "    model_args.output_dir = text_type + '_NER_mudel'\n",
    "\n",
    "    # Initialize the model\n",
    "    model = NERModel(\"camembert\", \"EMBEDDIA/est-roberta\", args=model_args, labels=unique_labels)\n",
    "\n",
    "    # Train the model\n",
    "    model.train_model(train_df, eval_data=test_df)\n",
    "\n",
    "    print(f\"Starting evaluation\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    result, model_outputs, preds_list = model.eval_model(text_type_data)\n",
    "    print(result)\n",
    "    print(f\"Evaluation Loss:{result['eval_loss']:.4f}\")\n",
    "    print(f\"Precision: \\t{result['precision']:.4f}\")\n",
    "    print(f\"Recall: \\t{result['recall']:.4f}\")\n",
    "    print(f\"F1 Score: \\t{result['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New trained and evaluated NER models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Text type         | Token count | eval_loss            | precision          | recall             | f1_score           |\n",
    "|-------------------|-------------|----------------------|--------------------|--------------------|--------------------|\n",
    "| blogs_and_forums  | 115719      | 0.23976334929466248  | 0.9538220130015691 | 0.9520293551707164 | 0.9529248409925647 |\n",
    "| fiction           | 115847      | 0.2396537810564041   | 0.9538792523867151 | 0.952342596321654  | 0.9531103049845493 |\n",
    "| periodicals       | 115050      | 0.24317898601293564  | 0.9550244383659926 | 0.9530585760952253 | 0.9540404945350295 |\n",
    "| science           | 115347      | 0.2376849427819252   | 0.9557577659240665 | 0.9541325457555824 | 0.9549444643496955 |\n",
    "| wikipedia         | 115014      | 0.23429438471794128  | 0.9547409930094999 | 0.953416565982011  | 0.9540783198620781 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General NER model evaluations by text type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Text type         | Token count | eval_loss            | precision          | recall             | f1_score           |\n",
    "|-------------------|-------------|----------------------|--------------------|--------------------|--------------------|\n",
    "| blogs_and_forums  | 115719      | 0.08935269061475992  | 0.974046101995129  | 0.9733038348082595 | 0.9736748269375007 |\n",
    "| fiction           | 115847      | 0.004841572717996314 | 0.9990726034260374 | 0.9989767640162013 | 0.9990246814225794 |\n",
    "| periodicals       | 115050      | 0.09677119304736455  | 0.9807560725282244 | 0.9801692452346354 | 0.9804625710743448 |\n",
    "| science           | 115347      | 0.1642883848398924   | 0.9677601809954751 | 0.9688169868554095 | 0.9682882955716798 |\n",
    "| wikipedia         | 115014      | 0.2906533405184746   | 0.9479240975888144 | 0.9471808396892152 | 0.9475523228865832 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='end'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
